[["index.html", "Linear models in Agriculture and Natural Resources 1 Preface 1.1 Learning outcomes: 1.2 Overview of statistical analysis workflow", " Linear models in Agriculture and Natural Resources Phil Hahn &amp; Leo Ohyama 2025-01-08 1 Preface This is a book written in Markdown for ALS6502C at the University of Florida, instructed by Dr. Phil Hahn (hahnp@ufl.edu). Currently, this is a draft version and is still a work in progress. This book is meant to be used in conjunction with the class lectures and is not meant to replace lectures. With that being said, this book is written to be semi-standalone and to provide students to review the material. Please visit to course canvas page for syllabus and additional materials. 1.1 Learning outcomes: By the end of the course, students will be able to: Propose biological questions and formulate hypotheses to test them. Construct statistical models that are commonly used in agricultural and natural resource studies, including linear models, generalized linear models, linear mixed models, and generalized linear mixed effects models, using freely available packages in R. Analyze, visualize, interpret, and report the results of statistical models using formats acceptable for publication. Critique results of analyses reported by peers and in the literature. Select R documentation and use new R packages and functions. 1.2 Overview of statistical analysis workflow 1.2.1 Workflow pre-analysis What is your question of interest? Hypothesis? Pick an approach (observational, experiment, model) What data (and covariates) to collect? Carefully plan your analysis ahead of time Focus on testing your hypothesis Data wrangling 1.2.2 Workflow for analysis Data exploration Construct a statistical model to test your hypothesis Response variable - which distribution? Fixed effects and potential interactions Random effects Check model assumptions Residuals Overdispserion or zero-inflation (if applicable) Check random effects and/or distribution using AIC or LRT Be cautious with this step Usually random effects and distributions should be selected a priori based on the study design and type of data collect Examine significance of terms in model Examine parameter estimates emmeans and/or emtrend Construct contrasts also emmeans/emmtrends Calculate R2m and R2c Make pretty figure "],["data-vizualization-and-review-of-linear-models.html", "2 Data vizualization and review of linear models 2.1 Introduction to Data Visualization 2.2 Regression as a linear model 2.3 ANOVA as a linear model 2.4 Extra Data Visualization 2.5 ggplot2 app via Shiny", " 2 Data vizualization and review of linear models After completing this module, students will be able to: 2.1 Create figures for graphing continuous and categorical variable 2.2 Analyze continuous data using linear models (aka regression) in R 2.3 Analyze categorical data using linear models (aka ANOVA) in R 2.1 Introduction to Data Visualization Data visualization is a great way to start a statistics course because it is a powerful form of communication. Think about this, where do your eyes normally go to when you read a peer-reviewed paper? For most people its the figures. Oftentimes many academics will structure their papers around several key figures allowing these figures to tell the “story” of the manuscript. As such, effective data viz is a powerful tool for any scientist. Additionally, by setting up the x- and y-axes, it makes us think about the statistical model that we will eventually use to analyze the data. In this section, we will begin by practicing plotting data in RStudtio using the iris dataset. The iris dataset contains measurements of plant floral traits across several species. Let’s take a look. data(iris) # load data (already exists in base R) Quick note: the # allows us to make comments in the code without R reading it. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... The str() function allows us to see the structure of the data. The iris dataframe has measurements of 4 traits on 3 species. The traits (Sepal.Lenth, Sepal.Width, Petal.Length, Petal.Width) are continuous, or numeric, variables, whereas the Species column is a factor with three levels (three species). Below we can use the head() function to print the first 6 rows of the dataframe. head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa 2.1.1 Continuous Variables 2.1.1.1 Scatterplots with base R We can start with a simple plot where we plot sepal length by width: plot(Sepal.Length~Sepal.Width, data=iris) Figure 2.1: Plot of Sepal.Length by Sepal.Width We can also specify colors using the “col =” argument within the plot function wrapper. plot(Sepal.Length~Sepal.Width, data=iris, col=&quot;blue&quot;) Figure 2.2: Plot of Sepal.Length by Sepal.Width with blue points We can get more complex by specifying solid dots with separate colors for each species. plot(Sepal.Length~Sepal.Width, data=iris, pch=16, col=c(&quot;red&quot;,&quot;blue&quot;,&quot;purple&quot;)[iris$Species]) Figure 2.3: Plot of Sepal.Length by Sepal.Width with different colored points for each of the three species We can specify to plot only one species by editing the “data=” argument. The “==” in the code means “exactly equals”. Using just “=” will not work. ### plot only the data for Iris virginica plot(Sepal.Length~Sepal.Width, data=iris[iris$Species==&#39;virginica&#39;,]) ## use brackets to select the columns you want Figure 2.4: Plot of Sepal.Length by Sepal.Width for only viriginica species Finally, we can add a standard linear trend line across the data. These trend lines can be used to both explore potential relationships in the data and to also show readers or viewers of an established and existing trend. plot(Sepal.Length~Sepal.Width, data=iris[iris$Species==&#39;virginica&#39;,]) abline(lm(Sepal.Length~Sepal.Width, data=iris[iris$Species==&#39;virginica&#39;,])) ## adds trend line for linear model Figure 2.5: Plot of Sepal.Length by Sepal.Width with trendline for viriginica species 2.1.1.2 Scatter plots with ggplot2 In the previous plots we used the basic R commands to plot. But we can also use other packages to ease the coding and specification of plot types. One of these packages is called gpplot2. ggplot2 comes with the metapackage (a package of packages) called the ‘tidyverse’ package. library(tidyverse) ## install tidyverse if necessary More information on how to use ggplot2 can be found here. For the sake of time, we provide a very bare bones but basic introduction to this powerful package in the next few lines. Box 1. ggplot2 is a popular package for many life science researchers because it offer an intuitive way to code graphics. The ggplot language structure uses layers of different geom objects. A geom object is essentially what the structure of a ggplot plot object. So a scatter plot would include a geom object for points which is created with the geom_points() function. The basic framework that ggplot2 revolves around specifying a data set and plotting those data using arguments in the aesthetic section of the ggplot2 code. The data set is specified with the data= argument. The aesthetic section of code is specified by the function aes(). Within the parentheses of this function you will be able to choose and manipulate a variety of variables from your chosen data set. Let’s start by recreating the simple plot of sepal length by width. Here we set the data = argument to iris because we are plotting data from the iris data set. Noticed we specify the x and y axes within the aes() function in the code below. The ggplot() merely draws the axes; we need to add the points using the `geom_point()’ function. The aestetics will carry through from the main ggplot call. ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length)) + geom_point() Figure 2.6: Sepal.Length by Sepal.Width made with ggplot We can also add colors to the points just like base R using the “color =” argument within the geom_point function. Here we specify the color to be blue. If you want to explore colors and choose different ones, this website is an excellent resource. ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length)) + geom_point(color=&#39;blue&#39;) Figure 2.7: Sepal.Length by Sepal.Width made using ggplot with blue points We can also color the points by species identify by including the “color =” argument within the aes() wrapper in the first ggplot line. As previously mentioned, aes() sets the aesthetics of the plot based on the data that is being used. The data is specified with the “data =”. Therefore, to change colors, sizes etc. of the plot in the context of your data, all arguments should be done typed within the aes() wrapper. ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length, color=Species)) + geom_point() Figure 2.8: Sepal.Length by Sepal.Width made using ggplot with different colored points for each species What if we want to see the different species in different plot panels? We can do this by using the argument facet_wrap(). facet_wrap() allows you to facet the plot by a a categorical variable from the data set. In the example below, we facet the plot above by species. ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length, color=Species)) + geom_point() + facet_wrap(~Species) Figure 2.9: Sepal.Length by Sepal.Width made with ggplot facet-wrapped so each species is in a seperate panel Now finally let’s redo the plots we did in base R. Let’s first plot only one species. We can do this by manipulating the data set being used within the “data =” argument. We filter the original data set “iris” so that only a specific species is being used in the plot. To make things clear, we should also add a title stating how the plot reflects only one species. In the below code we do all this and also show how to change the size of the plot title using the theme() function. ggplot(data=iris %&gt;% filter(Species==&#39;virginica&#39;), aes(x=Sepal.Width, y=Sepal.Length)) + geom_point() + labs(title = &quot;Plot with only Virginica&quot;) +#we can add a title to the plot using the labs() and specifying the &quot;title =&quot; argument theme(plot.title = element_text(size = 25)) Figure 2.10: Sepal.Length by Sepal.Width made with ggplot with only species viriginica Now let’s plot the data and add a linear trend line. To do this we use geom_smooth(). Within geom_smooth() we use the argument “method =” to specify the type of trend line. Since we want a linear one based on a linear model we use “lm”. ggplot(data=iris %&gt;% filter(Species==&#39;virginica&#39;), aes(x=Sepal.Width, y=Sepal.Length)) + geom_point() + geom_smooth(method=&#39;lm&#39;) Figure 2.11: Sepal.Length by Sepal.Width made using ggplot with trendline for species virginica We can also add a separate trend line for all three species. ggplot2 can do this in a very user friendly way. By specifying different colors for different species in the first ggplot() line, the usage of geom_smooth() automatically applies the categorization by colors to the trend line resulting in separate trend lines for each species. ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length, color=Species)) + geom_point() + geom_smooth(method=&#39;lm&#39;) Figure 2.12: Sepal.Length by Sepal.Width made using ggplot with trendline for each species Notice that if we move the “color =” argument to the outside of the aes() and just specify a single color. it changes the colors of all points. This happens because any arguments (e.g. color = ) that are entered out of the aes() wrapper do not apply to the data (e.g. species types). ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length)) + geom_point(color=&quot;blue&quot;) + facet_wrap(~Species) + geom_smooth(method=&#39;lm&#39;) Figure 2.13: Sepal.Length by Sepal.Width made using ggplot with each species on a seperate panel with all blue points 2.1.2 Fancy Scatter plots For more appealing color options we use the ‘viridis’ package. This package comes with several pre-made colorways or palettes that work well with a variety of plot and data types. Furthermore, many of their palettes are geared to be color-blind friendly which is important especially when publishing visuals. library(viridis) ## install viridis package if necassary Let’s set up a basic plot from the previous examples shown above. The default colors work fine but could be better and the plot background could also be cleaner and more improved. ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length, color=Species)) + geom_point() + geom_smooth(method=&#39;lm&#39;) Figure 2.14: Sepal.Length by Sepal.Width made using ggplot with trendline for each species We can change the background elements of the plot (a.k.a the “theme”) with preset defaults such as theme_bw(). These defaults are a quick way to improve the general plot appearance and to also remove possibly distracting plot elements. ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length, color=Species)) + geom_point() + geom_smooth(method=&#39;lm&#39;) + theme_bw() Figure 2.15: Sepal.Length by Sepal.Width made using ggplot with trendline for each species and a black and white theme We can change the size of the points by adding a “size =” argument in the geom_point(). But what about colors? To change the colors and make the visualization more accessible to people (e.g. making them more color-blind friendly) using the scale_color_virdis(). Within the virdis function, we specify that we want a discrete color scale with “discreet = TRUE”. This is because we want to split our regressions and show differences between species which is a categorical variable. If we were hoping to use viridis for continuous variables we would use “discreet = FALSE”. ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length, color=Species)) + geom_point(size=3) + # change point size to make them bigger geom_smooth(method=&#39;lm&#39;) + scale_color_viridis(discrete = TRUE) + # change points to a color-blind friendly palette. Can specify specific colors theme_bw() # new theme Figure 2.16: Sepal.Length by Sepal.Width made using ggplot with trendline for each species, a black and white theme, and an accessible color palette We can change the shapes of the points based on species with the “shape =” argument in the first ggplot() line. We specify that this argument equals “Species” which basically means, set different shapes for different species. We can further specify which shapes we want for the three species with the scale_shape_manual(). ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length, shape=Species)) + geom_point(size=3) + geom_smooth(method=&#39;lm&#39;) + scale_shape_manual(values=c(&quot;circle&quot;,&quot;square&quot;,&quot;triangle&quot;)) + theme_bw() Figure 2.17: Sepal.Length by Sepal.Width made using ggplot with trendline and shapes for each species and a black and white theme Let’s specify with colors and shapes. Here’s an annotated code chunk to show exactly what components are being specified for the plot. ggplot(data=iris, aes(x=Sepal.Width, #Sets x axis variable y=Sepal.Length, #Sets y axis variable shape=Species, #use different shapes for species color=Species #use different colors for species )) + geom_point(size=3) + #set size if points geom_smooth(method=&#39;lm&#39;) + #set a linear model trendline scale_color_viridis(discrete=T) + #color palette scale_shape_manual(values=c(&quot;circle&quot;,&quot;square&quot;,&quot;triangle&quot;)) + #specify which shapes theme_bw() #different more black and white theme Figure 2.18: Sepal.Length by Sepal.Width plot with many specialized features. We can also facet the plot above and increase the font size ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length, shape=Species, color=Species)) + geom_point(size=3) + geom_smooth(method=&#39;lm&#39;) + scale_color_viridis(discrete=T) + scale_shape_manual(values=c(&quot;circle&quot;,&quot;square&quot;,&quot;triangle&quot;)) + facet_wrap(~Species) + #facet by species theme_bw(base_size = 14) # increase font size for the entire plot Figure 2.19: Sepal.Length by Sepal.Width plot with many specialized features, facet-wrapped. Some people have to render multiple plots and retyping your theme settings can be a pain! So this is where the beauty of working in R and RStudio come into play. You can set your own theme as a object in R: my_theme&lt;-theme(axis.title = element_text(face = &quot;bold&quot;,size = 12), legend.position = &quot;top&quot;, axis.ticks = element_blank(), axis.text = element_text(color = &quot;blue&quot;, size = 11), title = element_text(color = &quot;dodgerblue&quot;, size = 15)) Now you can call on this object my_theme any time when making a plot and avoid typing out the several lines of code over and over again: ggplot(data=iris, aes(x=Sepal.Width, y=Sepal.Length, shape=Species, color=Species)) + geom_point(size=3) + geom_smooth(method=&#39;lm&#39;) + scale_color_viridis(discrete=T) + my_theme Figure 2.20: Sepal.Length by Sepal.Width plot with specialized features from my theme. 2.1.3 Categorical Variables Categorical variables are usually things with names. In the iris dataset, Species is a factor, or a categorical, variable. When plotting with categorical variables, we can no longer use scatterplots, so here we will go through a few different types of plots. 2.1.3.1 Boxplots with base R This line of code will make a simple boxplot. Notice the usage of ~. To avoid errors, make sure your continuous variable comes before ~ and your categorical variable comes after. plot(Sepal.Length~Species, data=iris) #make boxplot Figure 2.21: Boxplot showing Sepal.Length by the three Species. We can add colors to the different boxplots. plot(Sepal.Length~Species, data=iris, col=c(&quot;red&quot;,&quot;blue&quot;,&quot;purple&quot;)) #make boxplot with color Figure 2.22: Boxplot showing Sepal.Length by the three Species with colors. 2.1.4 Plots for categorical data with ggplot2 ggplot2 is also a great tool for plotting categorical data. Boxplots show the distribution of data based on the median and interquartile ranges. To make a boxplot with ggplot we use ‘geom_boxplot()’. ggplot(iris, aes(x=Species, #variable on x axis y=Sepal.Length #variable on y axis )) + geom_boxplot() #specify boxplot option Figure 2.23: Boxplot showing Sepal.Length by the three Species using ggplot. If the width of the boxplots are too wide, this can be changed with the “width =” argument: ggplot(iris, aes(x=Species, #variable on x axis y=Sepal.Length #variable on y axis )) + geom_boxplot(width = 0.5) #specify boxplot option Figure 2.24: Boxplot showing Sepal.Length by the three Species in ggplot. We can also overlay points on the boxplots. ggplot(iris, aes(x=Species, y=Sepal.Length )) + geom_boxplot() + geom_point() Figure 2.25: Boxplot showing Sepal.Length by the three Species in ggplot with points added. If there are many overlapping points then the plot above could be misleading i.e. 100 points with the same value would still appear to be one single data point. To clearly assess the scatter of the points we can jitter their position with geom_jitter(): ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_boxplot() + geom_jitter() Figure 2.26: Boxplot showing Sepal.Length by the three Species in ggplot with points jittered. We may want to adjust how much jitter we give the points. This can be done with the “height =” and “width =” arguments. ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_boxplot(outlier.shape=NA) + geom_jitter(height=0, width=.15) Figure 2.27: Boxplot showing Sepal.Length by the three Species in ggplot with points jittered. We can also make dot plots for the data using geom_dotplot(). ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_dotplot(binaxis = &quot;y&quot;, stackdir = &quot;center&quot;) Figure 2.28: Dotplot showing Sepal.Length by the three Species in ggplot. There are also violin plots with geom_violin(). Violin plots are useful because they show a more user-friendly visual of the distribution of data. ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_violin(trim=F) Figure 2.29: Violin plot showing Sepal.Length by the three Species in ggplot. A combination of violin plots with dot plots is also possible. ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_violin(trim=F) + geom_dotplot(binaxis = &quot;y&quot;, stackdir = &quot;center&quot;) Figure 2.30: Violin plot showing Sepal.Length by the three Species in ggplot with dotplot added. Finally, we can also plot violin plots with boxplots! This is very useful because we not only get distribution information from both but we can also roughly see the median values and the inter-quartile ranges! ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_violin(trim=F, bw=.5) + geom_boxplot(width=.1) Figure 2.31: Boxplot showing Sepal.Length by the three Species in ggplot with violin plot added. 2.1.4.1 Adding colors and summary statistics (e.g. averages) to plots Let’s change the color of the boxes. Note we don’t use the “color =” argument but instead use the “fill =” argument. This is because fill will affect the fill color inside the box while color will affect the color of the border of the box. Given this, when we use the viridis function for the nicer colors, we specify scale_fill_virdis rather than scale_color_virdis. ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) + geom_boxplot(outlier.shape=NA) + geom_jitter(height=0, width=.15) + scale_fill_viridis(discrete=T) Figure 2.32: Boxplot showing Sepal.Length by the three Species in ggplot with viridis colors. If we want to see the averages or means per species we can do this by using stat_summary() and specifying “mean” with the “fun =” argument. Let’s also change the viridis color palette to something different. To do this we can enter the “options=” argument in the viridis function. The options span from A to H but for a details on specific palette options see this website. ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) + geom_boxplot(outlier.shape=NA) + geom_jitter(height=0, width=.15) + scale_fill_viridis(discrete=T, option = &quot;A&quot;) + stat_summary(fun=mean, geom=&quot;point&quot;, size=4, color=&quot;red&quot;) ## add point for mean Figure 2.33: Boxplot showing Sepal.Length by the three Species in ggplot with means added. Let’s edit the theme of the plot above ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) + geom_boxplot(outlier.shape=NA) + geom_jitter(height=0, width=.15) + scale_fill_viridis(discrete=T, option = &quot;D&quot;) + stat_summary(fun=mean, geom=&quot;point&quot;, size=4, color=&quot;red&quot;) + theme_bw(base_size = 16) #theme change Figure 2.34: Boxplot showing Sepal.Length by the three Species in ggplot with means added. To save a plot like this as a .tiff we should first assign the plot as an object in the environment. In the example below we assign is as an object called “plot1” using the “&lt;-” or “=”. plot1 &lt;- ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) + ## plot now saved as object called &#39;plot1&#39; geom_boxplot(outlier.shape=NA) + geom_jitter(height=0, width=.15) + scale_fill_viridis(discrete=T) + stat_summary(fun=mean, geom=&quot;point&quot;, size=3, color=&quot;red&quot;) + theme_bw(base_size = 16) Then we use ggsave() function to save the plot into whatever working directory you are using. We can set the dimensions of the image and we specify the plot we want to save using “plot1”. ggsave(&quot;ExamplePlot.tiff&quot;, #file name to be used plot1, #what plot being saved width=4, #width height=3, #height units=&quot;in&quot;, #units being used for width and height dpi=300) #resolution of photo, higher number = higher res Note that tiff files can often take up a lot of space. This can be a problem when sending quick emails to collaborators. I recommend using the portable network graphic format also known as .png files. These files are smaller and retain much of the quality of the visualizations. 2.1.5 Breakout group challenge For this challenge use the mtcars dataset: data(mtcars) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Plot x=mpg by y=hp Color code points by wt Add trend line. Make the background white. theme_bw is okay, if time try playing around with other themes. Make a boxplot of mpg for the three cyl groups. You should have three boxes. If only one, why? Change colors, themes, add data points, etc. If data visualization is an interest to you then be sure to check out the following resources for useful code, information, and methods! R graph gallery Cedric Sherer, an amazing ggplot2 user who frequently posts his newest projects and the code 3-Dimensional Plots 2.2 Regression as a linear model Regression analysis is a form of analysis that allows the user to evaluate a response variable as a function of predictor variable in a quantitative and statistical manner. Most people are accustomed to seeing regressions in the form of simple linear regressions, also known as ordinary least squares (OLS) regression. The equation \\[y = \\beta_{0} + \\beta_{1}*x + \\varepsilon\\] is the basic linear regression equation, where the terms mean the following: \\(y\\) is the response variable \\(\\beta_{0}\\) is the intercept \\(\\beta_{1}\\) is the slope \\(x\\) is the predictor variable and \\(\\varepsilon\\) are the residuals. The key features that make this type of linear model a regression are that the response variable and predictor variables are both continuous variables. We will use different versions of this linear model throughout the book. 2.2.1 An Example of Linear Regression in R For this section we will use the tidyverse, car, and glmmTMB packages. For constructing the linear model we will use the function glmmTMB(). There are simpler alternatives to glmmTMB but we will start with this because it is very flexible and we will use it throughout the course for more complex models. The more simple alternative to run a linear regression is the function lm(), which is built into the base R libraries. The results from lm() and glmmTMB() should be nearly identical. The limitation of lm() is that is can only run a linear regression and not more complex models. Starting with glmmTMB() will allow us to use this throughout the course without switching functions for different types of linear models. More later, but let’s get started. library(tidyverse) library(car) library(glmmTMB) For this example, we will generate random numbers to create a data set that we can use to apply a linear model to. To make things reproducible (ie. so everyone gets the same results from the random numbers) we will need to set a seed. To do this we use the function set.seed(). If everyone uses the same seed they will get the same random numbers that are generated from the following code. Setting seeds is an important practice for scientific reproducibility and will be more common as your analyses become more complex. #Set Seed set.seed(21) # Generate random data temp &lt;- round(runif(20,12,30), 2) mass &lt;- round(rnorm(20,5*temp,25), 2) r1 &lt;- as.data.frame(cbind(temp,mass)) This is the data set we will now use for the example: head(r1) ## temp mass ## 1 26.15 73.83 ## 2 16.54 101.64 ## 3 24.59 109.24 ## 4 15.32 80.91 ## 5 29.27 160.42 ## 6 28.54 180.50 In this dataset, temp represent rearing temperature in Celsius and mass is the mass of adults in milligrams. Basically, we want to know if adult mass increases with warmer temperatures. Let’s plot this data: ggplot(r1, aes(x=temp, y=mass))+ geom_point(size = 5, pch = 21, fill = &quot;gray&quot;, color = &quot;black&quot;, width = 2)+ theme_bw(base_size = 14) Figure 2.35: temp by mass. Definitely looks like a positive correlation. Now let’s construct a linear model to estimate the change in average adult mass per degree Celsius of temperature increase. For a continuous variable (temperature in Celsius), we are interested in estimating the slope (\\(\\beta_{1}\\)) between mass and temperature. We can set up the model using the lm() function. The variable specified before (ie. left-hand size) “~” is your response (mass in this case) and the variable specified after (ie. right-hand side) is your predictor (temperature in this case). lm1 &lt;- glmmTMB(mass~temp, data=r1) # all &quot;calculations&quot; are saved in an object we called &#39;lm1&#39; We can construct an ANOVA table of this model. The ANOVA table tests the null hypothesis that the slope is different than zero. It’s not not as useful for regressions compared to actual categorical variable analyses but still worth to look at. Anova(lm1, type=2) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: mass ## Chisq Df Pr(&gt;Chisq) ## temp 25.707 1 3.973e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 To see the summary of the model coefficients we use summary(). summary(lm1) ## Family: gaussian ( identity ) ## Formula: mass ~ temp ## Data: r1 ## ## AIC BIC logLik deviance df.resid ## 187.3 190.3 -90.6 181.3 17 ## ## ## Dispersion estimate for gaussian family (sigma^2): 506 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 9.8625 19.0484 0.518 0.605 ## temp 4.3716 0.8622 5.070 3.97e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The coefficients allow you rebuild the means from the linear model equation (same as above): \\[y = \\beta_{0} + \\beta_{1}*x + \\varepsilon\\] In this case, the intercept (\\(\\beta_{0}\\)) is conveniently labeled (Intercept) and the slope (\\(\\beta_{1}\\)) is labeled temp. The estimated coefficients are displayed under Estimate, along with the standard error and tests of significance. We can also look at the model coefficients with fixef(). fixef(lm1) ## ## Conditional model: ## (Intercept) temp ## 9.862 4.372 We can take these coefficients and plug them into our linear model equation, like this: \\[y = 9.86 + 4.37*x + \\varepsilon\\] We can make a plot with the best-fit regression line and intercept. ggplot(r1, aes(x=temp, y=mass))+ geom_point(size=3)+ geom_smooth(method=&quot;lm&quot;)+ theme_bw() Figure 2.36: mass plotted against temp with linear regression line. The intercept value of 9.86 is where the prediction line cross the y-intercept when x=0. We can’t quite see that on the figure above, but you can imagine it. The slope value of 4.37 is perhaps a little more interesting. It means that for each 1 degree C increase in temp, the average adult mass increases by 4.37mg. With those two parameters, we can make predictions about the average adult mass at any temperature. These predictions of average mass is what is represented by the blue line in the figure above. To check assumptions of the model, we examine residuals. Box 2. Residuals. A quick note on residuals. Many people assume the normality assumption can be assessed by assessing the normality of the response variables. This is a common mistake as the assumption of normality rests on the residuals or the error term of the model, essentially what is not explained by the model. Therefore, to assess normality in a model you assess the normality of the residuals/ We can check normality of residuals with a histogram. Note others sometimes use a Shapiro-Wilkes test on the residuals to statistically test for significance in the normality of residuals, which can be problematic (we will return to those problems in Module 3). Visual assessment of the residuals is almost always sufficient to assess model fit. hist(resid(lm1)) Figure 2.37: Histogram of residuals. We can check the homogeneity of the residuals by plotting the residuals against the model’s fitted values. The residuals should be evenly dispersed around 0 across the range of x values. Funnel shapes or curvature in the dispersion would indicate violations. plot(resid(lm1)~fitted(lm1)) abline(h=0) Figure 2.38: Residuals plotted against fitted values. Using the car package we can also make a qqplot (quantile-quantile plot). Residuals should line up pretty closely to the blue line and points that drift from line may be outlier points. qqPlot(resid(lm1)) Figure 2.39: QQplot of residuals. ## [1] 1 6 In our example, the residuals look fine; no problems. Problems with residuals indicate assumptions of the linear model are violated and may cause problems with coefficients and p-values. These problems can span a variety of examples such as: inaccurate model estimates, high errors in model predictions, false-positive statistical significance etc. To alleviate potential issues applying transformations to the data may help. It’s useful to note that assumptions can be slightly violated without causing problems. For more reading check this paper out! Finally, we can now make a fancy plot of this model. ggplot(r1, aes(x=temp, y=mass)) + geom_point(size=3,color=&#39;blue&#39;) + geom_smooth(method=&#39;lm&#39;, fill=&quot;blue&quot;, alpha=.1) + labs(x = &quot;Temperature&quot;, y = &quot;Mass&quot;) + theme_bw() + theme(axis.title = element_text(face = &quot;bold&quot;, size = 14)) Figure 2.40: Nice looking plot of mass by temp. 2.2.2 Regression Challenge Run the code shown below to answer questions regarding the orange data set. The data set has measurements of circumference on five trees at seven time points. data(&quot;Orange&quot;) ## load Orange dataset from base R head(Orange) ## Grouped Data: circumference ~ age | Tree ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 ## 6 1 1372 142 Healthy orange trees typically produce fruit at 100 cm in circumference. A homeowner calls and says their orange tree is 3 years old (1095 days), but isn’t fruiting. They didn’t measure it. They also said their are some white spots on the leaves. Build a linear model (and make plot) to answer the following questions. What circumference should their tree be, on average? Should their tree be fruiting by now? What advice would you give the grower? Are the model assumptions met? Make a nice figure. 2.3 ANOVA as a linear model Let’s load the necessary packages for an introduction to ANOVAs (Analysis of Variance). library(tidyverse) library(car) library(glmmTMB) library(emmeans) # emmeans package, helpful for getting means from linear models 2.3.1 Example of a one-way ANOVA in R For this section we will use the insect spray data set. The dataset contains counts of insects in plots with different insecticides applied. We want to know which insecticides, if any, are effective at reducing insect counts. Each insecticide treatment is replicated in eight plots. data(&quot;InsectSprays&quot;) str(InsectSprays) ## &#39;data.frame&#39;: 72 obs. of 2 variables: ## $ count: num 10 7 20 14 14 12 10 23 17 20 ... ## $ spray: Factor w/ 6 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... head(InsectSprays) ## count spray ## 1 10 A ## 2 7 A ## 3 20 A ## 4 14 A ## 5 14 A ## 6 12 A Let’s filter the data to just 4 treatments using some ‘dplyr’ functions. We filter InsectSprays and create a new dataframe called d. Spray F if the the control and the other three are experimental sprays. d &lt;- InsectSprays %&gt;% filter(spray==&#39;A&#39;|spray==&#39;B&#39;|spray==&#39;C&#39;|spray==&#39;F&#39;) %&gt;% droplevels() After that, we can plot the data using boxplots. ggplot(d, aes(x=spray,y=count)) + geom_boxplot(outlier.shape = NA) + # need to suppress outliers if you jitter plot points geom_jitter(height=0,width=.1) Figure 2.41: Boxplot showing counts of insects in the different treatments. Let’s construct linear model to examine the effect of the different sprays on insect counts. For a categorical variable (spray with four levels), we are interested in comparing group means. We use a model very similar to the regression we used in the previous section, but here our predictor variable is just different groups rather than a continuous x variable. Here is the modified model: \\[y = \\beta_{0} + \\beta_{i} + \\varepsilon\\] is the basic linear regression equation, where the terms mean the following: \\(y\\) is the response variable \\(\\beta_{0}\\) is the intercept \\(\\beta_{i}\\) is the adjustment to the intercept for each group \\(_{i}\\) and \\(\\varepsilon\\) are the residuals. For this example, we have four groups and so also four coefficients. The first alphabetical group (Spray A) will be assigned the intercept (\\(\\beta_{0}\\)) and the other three sprays will be adjustments to the intercept (\\(\\beta_{1:3}\\)). First lets setup the model: lm2 &lt;- glmmTMB(count~spray, data=d) To compare group means we can use the Anova(). Anova(lm2, type=2) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: count ## Chisq Df Pr(&gt;Chisq) ## spray 86.656 3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the above case, the null hypothesis is that all group means are equal (group means would be the means of treatment types). With the low p-value, we can say at least one of the sprays is different than one other spray. But which ones? Box 3. Sums of Squares | | A quick note on Sums of Squares. Specifying the argument, “type = 2”, provides Type II sums of squares, which is usually better than the default Type I, especially for more complicated models. Note that Type II Sums of Squares is different than how they are calculated in SAS (in SAS, Type III is often preferred). In the car package in R Type II is usually preferred. Other functions (anova(), aov(), etc.) will provide similar ANOVA tables, but the Anova() is more flexible./ Now let’s look at the summary of the model to examine the coefficients. summary(lm2) ## Family: gaussian ( identity ) ## Formula: count ~ spray ## Data: d ## ## AIC BIC logLik deviance df.resid ## 287.6 297.0 -138.8 277.6 43 ## ## ## Dispersion estimate for gaussian family (sigma^2): 19 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 14.5000 1.2592 11.516 &lt; 2e-16 *** ## sprayB 0.8333 1.7807 0.468 0.640 ## sprayC -12.4167 1.7807 -6.973 3.11e-12 *** ## sprayF 2.1667 1.7807 1.217 0.224 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can plug these coefficients into our equation above to get the means for each group. For example, the means for “Spray A” is the intercept, so 14.5. The mean for “Spray B” we have to calculate as 14.5 + 0.8333 = 15.333 and so on for the other sprays. In this example, rebuilding the model from the coefficients is not super helpful and the p-values aren’t very meaningful. To address this, we can use the package ‘emmeans’ and the function emmeans() which will rebuild the model for you. It will print off the means, SE, and confidence intervals for each treatment group! While this can be done with manual coding, the package allows this to be done across different types of models with just one function! emmeans(lm2, ~spray) ## spray emmean SE df lower.CL upper.CL ## A 14.50 1.26 43 11.961 17.04 ## B 15.33 1.26 43 12.794 17.87 ## C 2.08 1.26 43 -0.456 4.62 ## F 16.67 1.26 43 14.127 19.21 ## ## Confidence level used: 0.95 We can also look at pairwise differences between groups and automatically adjust p-values using “tukey” adjust. Pairwise comparisons are useful as they allow us to understand the full extent of differences in means between specific treatment types. emmeans(lm2, pairwise~spray) ## $emmeans ## spray emmean SE df lower.CL upper.CL ## A 14.50 1.26 43 11.961 17.04 ## B 15.33 1.26 43 12.794 17.87 ## C 2.08 1.26 43 -0.456 4.62 ## F 16.67 1.26 43 14.127 19.21 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## A - B -0.833 1.78 43 -0.468 0.9657 ## A - C 12.417 1.78 43 6.973 &lt;.0001 ## A - F -2.167 1.78 43 -1.217 0.6198 ## B - C 13.250 1.78 43 7.441 &lt;.0001 ## B - F -1.333 1.78 43 -0.749 0.8767 ## C - F -14.583 1.78 43 -8.190 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 4 estimates We should also check assumptions of the model. hist(resid(lm2)) ## residuals should be normally distributed Figure 2.42: Residual plots. plot(resid(lm2)~fitted(lm2)) ## residuals should be evenly dispersed abline(h=0) Figure 2.43: Residual plots. qqPlot(resid(lm2)) Figure 2.44: QQplot of residuals. ## [1] 45 46 Boxplots of residuals across treatment types should show that the variances should be homogeneous for each group. boxplot(resid(lm2) ~ d$spray) Figure 2.45: Boxplot of residuals. Problems with residuals indicate assumptions of the linear model are violated and may cause problems with coefficients and p-values. Transforming the data or using a different type of model may help (we will return to this example later in the book to improve it). Again, assumptions can be slightly violated without causing problems, for example this model is totally fine but could be better. It is best practice to be transparent with residual diagnostics. 2.3.2 ANOVA Challenge Have a look at the dataset below. Baby chickens were fed different diets and they were weighed after 10 days. The variable ‘weight’ is the weight of a baby chicken (g); ‘feed’ is the type of type of diet the chicken was fed. d1 &lt;- chickwts head(d1) ## weight feed ## 1 179 horsebean ## 2 160 horsebean ## 3 136 horsebean ## 4 227 horsebean ## 5 217 horsebean ## 6 168 horsebean Construct a linear model to analyze the data. Is there evidence at least one mean is different than another? How much variation in the data does the model explain? The feed ‘casein’ is the standard chicken diet. What types of feed are significantly worse than ‘casein’. By how much are they worse? Are the assumptions met? Make a nice looking figure. show all the data. 2.4 Extra Data Visualization This section covers some more advanced plotting with ggplots Let’s load the necessary libraries or packages library(tidyverse) library(emmeans) 2.4.1 Adding Model Averages to Plots In the previous section of data visualization we learned how to create a boxplot with averages based on species. Just like this: ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) + geom_boxplot(outlier.shape=NA) + geom_jitter(height=0, width=.15) + scale_fill_manual(values=c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;)) + stat_summary(fun=mean, geom=&quot;point&quot;, size=5, color=&quot;red&quot;) The stat_summary option doens’t allow a lot of flexibility. We can do better by calculating the means and SE ourself to use in the plot. We can do this by constructing a linear model to estimate means and standard errors of the means for plotting. First we construct the model. sl1 &lt;- lm(Sepal.Length~Species, data=iris) Then we use emmeans() to calculate the means and also pipe it as a data.frame. Basically what this is doing if we were to verbally explain is: “We first calculate the averages and the errors of those averages from the model. We then turn this information into a data.frame so we can access individual values for plotting”. sl_means &lt;- emmeans(sl1, ~Species) %&gt;% as.data.frame() ## saves emmeans as dataframe head(sl_means) ## Species emmean SE df lower.CL upper.CL ## setosa 5.006 0.07280222 147 4.862126 5.149874 ## versicolor 5.936 0.07280222 147 5.792126 6.079874 ## virginica 6.588 0.07280222 147 6.444126 6.731874 ## ## Confidence level used: 0.95 With the object sl_means we can make bar plots with standard error bars. ggplot(sl_means, aes(x=Species, y=emmean)) + geom_bar(stat=&quot;identity&quot;, color=&quot;black&quot;, fill=&#39;grey&#39;) + geom_errorbar(aes(ymin=(emmean-SE), ymax=(emmean+SE)), width=.2) We can tidy up the plot. ggplot(data = sl_means, aes(x=Species, y=emmean)) + geom_bar(stat=&quot;identity&quot;, color=&quot;black&quot;, fill=&#39;grey&#39;, width=.5) + geom_errorbar(aes(ymin=(emmean-SE), ymax=(emmean+SE)), width=.2) + ## make bars thinner geom_hline(yintercept = 0) + theme(panel.background = element_blank(), panel.border = element_rect(color=&quot;black&quot;, fill=NA, size=2)) + ## change &quot;theme&quot; so the background is blank and the border is thicker theme(axis.ticks.length=unit(0.3, &quot;cm&quot;), axis.text.x = element_text(margin=margin(5,5,5,5,&quot;pt&quot;),colour=&quot;black&quot;), axis.text.y = element_text(margin=margin(5,5,5,5,&quot;pt&quot;),colour=&quot;black&quot;)) + ## change axis tick marks to make them a little longer theme(text = element_text(size=20)) We can also add points to the barplot. ggplot() + geom_bar(data=sl_means, aes(x=Species, y=emmean), stat=&quot;identity&quot;, color=&quot;black&quot;, fill=&#39;grey&#39;, width=.5) + geom_errorbar(data=sl_means , aes(x=Species, y=emmean, ymin=(emmean-SE), ymax=(emmean+SE)), width=.2) + ## make bars thinner geom_jitter(data=iris, aes(x=Species, y=Sepal.Length), height=0, width=.15) + theme(panel.background = element_blank(), panel.border = element_rect(color=&quot;black&quot;, fill=NA, size=2)) + ## change &quot;theme&quot; so the background is blank and the border is thicker theme(axis.ticks.length=unit(0.3, &quot;cm&quot;), axis.text.x = element_text(margin=margin(5,5,5,5,&quot;pt&quot;), colour=&quot;black&quot;), axis.text.y = element_text(margin=margin(5,5,5,5,&quot;pt&quot;), colour=&quot;black&quot;)) + ## change axis tick marks to make them a little longer theme(text = element_text(size=20)) We can also try a dot plot with standard error bars. ggplot() + geom_jitter(data=iris, aes(x=Species, y=Sepal.Length), height=0, width=.1) + geom_point(data=sl_means, aes(x=Species, y=emmean), color=&quot;red&quot;, size=5) + geom_errorbar(data=sl_means, aes(x=Species, y=emmean, ymin=(emmean-SE), ymax=(emmean+SE)), width=.2, color=&quot;red&quot;, lwd=2) + ## make bars thinner theme(panel.background = element_blank(), panel.border = element_rect(color=&quot;black&quot;, fill=NA, size=2)) + ## change &quot;theme&quot; so the background is blank and the border is thicker theme(axis.ticks.length=unit(0.3, &quot;cm&quot;), axis.text.x = element_text(margin=margin(5,5,5,5,&quot;pt&quot;),colour=&quot;black&quot;), axis.text.y = element_text(margin=margin(5,5,5,5,&quot;pt&quot;),colour=&quot;black&quot;)) + ## change axis tick marks to make them a little longer theme(text = element_text(size=20)) 2.4.2 Extra R Challenge From R file 1c_R Intro_anova #5. Make a nice looking plot that includes the mean and SE of chick weight for the six feeds. Try making a boxplot with jittered points and then overlay the mean +/- SE in a large dot of a different color. Try changing the color of each box. Customize the colors, themes, etc. to make it look nice and readable. 2.5 ggplot2 app via Shiny This shinyapp will allow you to practice and implement ggplot2 commands. You can specify custom plot commands and view the actual product (the plot) as well as the code necessary to generate that plot. knitr::include_app(&quot;https://leoohyama.shinyapps.io/ggplot_practice/&quot;, height = 800) "],["data-management-and-exploration.html", "3 Data Management and Exploration 3.1 Data Management 3.2 Data Exploration 3.3 Model Selection", " 3 Data Management and Exploration After completing this module, students will be able to: 3.1 Describe the structure of data set and variables 3.2 Summarize variables in a dataset 3.3 Conduct exploratory analyses using tidyverse in R 3.4 Compare AIC scores to select among competing models 3.1 Data Management A key element to managing data in the RStudio GUI (graphic user interface) is to understand the structure of the data set(s) you are working with. While there are plenty of ways to do this, the basic and important functions will be explained in this section. First, we will go over examining your data set at a very basic level. We will practice using the iris dataset, which we used in the previous chapter. data(iris) # load data (already exists in base R) Sometimes, we want to see the data that make up our data set. The best way may be to simply just skim the top or bottom rows of the data set or dataframe. To do this we can use the head() and tail() function in R. These functions will print out the first or last several lines of the data set. head(iris) # print first 6 lines of dataset ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa tail(iris) # print last 6 lines of dataset ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica But perhaps we want to examine the type of data we have in each column. Do we have numerical data? Do we have count data that are only integers? What about categories? For large data sets, getting this information can be difficult through scanning the first or last several rows. So instead we can implement the str() function to look at the structure of the data set. When this function is used, R will print out a summary of each variable (column) in your dataframe along with its variable designation. str(iris) # print &#39;structure&#39; of dataset giving you info about each column ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 3.1.1 Making and modifying variables We can also quickly add new variables (columns) or exist pre-existing variables in a dataframe. Here’s how we make a new column that is a unique number. iris$Plant &lt;- 1:nrow(iris) The &lt;- is the assignment command; anything on the left-hand side of the &lt;- will be created and everything on the right-hand side is what we will command R to create. In this example we are creating a new column in the ‘iris’ dataset called ‘Plant’. The \"$\" is how we tell R the dataset (before \"$\" is the data set) and the column within that dataset (after \"$\"). So, the bit of code iris$Plant you can read as “within the data.frame ‘iris’ create a new column called ‘Plant’. Then, on the right-hand side of the &lt;- we are assigning this column values from 1 to the total number of rows in the data (150 rows). We can also create columns based on other columns. Here’s how we make a new column that is total petal and sepal length: iris$PetSep.Length &lt;- iris$Petal.Length+iris$Sepal.Length Modifying columns is also another aspect of working with data. Here’s how to make a new column called ‘lnPS.Len’ that log-transforms PetSep.Length: iris$lnPS.Len &lt;- log(iris$PetSep.Length) Here’s how to make a new column for “genus”. The only values you want is “Iris”: iris$Genus &lt;- &#39;Iris&#39; Here’s how to combine two columns: iris$GenSpp &lt;- paste(iris$Genus, iris$Species, sep=&quot;_&quot;) Here’s how to change Species ‘versicolor’ to ‘versi’ in the GenSpp column using gsub: iris$GenSpp &lt;- gsub(&#39;versicolor&#39;, &#39;versi&#39;, iris$GenSpp ) ## looks for &#39;versicolor&#39; and replaces it with &#39;versi&#39; in the column iris$GenSpp sub() can be used for replacement but will only do 1 replacement and gsub() can also be used for replacement but with all matching instances. You can use gsub() to add genus name to species column (alternative to making new column and then pasting together). iris$GenSpp1 &lt;- gsub(&#39;.*^&#39;, &#39;Iris_&#39;, iris$Species) 3.1.2 Variables with the tidyverse Everything so far in terms of modifying or making variables have been done with base R functions. However, sometimes these functions are not very intuitive. Take for example this new variable I created from the iris dataset: iris$new_var &lt;-unique(mean(c(log10(iris$Petal.Width), sqrt(iris$Petal.Width+5)), na.rm =T)) While the new variable is random mess, the more important issue is whether or not you understand what I did? If you know what the functions do you may be able to figure it out but nonetheless it takes a longer time because you are forced to read the code inside out. This is where the ‘tidyverse’ metapackage comes in. This metapackage offers a variety of ways to modify and create new variables. One thing to note is that the functions that are used here are often slow to process when working with extremely large and complicated data. For the purpose of this class, we do not deal with such large high dimensional data and therefore tidyverse is more than capable of providing what we need at reliable and consistent speeds. However, if one were to start working with large data or what we call “big data” (e.g. ‘omics’ data, sequence data, large ecological monitoring datasets) then we advise you to get comfortable with base R functions as they are extremely fast but not as intuitive. library(tidyverse) # load package tidyverse (install if needed) ## Warning: package &#39;ggplot2&#39; was built under R version 4.3.3 ## Warning: package &#39;tidyr&#39; was built under R version 4.3.3 ## Warning: package &#39;readr&#39; was built under R version 4.3.3 ## Warning: package &#39;lubridate&#39; was built under R version 4.3.3 ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(viridis) # viridis package for color palates ## Warning: package &#39;viridis&#39; was built under R version 4.3.3 ## Loading required package: viridisLite data(iris) # reload iris to clear changes from above iris1 &lt;- as_tibble(iris) # load iris and convert to tibble glimpse(iris1) ## similar to str(), just glimpses data ## Rows: 150 ## Columns: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s… The mutate() function will allow you create and modify variables. Notice in this example we create all the new variables we had made in the previous section all within one chunk of code: iris1 &lt;- iris1 %&gt;% mutate(Plant = 1:length(Species), PetSep.Length = Petal.Length+Sepal.Length, lnPS.Len =log(PetSep.Length), Genus=&#39;Iris&#39;, GenSpp=gsub(&#39;.*^&#39;, &#39;Iris_&#39;, Species)) ## note that I am overwriting iris1. Overwrite data.frames/tibbles with caution The ‘summarize()’ function can calculate means, sd, min, max, etc. of a dataset: iris1 %&gt;% summarize(mean(Petal.Length)) ## mean of Petal.Length in dplyr ## # A tibble: 1 × 1 ## `mean(Petal.Length)` ## &lt;dbl&gt; ## 1 3.76 mean(iris1$Petal.Length) ## mean of Petal.Length in base R ## [1] 3.758 Producing one mean is fine, but often we want to summarize by groups to get means for different groups or categories. summarize() can do this no problem, with a few other commands. Here we summarize ‘Petal.Length’ by Species with both Tidyverse and base R: means_PetLen1 &lt;- iris1 %&gt;% group_by(Species) %&gt;% summarize(Petal.Length=mean(Petal.Length)) ## tidy code means_PetLen1 ## means are saved in this new tibble ## # A tibble: 3 × 2 ## Species Petal.Length ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 1.46 ## 2 versicolor 4.26 ## 3 virginica 5.55 means_PetLen2 &lt;- aggregate(Petal.Length~Species, FUN=&quot;mean&quot;, data=iris1) ## base R means_PetLen2 ## means are saved in this data.frame ## Species Petal.Length ## 1 setosa 1.462 ## 2 versicolor 4.260 ## 3 virginica 5.552 Here we summarize multiple variables by species use summarize_all(): means1 &lt;- iris1 %&gt;% select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, lnPS.Len, Species) %&gt;% group_by(Species) %&gt;% summarize_all(list(mean=mean,sd=sd,n=length)) means1 ## # A tibble: 3 × 16 ## Species Sepal.Length_mean Sepal.Width_mean Petal.Length_mean Petal.Width_mean ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versico… 5.94 2.77 4.26 1.33 ## 3 virgini… 6.59 2.97 5.55 2.03 ## # ℹ 11 more variables: lnPS.Len_mean &lt;dbl&gt;, Sepal.Length_sd &lt;dbl&gt;, ## # Sepal.Width_sd &lt;dbl&gt;, Petal.Length_sd &lt;dbl&gt;, Petal.Width_sd &lt;dbl&gt;, ## # lnPS.Len_sd &lt;dbl&gt;, Sepal.Length_n &lt;int&gt;, Sepal.Width_n &lt;int&gt;, ## # Petal.Length_n &lt;int&gt;, Petal.Width_n &lt;int&gt;, lnPS.Len_n &lt;int&gt; 3.1.3 Reshape data for better usability Sometimes the data we are working with needs to be completely reorganized. For example you may have a dataframe containing ecological community data where you have different plots as rows and different species as columns. Within this hypothetical dataframe you have different abundances of species by site. Something like this: spxsite&lt;-data.frame(Site_ID = 1:20, sp1 = round(runif(1:20)), sp2 = round(runif(1:20)),sp3 = round(runif(1:20))) head(spxsite) ## Site_ID sp1 sp2 sp3 ## 1 1 0 1 0 ## 2 2 1 0 1 ## 3 3 0 0 0 ## 4 4 0 0 1 ## 5 5 0 1 0 ## 6 6 0 0 1 But you may want to reorganize the data so that you had a total count of species for each site in one column. In other words, we want to reshape from “wide” format to “long” format. We can do this using the pivot_longer function: spxsite %&gt;% pivot_longer( cols = c(sp1, sp2, sp3), names_to = &quot;Species&quot;, values_to = &quot;Occurrence&quot;) ## # A tibble: 60 × 3 ## Site_ID Species Occurrence ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 sp1 0 ## 2 1 sp2 1 ## 3 1 sp3 0 ## 4 2 sp1 1 ## 5 2 sp2 0 ## 6 2 sp3 1 ## 7 3 sp1 0 ## 8 3 sp2 0 ## 9 3 sp3 0 ## 10 4 sp1 0 ## # ℹ 50 more rows Ta da! Very simple to reshape. Here is how we reshape the iris data from wide to long: iris_long &lt;- iris1 %&gt;% group_by(Species) %&gt;% pivot_longer(cols=c(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, lnPS.Len), names_to = &#39;Trait&#39;, values_to = &#39;value&#39;) head(iris_long) ## # A tibble: 6 × 7 ## # Groups: Species [1] ## Species Plant PetSep.Length Genus GenSpp Trait value ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 setosa 1 6.5 Iris Iris_setosa Sepal.Length 5.1 ## 2 setosa 1 6.5 Iris Iris_setosa Sepal.Width 3.5 ## 3 setosa 1 6.5 Iris Iris_setosa Petal.Length 1.4 ## 4 setosa 1 6.5 Iris Iris_setosa Petal.Width 0.2 ## 5 setosa 1 6.5 Iris Iris_setosa lnPS.Len 1.87 ## 6 setosa 2 6.3 Iris Iris_setosa Sepal.Length 4.9 With this new form of data we can calculate the mean, sd, and n (sample size) for each Species by trait combination and then calculate the standard error (se). Note that we need to use the group_by() function to tell R how we want the data to be grouped. means2 &lt;- iris_long %&gt;% group_by(Species,Trait) %&gt;% summarize(mean=mean(value), sd=sd(value), n=length(value)) %&gt;% mutate(se=sd/sqrt(n)) %&gt;% filter(Trait!=&#39;lnPS.Len&#39;) ## `summarise()` has grouped output by &#39;Species&#39;. You can override using the ## `.groups` argument. head(means2) ## # A tibble: 6 × 6 ## # Groups: Species [2] ## Species Trait mean sd n se ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 setosa Petal.Length 1.46 0.174 50 0.0246 ## 2 setosa Petal.Width 0.246 0.105 50 0.0149 ## 3 setosa Sepal.Length 5.01 0.352 50 0.0498 ## 4 setosa Sepal.Width 3.43 0.379 50 0.0536 ## 5 versicolor Petal.Length 4.26 0.470 50 0.0665 ## 6 versicolor Petal.Width 1.33 0.198 50 0.0280 Note that all the previous code could all be done in one piped command: means2a &lt;- iris1 %&gt;% group_by(Species) %&gt;% pivot_longer(cols=c(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, lnPS.Len), names_to = &#39;Trait&#39;, values_to = &#39;value&#39;) %&gt;% group_by(Species,Trait) %&gt;% summarize(mean=mean(value), sd=sd(value), n=length(value)) %&gt;% mutate(se=sd/sqrt(n)) %&gt;% filter(Trait!=&#39;lnPS.Len&#39;) ## `summarise()` has grouped output by &#39;Species&#39;. You can override using the ## `.groups` argument. means2a ## # A tibble: 12 × 6 ## # Groups: Species [3] ## Species Trait mean sd n se ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 setosa Petal.Length 1.46 0.174 50 0.0246 ## 2 setosa Petal.Width 0.246 0.105 50 0.0149 ## 3 setosa Sepal.Length 5.01 0.352 50 0.0498 ## 4 setosa Sepal.Width 3.43 0.379 50 0.0536 ## 5 versicolor Petal.Length 4.26 0.470 50 0.0665 ## 6 versicolor Petal.Width 1.33 0.198 50 0.0280 ## 7 versicolor Sepal.Length 5.94 0.516 50 0.0730 ## 8 versicolor Sepal.Width 2.77 0.314 50 0.0444 ## 9 virginica Petal.Length 5.55 0.552 50 0.0780 ## 10 virginica Petal.Width 2.03 0.275 50 0.0388 ## 11 virginica Sepal.Length 6.59 0.636 50 0.0899 ## 12 virginica Sepal.Width 2.97 0.322 50 0.0456 Before getting further into this, notice the code written above. It is by far easier to read and understand what is being done to the data compared to base R functions that are wrapped within one another! This improves our ability to communicate code and more importantly develop REPRODUCIBLE analytical pipelines. It’s important that your code is not only legible to you but to others as well. We can make plots (and do other things) not possible without reshaping and summarizing the data. Below are two plots to start with. Note that one is more effective than the other, but demonstrate two ways you can plot the data using the summarized and long-format data, respectively. ggplot(data=means2, aes(x=Species, y=mean, fill=Trait)) + geom_point(size=5, position=position_dodge(width=0.25), pch=22) + labs(y=&quot;Floral part measurement (mm)&quot;) + geom_errorbar(aes(ymin=(mean-sd), ymax=(mean+sd)), width=.2, position=position_dodge(width=0.25), lwd=1.5) + scale_fill_viridis(discrete = T, labels=c(&quot;Petal Length&quot;,&quot;Petal Width&quot;, &quot;Sepal Length&quot;, &quot;Sepal Width&quot;), option=&quot;magma&quot;) + theme(panel.border=element_rect(color=&quot;black&quot;,size=2, fill=NA)) + xlab(&quot;Species&quot;) ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 3.1: Means and SE for species and floral traits made with ggplot ggplot(data=iris_long %&gt;% filter(Trait!=&#39;lnPS.Len&#39;), aes(x=Species, y=value, fill=Species)) + geom_boxplot() + facet_wrap(~Trait, scales = &#39;free_y&#39;) + labs(y=&quot;Floral part measurement (mm)&quot;) + scale_fill_viridis(discrete = T, option = &quot;plasma&quot;, direction = -1, begin=.2) + theme_bw() Figure 2.20: Means and SE for species and floral traits made with ggplot Which of the above plots do you think is more effective in conveying data and summary statistics? 3.2 Data Exploration For data exploration we need to load the following libraries: library(tidyverse) library(agridat) ## Warning: package &#39;agridat&#39; was built under R version 4.3.3 library(corrplot) ## Warning: package &#39;corrplot&#39; was built under R version 4.3.3 ## corrplot 0.95 loaded library(EnvStats) ## Warning: package &#39;EnvStats&#39; was built under R version 4.3.3 ## ## Attaching package: &#39;EnvStats&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## predict, predict.lm Data exploration is an important part of cleaning, understanding, and dealing with your data. Data exploration encompasses a variety of processes that often revolve around visualization and bigger picture perspectives of the data you are working with. It’s also a useful time to identify key patterns in your dataset that may be interesting in later analyses or highlight bias in your dataset. We will still use the Iris dataset for our data exploration. data(iris) # load data (already exists in base R) iris[8,3] &lt;- 7 # plant data point for demo head(iris) # print first 6 lines of dataset ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa tail(iris) # print last 6 lines of dataset ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica Here we use str() to print the ‘structure’ of dataset giving you info about each column as shown earlier. There is also another function, glimpse() that is the tidyverse equivalent of str(). str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 7 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... glimpse(iris) # glimpse is similar to str() in tidyverse ## Rows: 150 ## Columns: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 7.0, 1.4, 1.5, 1.5, 1.… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s… 3.2.1 Distributions &amp; summary statistics Distribution of data are important to visualize and understand in your data sets. For example, these distributions can help inform the types of models you may need to rely on or can also help identify distinct clustering of different values based on different categories. We can view histograms of petal lengths and also calculate where the mean and median of this distribution are: ggplot(iris, aes(x = Petal.Length)) + geom_histogram(bins=12, color=&quot;white&quot;) + theme_bw(base_size = 16) + geom_vline(aes(xintercept = mean(Petal.Length)), color = &quot;blue&quot;, size = 2) + geom_vline(aes(xintercept= median(Petal.Length)), color = &quot;orange&quot;, size = 2) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 2.25: Histogram of petal length with mean and median shown We can also facet the histograms: ggplot(iris, aes(x = Petal.Length)) + geom_histogram(bins=12, color=&quot;white&quot;) + facet_wrap(~Species, scales=&quot;free&quot;) + theme_bw(base_size = 16) + labs(x = &quot;Petal Length&quot;, y = &quot;Count&quot;) Figure 2.26: Histogram of petal length with mean and median shown Or we can put them all in one plot and split them by color, this would allow for a more easier and quicker evaluation of how the different species stack up to one another. ggplot(iris, aes(x = Petal.Length)) + geom_histogram(bins=12, color=&quot;white&quot;, aes(fill = Species, color = Species)) + theme_bw(base_size = 16) + labs(x = &quot;Petal Length&quot;, y = &quot;Count&quot;) Figure 2.27: Histogram of petal length with mean and median shown We can use summary() to examine mean, median, and ranges of each of the columns: summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.400 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.795 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :7.000 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## If we need a table of these summary stats then we can also get a table of means and medians with this tidyverse code: iris %&gt;% pivot_longer(cols=c(1:4)) %&gt;% group_by(Species,name) %&gt;% summarize(mean=mean(value),median=median(value)) ## `summarise()` has grouped output by &#39;Species&#39;. You can override using the ## `.groups` argument. ## # A tibble: 12 × 4 ## # Groups: Species [3] ## Species name mean median ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa Petal.Length 1.57 1.5 ## 2 setosa Petal.Width 0.246 0.2 ## 3 setosa Sepal.Length 5.01 5 ## 4 setosa Sepal.Width 3.43 3.4 ## 5 versicolor Petal.Length 4.26 4.35 ## 6 versicolor Petal.Width 1.33 1.3 ## 7 versicolor Sepal.Length 5.94 5.9 ## 8 versicolor Sepal.Width 2.77 2.8 ## 9 virginica Petal.Length 5.55 5.55 ## 10 virginica Petal.Width 2.03 2 ## 11 virginica Sepal.Length 6.59 6.5 ## 12 virginica Sepal.Width 2.97 3 3.2.2 Examining for outliers Outliers can cause a lot of distortion in the data as well as any downstream analyses. Therefore, the ability to quickly identify and potentially remove outliers from a data set is important. Boxplots can be used to examine distribution and look for outliers: ggplot(iris, aes(x=Species, y = Petal.Length)) + geom_boxplot(fill=&quot;grey&quot;, width=.5) + facet_wrap(~Species, scales=&quot;free&quot;) + theme_bw(base_size = 16) Figure 2.30: Boxplot of petal length by species We can use a Dixon test for outliers or other tests like the grubbs test or Rosner test (for multiple outliers): library(outliers) ## grubbs test for outliers, highest then lowest. Other functions EnvStats::rosnerTest() can test for multiple outliers grubbs.test(iris$Petal.Length) ## full dataset ## ## Grubbs test for one outlier ## ## data: iris$Petal.Length ## G = 1.80564, U = 0.97797, p-value = 1 ## alternative hypothesis: highest value 7 is an outlier grubbs.test(iris$Petal.Length[iris$Species==&#39;setosa&#39;]) ## just species setosa ## ## Grubbs test for one outlier ## ## data: iris$Petal.Length[iris$Species == &quot;setosa&quot;] ## G = 6.765525, U = 0.046807, p-value &lt; 2.2e-16 ## alternative hypothesis: highest value 7 is an outlier grubbs.test(iris$Petal.Length[iris$Species==&#39;setosa&#39;], opposite=T) ## test lower outlier for species setosa ## ## Grubbs test for one outlier ## ## data: iris$Petal.Length[iris$Species == &quot;setosa&quot;] ## G = 0.71295, U = 0.98941, p-value = 1 ## alternative hypothesis: lowest value 1 is an outlier Here we can remove outliers and remake boxplots. We can remove data entries where the species was ‘setosa’ and the petal length was less than four. Filtering with | (OR) will select all observations where one condition is met but not the other. iris1 &lt;- iris %&gt;% filter(Petal.Length&lt;4 | !Species==&#39;setosa&#39;) Plotting data: ggplot(iris1, aes(x=Species, y = Petal.Length)) + geom_boxplot(fill=&quot;grey&quot;, width=.5) + facet_wrap(~Species, scales=&quot;free&quot;) + theme_bw(base_size = 16) Figure 2.34: Boxplot of petal length by species with outlier removed Not the usage of scales = 'free' in the code above. What this does is it allows the scales to vary across the facetted variables, in this case the three different types of species. When we don’t call upon this in the code, all the species will be on the same scale, in this case y - axis. While this may make things more comparable across facets, it will make it more difficult to examine specific species’ distributions: ggplot(iris1, aes(x=Species, y = Petal.Length)) + geom_boxplot(fill=&quot;grey&quot;, width=.5) + facet_wrap(~Species) + theme_bw(base_size = 16) Figure 3.2: Boxplot of petal length by species with outlier removed 3.2.3 Exploring relationships in data Correlations and trends across different variables in a data set are important to explore because they allow us to intuitively understand how our data is structured; e.g. which variables are so alike relative to other variables. Yet plotting multiple scatter plots of pairwise comparisons across all variables in a data set is exhausting. For example, below we plot three different pairwise comparisons with the iris data. data(iris) plot(iris$Sepal.Length, iris$Sepal.Width) Figure 3.3: Three scatter plots plot(iris$Sepal.Length, iris$Petal.Length) Figure 3.4: Three scatter plots plot(iris$Sepal.Length, iris$Petal.Width) Figure 3.5: Three scatter plots What if we could examine these plots with a few lines of code and what if we could get all the plots onto one plot panel? We can first use the GGally package. The ggpairs() code provides us with scatter plots that plot variables against one another in a pairwise fashion. We also see the distribution of the data and the correlation coefficients between a pair of variables. library(GGally) ## install and load GGally package, if necessary ## Warning: package &#39;GGally&#39; was built under R version 4.3.3 ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ggpairs(iris1) ## Make a big panel plot for exploration!!! Figure 3.6: ggpairs plots ggpairs(iris1, aes(color=Species, alpha=.75)) ## add color to seperate by species Figure 3.7: ggpairs plots Alternative to ggpairs() is the cor() function which can be even more effective for quickly scanning complex datasets: library(corrplot) iris_cor &lt;- cor(iris1 %&gt;% select(-Species) %&gt;% as.matrix()) ## first make correlation matrix corrplot(iris_cor, method = &quot;circle&quot;, type = &quot;upper&quot;) ## plots strength of correlation as color-coded circles Figure 3.8: corrplot matrix 3.3 Model Selection This section will include a quick overview on how to rank models based on AIC (Akaike Information Criterion). For this we need to load a few libraries: library(glmmTMB) ## Warning: package &#39;glmmTMB&#39; was built under R version 4.3.3 ## Warning in check_dep_version(): ABI version mismatch: ## lme4 was built with Matrix ABI version 1 ## Current Matrix ABI version is 0 ## Please re-install lme4 from source or restore original &#39;Matrix&#39; package library(bbmle) ## Loading required package: stats4 ## ## Attaching package: &#39;bbmle&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice library(viridis) 3.3.1 Quick introduction Model selection is the process in choosing a statistical model from a set of models given your data. Keep in mind that model selection can get messy and its implementation (including approaches and methods) is often debated. Model selection can be used in the context of exploration, inference, and prediction. For today’s section we focus a widely-used inferential approach with AIC. 3.3.2 Basic calculations AIC was developed by Hirotogu Akaike and is a metric of relative model quality for a given set of data. AIC is calculated using the following formula: \\[ AIC = -2 * ln(model\\ likelihood) + 2K \\] K = number of parameters in the model Note: your data and your response variable have to be the same for AIC selection! We can manually calculate AIC in R. Let’s do this with an example dataset. We will use the mtcars dataset, which is already loaded into base R: data(mtcars) ?mtcars ## starting httpd help server ... done Let’s run a regression between miles per gallon (mpg) and horsepower (hp): m1&lt;-glmmTMB(data = mtcars, mpg ~ hp) Now that we ran the model, saved as m1 let’s calculate the AIC by hand: AIC = 2*3 - 2*logLik(m1) AIC[1] ## [1] 181.2386 Note: K = 3 in this case because the model ‘m1’ has 3 degrees of freedom which can be checked when running the logLik() as seen here: logLik(m1) ## &#39;log Lik.&#39; -87.61931 (df=3) Now let’s see what the regular AIC() gives us: AIC(m1) ## [1] 181.2386 Looks like our calculations are the same! So why use AIC? The lectures that follow along this section provides more detail but to quickly sum it up: AIC is commonly used and accepted Works well within a multiple competing hypothesis framework A step forward in getting away from the p-value problem 3.3.3 AICc Small sample sizes can bias AIC to select for models with too many parameters. To account for this AICc (Akaike Information Criterion Corrected for Small Sample Sizes) was developed and the equation for that goes as follows: \\[ AICc = AIC + \\frac{2K^2 + 2K}{n - K -1} \\] Where: n = sample size K = no. of parameters 3.3.4 Case study with AIC Let’s load in some beetle size data: df&lt;-readRDS(&quot;size_data.rds&quot;) This data includes global coverage of average beetle sizes (geo_avg). The cell number represents an individual hexagonal bin and within each bin the average beetle size (based on species lists), species richness (SR), average temperature (MAT), temperature range (ATR), and net primary productivity (NPP_men) was calculated. glimpse(df) ## Rows: 309 ## Columns: 6 ## $ cell &lt;chr&gt; &quot;1&quot;, &quot;9&quot;, &quot;10&quot;, &quot;12&quot;, &quot;13&quot;, &quot;22&quot;, &quot;23&quot;, &quot;31&quot;, &quot;32&quot;, &quot;33&quot;, &quot;41&quot;… ## $ SR &lt;int&gt; 56, 7, 50, 15, 22, 19, 18, 3, 90, 13, 66, 91, 5, 134, 160, 88,… ## $ geo_avg &lt;dbl&gt; 1.0912129, 1.6880459, 1.2090465, 1.2123414, 1.0968534, 1.12845… ## $ MAT &lt;dbl&gt; 4.7616038, 1.1910487, 0.9827181, -0.6845881, -7.2203846, -3.78… ## $ NPP_men &lt;dbl&gt; 550.4055, 332.4632, 420.8561, 418.0304, 287.1639, 330.6352, 36… ## $ ATR &lt;dbl&gt; 26.58891, 23.71292, 29.55614, 34.34307, 46.19601, 38.76278, 48… If we wanted to plot the data to show the average beetle size across the planet it would like this: It seems like there’s a weak but detectable trend of larger sized assemblages in northern/temperate areas. Although the continent of Australia really pops up as well. What could be causing this pattern? Based on the literature there are several hypotheses we could test with models and AIC! They are as follows: Larger assemblages are just an artifact of undersampling. Sampling is more extensive in North America and Europe and therefore, size is simply a function of sampling. A proxy to measure sampling could be species richness. sample&lt;-glmmTMB(log(geo_avg) ~ SR, data = df) Areas that experience harsher environments (larger ranges in temperature) likely have larger sized organisms that can weather such conditions. seasonality&lt;-glmmTMB(log(geo_avg) ~ ATR, data = df) Areas that are colder have larger organisms due to thermoregulatory properties where larger organisms are able to trap and effectively conserve heat. TEMP&lt;-glmmTMB(log(geo_avg) ~ MAT, data = df) Resource availability. Organisms can grow larger in resource rich areas. NPP&lt;-glmmTMB(log(geo_avg) ~ NPP_men, data = df) Finally, there could be no statistical pattern and therefore there is no effect on size i.e. a statistical null. nullm&lt;-glmmTMB(log(geo_avg) ~ 1, data = df) Now let’s calculate the AIC scores of the models: bbmle::AICtab(sample, seasonality, TEMP, NPP, nullm, weights = T, delta = T, base = T) ## AIC dAIC df weight ## seasonality -124.1 0.0 3 1 ## TEMP -96.1 28.0 3 &lt;0.001 ## NPP -89.4 34.7 3 &lt;0.001 ## sample -89.3 34.8 3 &lt;0.001 ## nullm -73.2 50.9 2 &lt;0.001 The seasonality hypothesis seems like the best or most likely model while the null and the sampling effect models show the least support. We also see that the next closest model is the temperature model but is beyond a \\(\\Delta\\) AIC difference of 2. All in all we show strong support for the seasonality hypothesis. We also see another metric labelled as “weight”. The AIC weight represents the probability that the model is best from the set of competing models and it’s another metric that can be used to assess models other than raw AIC scores. We can make a nice plot with this data along a seasonality axis: ggplot(data = df) + geom_point(aes(x = ATR, y = geo_avg, fill = log(SR)), pch = 21, size = 5, color = &#39;grey&#39;, alpha = 0.8) + geom_rug(aes(x = ATR, y = geo_avg)) + scale_fill_viridis_c() + scale_y_log10() + labs(x = &quot;Annual Temp. Range&quot;, y = &quot;Size&quot;, fill = &quot;Log(SR)&quot;) + geom_smooth(aes(x = ATR, y = geo_avg), method = &quot;lm&quot;, color = &quot;black&quot;) + ylim(c(0, 2)) + theme_minimal() + theme(axis.title = element_text(size = 22, face = &#39;bold&#39;), legend.title = element_text(size = 15, face = &#39;bold&#39;), legend.position = &quot;top&quot;) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 3.9: Average beetle size plotted against annual temperature range "],["generalized-linear-models.html", "4 Generalized Linear Models 4.1 Overview of Generalized Linear Models 4.2 Overdispersion 4.3 Binomial GLM 4.4 Gamma GLM 4.5 Briefly running Gamma GLMs", " 4 Generalized Linear Models After completing this module, students will be able to: 4.1 Describe common statistical distributions and types of data they generate 4.2 Analyze data using non-normal distributions in R 4.3 Evaluate and justify model fit 4.4 Interpret and report results from statistical analysis 4.1 Overview of Generalized Linear Models Generalized linear models (GLMs) are a class of linear-based regression models developed to handle varying types of error distributions. These class of models are extremely useful for data types that may not conform to what is typically expected given Gaussian expectations or assumptions. For example, data that is binary (e.g. 0 or 1, alive or dead) or count data that is never negative all have different properties. While data transformations prior to model implementation can be done, it may be difficult to interpret results or may not help in meeting the assumptions of the model. In this section we first start with basic data transformations that can be applied to ordinary linear models and then move into different types of GLMs. library(tidyverse) library(emmeans) library(car) library(agridat) library(glmmTMB) library(viridis) 4.1.1 Example: One-way ANOVA with non-normal data In this example we will run an ANOVA with non-normal data. To do this we first must load the InsectSprays data: data(&quot;InsectSprays&quot;)# available from base R Now we need to filter the data to just 4 treatments (A, B, C, F): d &lt;- InsectSprays %&gt;% filter(spray==&#39;A&#39;|spray==&#39;B&#39;|spray==&#39;C&#39;|spray==&#39;F&#39;) %&gt;% droplevels() Let’s plot out data: ggplot(d, aes(x=spray,y=count)) + geom_boxplot(outlier.shape = NA, width = 0.5) + geom_jitter(height=0,width=.1, size = 3, pch = 21) + labs(x = &quot;Spray Treatment Type&quot;, y = &quot;Count&quot;) + theme_bw() + theme(axis.title = element_text(face= &#39;bold&#39;, size = 15), axis.text = element_text(face= &#39;bold&#39;, size = 15)) Figure 2.1: Insect count data by spray type Let’s examine a histogram of the response variable ‘count’. hist(d$count) Figure 2.2: Histogram of insect counts. The distribution is not visually normal but remember, with linear models the assumption of normality is not necessarily with the response variable but is instead about the errors (residuals of the model). Therefore, this histogram can hint of what sort of transformations we may want to use to have a model that better meets assumptions. While not ideal let’s construct a linear model to examine the effect of the different sprays on insect counts without executing any data transformations or applying fancy GLMs: lm1 &lt;- glmmTMB(count~spray, data=d) Anova(lm1, type=2) ## car::Anova will print out an ANOVA table ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: count ## Chisq Df Pr(&gt;Chisq) ## spray 86.656 3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s check the residuals of the model: hist(resid(lm1)) #&#39; residuals should be normally distributed, even for glm Figure 4.1: Residual plots. plot(resid(lm1)~fitted(lm1)) ## residuals should be evenly dispersed around 0 across the range of x&#39;s abline(h=0) # funnel shapes or curvature is bad Figure 4.2: Residual plots. qqPlot(resid(lm1)) ## residuals should line up pretty closely to the blue line Figure 2.5: QQ plot ## [1] 45 46 Normality of the residuals would pass an eye check. However when we look at the variance of the residuals across spray treatment types we see evidence of violation where the variances are not necessarily homogeneous for each group: boxplot(resid(lm1) ~ d$spray) ## variances should be homogeneous for each group Figure 4.3: Boxplot of residuals to check for heterogeneity. Let’s use emmeans to calculate model means and confidence intervals. emmeans(lm1, ~spray) ## spray emmean SE df lower.CL upper.CL ## A 14.50 1.26 43 11.961 17.04 ## B 15.33 1.26 43 12.794 17.87 ## C 2.08 1.26 43 -0.456 4.62 ## F 16.67 1.26 43 14.127 19.21 ## ## Confidence level used: 0.95 Note all the standard error estimates are the same and the lower CL can be negative (although we know that counts can never be negative). Is this ok? 4.1.2 Log-linear model Now let’s use a log-linear model to examine the effect of the different sprays on insect counts. To implement a log-linear model we can log transform the counts with the log() function. Note that the this function applies a natural logarithmic transformation to the specified variable or vector. If you want to use a base 10 logarithmic transformation then the correct function to use would be log10. Notice add 1 to the variable before the transformation because log(0) is undefined. You could add any small constant (e.g. 0.01 or 0.0001) but +1 is convenient because zeros go back to zero after the transformation. log(0) ## [1] -Inf log(1) ## [1] 0 lm2 &lt;- glmmTMB(log(count+1)~spray, data=d) Anova(lm2, type=2) ## car::Anova will print out an ANOVA table testing ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: log(count + 1) ## Chisq Df Pr(&gt;Chisq) ## spray 185.5 3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s check residuals: hist(resid(lm2)) ## residuals should be normally distributed, even for glm Figure 4.4: Residual plots. plot(resid(lm2)~fitted(lm2)) + ## residuals should be evenly dispersed around 0 across the range of x&#39;s abline(h=0) # funnel shapes or curvature is bad Figure 4.5: Residual plots. ## integer(0) Let’s plot the QQplot and boxplots next: qqPlot(resid(lm2)) ## residuals should line up pretty closely to the blue line Figure 4.6: QQ and box plots. ## [1] 27 25 boxplot(resid(lm2) ~ d$spray) ## variances should be homogeneous for each group Figure 4.7: QQ and box plots. We can see above that the transformation makes the homogeneity of the residuals a little but more constant and consistent across treatment types. This seems to help especially treatment C relative to the model without the log transformation. However, the residuals of treatment C are still highly variable compared to treatments A, B, and C. Let’s use emmeans again: emmeans(lm2, ~spray) ## note that all means still transformd to be on the log-scale ## spray emmean SE df lower.CL upper.CL ## A 2.697 0.115 43 2.465 2.93 ## B 2.757 0.115 43 2.525 2.99 ## C 0.953 0.115 43 0.721 1.18 ## F 2.816 0.115 43 2.584 3.05 ## ## Results are given on the log(mu + 1) (not the response) scale. ## Confidence level used: 0.95 Note that the means are now on the log-transformed scale, as pointed out at the bottom on the emmeans table. Usually we want to back-transform the means to the original scale so they are easier to interpret. To calculate back-transformed means with ‘emmeans’ we can add additional arguments to the function: emmeans(lm2, ~spray, type=&#39;response&#39;) ## note that now all means are back-transformed ## spray response SE df lower.CL upper.CL ## A 13.83 1.700 43 10.76 17.70 ## B 14.75 1.810 43 11.49 18.85 ## C 1.59 0.298 43 1.06 2.27 ## F 15.70 1.920 43 12.25 20.06 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log(mu + 1) scale Now notice how the means columns has been relabeled response and is in the same units that we originally started with. 4.1.3 Generalized linear models (GLMs) Now let’s use GLMs to examine the effect of the different sprays: glm1 &lt;- glmmTMB(count~spray, data=d, family=&#39;poisson&#39;) The nice thing about using glmmTMB() is that it is a general function that conducts can a generalized linear model. To do this, we simply specify the ‘family’ (aka error distribution). The default is the ‘Gaussian’ distribution (normal), so if we don’t specify a family it run a simple linear model. In the example above we implement a Poisson distribution, a distribution that is frequently used with count data. All the model “calculations” are saved in an object we called ‘glm1’. An alternative to the glmmTMB() function is glm() which is available in base R. Anova(glm1, type=2) ## car::Anova will print out an ANOVA table testing ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: count ## Chisq Df Pr(&gt;Chisq) ## spray 98.376 3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For the ANOVA table above, the null hypothesis that all group means are equal. The argument, type = 2, provides margin tests, which is usually better than the default Type I, especially for more complicated models (See 2.3.1 Box 3 for additional information about the type 2 method). For GLMs, Anova returns a likelihood ratio test with a chi-sq value. summary(glm1) ## summary() will provide the model coefficients (ie. the &quot;guts&quot; of the model) ## Family: poisson ( log ) ## Formula: count ~ spray ## Data: d ## ## AIC BIC logLik deviance df.resid ## 273.9 281.4 -133.0 265.9 44 ## ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.67415 0.07581 35.27 &lt;2e-16 *** ## sprayB 0.05588 0.10574 0.53 0.597 ## sprayC -1.94018 0.21389 -9.07 &lt;2e-16 *** ## sprayF 0.13926 0.10367 1.34 0.179 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The coefficients allow you rebuild the means from the linear model, just like we did in 2.3. In this case, rebuilding the model from the coefficients is not super helpful because they are still on the log-scale and the p-values aren’t as meaningful. Residual deviance should be about equal to the degrees of freedom. More than twice as high is problematic (note: we will come back to this problem when we discuss “overdispersion” in the next section). Now let’s check assumptions of model by examining residuals: hist(resid(glm1)) ## residuals should be normally distributed, but don&#39;t need to be for GLMs Figure 4.8: Residual plots. plot(resid(glm1)~fitted(glm1)) ## residuals should be evenly dispersed around 0 across the range of x&#39;s abline(h=0) # funnel shapes or curvature is bad Figure 4.9: Residual plots. qqPlot(resid(glm1)) ## calls from car package, residuals should line up pretty closely to the blue line Figure 4.10: QQ and box plots. ## [1] 45 46 # points that drift from line might be outliers boxplot(resid(glm1) ~ d$spray) ## variances should be homogeneous for each group Figure 4.11: QQ and box plots. Above, we see further improvement in the residual variances across treatment types. Diagnosing complex GLMs can be very difficult. Residuals are often NOT NORMALLY DISTRIBUTED, even though they should be. We will return to this later. emmeans(glm1, ~spray) ## emmeans::emmmeans will rebuild the model for you ## spray emmean SE df asymp.LCL asymp.UCL ## A 2.674 0.0758 Inf 2.526 2.82 ## B 2.730 0.0737 Inf 2.586 2.87 ## C 0.734 0.2000 Inf 0.342 1.13 ## F 2.813 0.0707 Inf 2.675 2.95 ## ## Results are given on the log (not the response) scale. ## Confidence level used: 0.95 The emmeans code above will print off the means, SE, and confidence intervals for each treatment group. Note, the coefficients are on the log-scale (look at model specifications of glm1 object). We can also use emmeans() to make pairwise comparisons to directly compare each spray to the others. emmeans(glm1, pairwise~spray, type=&#39;response&#39;) ## adding &#39;pairwise&#39; will conduct pairwise contrasts -- ie. compare each group mean to the others ## $emmeans ## spray rate SE df asymp.LCL asymp.UCL ## A 14.50 1.100 Inf 12.50 16.82 ## B 15.33 1.130 Inf 13.27 17.72 ## C 2.08 0.417 Inf 1.41 3.08 ## F 16.67 1.180 Inf 14.51 19.14 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale ## ## $contrasts ## contrast ratio SE df null z.ratio p.value ## A / B 0.946 0.1000 Inf 1 -0.528 0.9522 ## A / C 6.960 1.4900 Inf 1 9.071 &lt;.0001 ## A / F 0.870 0.0902 Inf 1 -1.343 0.5352 ## B / C 7.360 1.5700 Inf 1 9.364 &lt;.0001 ## B / F 0.920 0.0940 Inf 1 -0.816 0.8468 ## C / F 0.125 0.0265 Inf 1 -9.803 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 4 estimates ## Tests are performed on the log scale Adding the argument ‘pairwise’ will conduct pairwise contrasts – ie. compare each group mean to the others. This automatically adjusts p-values using the ‘tukey’ adjust. This can be changed using ‘adjust=XX’ within the emmeans() function. The type=‘response’ will back-transform (i.e. in this case, exponentiate) to the original scale Let’s compare residuals for normal, log-transformed, and poisson models: par(mfrow=c(1,3)) #allow plots to be shown in three boxplot(resid(lm1) ~ d$spray, main = &quot;Linear&quot;) boxplot(resid(lm2) ~ d$spray, main = &quot;Log-Linear&quot;) boxplot(resid(glm1) ~ d$spray, main = &quot;GLM&quot;) Figure 2.19: Residual plots for all models. par(mfrow=c(1,1)) #return to default Let’s also compare means for normal, log-transformed, and poisson models: emmeans(lm1, ~spray) ## spray emmean SE df lower.CL upper.CL ## A 14.50 1.26 43 11.961 17.04 ## B 15.33 1.26 43 12.794 17.87 ## C 2.08 1.26 43 -0.456 4.62 ## F 16.67 1.26 43 14.127 19.21 ## ## Confidence level used: 0.95 emmeans(lm2, ~spray, type=&#39;response&#39;) ## spray response SE df lower.CL upper.CL ## A 13.83 1.700 43 10.76 17.70 ## B 14.75 1.810 43 11.49 18.85 ## C 1.59 0.298 43 1.06 2.27 ## F 15.70 1.920 43 12.25 20.06 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log(mu + 1) scale emmeans(glm1, ~spray, type=&#39;response&#39;) ## spray rate SE df asymp.LCL asymp.UCL ## A 14.50 1.100 Inf 12.50 16.82 ## B 15.33 1.130 Inf 13.27 17.72 ## C 2.08 0.417 Inf 1.41 3.08 ## F 16.67 1.180 Inf 14.51 19.14 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale What model do you think we should go with? 4.2 Overdispersion In this section we will go over how to deal with overdispersion. While overdispersion is covered more extensively in the lecture portion of the class, we will quickly outline what it is. Overdispersion is when variation is higher than what would be expected. A great example of how overdispersion would biologically come about in a given dataset is found from this website. To summarize it briefly, imagine if you are collecting tree seedlings within a forest. These seedlings are naturally not uniformly distributed throughout the forest area. Instead they are more likely clumped, where most plots don’t have any seedlings but a few have a ton. Therefore your counts of seedlings for a given plot can vary from 0 to numbers that are extremely high. This sort of variation can cause overdispersion to be observed in the model if it is not appropriately accounted for. Let’s load the necessary libraries: library(tidyverse) library(emmeans) library(car) library(agridat) library(glmmTMB) Load in and read about the beall.webworms dataset. The variables of interest are the y=count of webworms, spray = spray treatment, and lead = lead pesticide treatment. Don’t worry about the block or other variables for now. data(&quot;beall.webworms&quot;) d1 &lt;- beall.webworms ?beall.webworms ## starting httpd help server ... done head(d1) ## row col y block trt spray lead ## 1 1 1 1 B1 T1 N N ## 2 2 1 0 B1 T1 N N ## 3 3 1 1 B1 T1 N N ## 4 4 1 3 B1 T1 N N ## 5 5 1 6 B1 T1 N N ## 6 6 1 0 B2 T1 N N Let’s examine and plot the data: ggplot(d1, aes(x=spray, y=y, fill=lead)) + geom_violin(scale=&quot;width&quot;, adjust=2) + geom_point(position = position_jitterdodge(jitter.width=.5, jitter.height=.1, dodge.width = 1), alpha=.1) + theme_bw(base_size = 14) Figure 2.23: Plot of webworm data. Let’s now run a model with the interaction of spray and lead. Its count data, so lets assume a Poisson distribution. r3 &lt;- glm(y ~ spray * lead, data=d1, family=&quot;poisson&quot;) Examine the model summary: summary(r3) ## ## Call: ## glm(formula = y ~ spray * lead, family = &quot;poisson&quot;, data = d1) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.33647 0.04688 7.177 7.12e-13 *** ## sprayY -1.02043 0.09108 -11.204 &lt; 2e-16 *** ## leadY -0.49628 0.07621 -6.512 7.41e-11 *** ## sprayY:leadY 0.29425 0.13917 2.114 0.0345 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1955.9 on 1299 degrees of freedom ## Residual deviance: 1720.4 on 1296 degrees of freedom ## AIC: 3125.5 ## ## Number of Fisher Scoring iterations: 6 Anova(r3) ## Analysis of Deviance Table (Type II tests) ## ## Response: y ## LR Chisq Df Pr(&gt;Chisq) ## spray 188.707 1 &lt; 2.2e-16 *** ## lead 42.294 1 7.853e-11 *** ## spray:lead 4.452 1 0.03485 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 emmeans(r3, ~spray:lead, type=&#39;response&#39;) ## spray lead rate SE df asymp.LCL asymp.UCL ## N N 1.400 0.0656 Inf 1.277 1.535 ## Y N 0.505 0.0394 Inf 0.433 0.588 ## N Y 0.852 0.0512 Inf 0.758 0.959 ## Y Y 0.412 0.0356 Inf 0.348 0.488 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale We need to load library(performance) to test for overdispersion: library(performance) ## Warning: package &#39;performance&#39; was built under R version 4.3.3 check_overdispersion(r3) # overdispersion ratio calculator from performance package ## # Overdispersion test ## ## dispersion ratio = 1.355 ## Pearson&#39;s Chi-Squared = 1755.717 ## p-value = &lt; 0.001 ## Overdispersion detected. Note that there is overdispersion and the “dispersion ratio = 1.355”, which is (approximately) the Residual deviance over the degrees of freedom from the summary() printout. Now let’s implement the negative binomial distribution, which will account for the overdispersion in the data. r4 &lt;- glmmTMB(y ~ spray * lead, data=d1, family=&quot;nbinom2&quot;) Anova(r4) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: y ## Chisq Df Pr(&gt;Chisq) ## spray 125.5047 1 &lt; 2.2e-16 *** ## lead 26.8005 1 2.256e-07 *** ## spray:lead 3.3942 1 0.06542 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 emmeans(r4, ~spray:lead, type=&#39;response&#39;) ## spray lead response SE df asymp.LCL asymp.UCL ## N N 1.400 0.0855 Inf 1.242 1.578 ## Y N 0.505 0.0441 Inf 0.425 0.599 ## N Y 0.852 0.0611 Inf 0.741 0.981 ## Y Y 0.412 0.0391 Inf 0.342 0.497 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale Let’s use the DHARMa package to simulate residuals for poisson and negative binomial models. library(DHARMa) ## Warning: package &#39;DHARMa&#39; was built under R version 4.3.3 ## This is DHARMa 0.4.7. For overview type &#39;?DHARMa&#39;. For recent changes, type news(package = &#39;DHARMa&#39;) We can interpret the simulated residuals very similarly to the raw residuals we have previously examined. The residuals should line up along the line in the QQ plot and there should be (roughly) equal scatter in the residuals among the groups. First let’s look at simulated residuals from the Poisson model: simulateResiduals(r3, plot=T) ## plot simulated residuals ## DHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = &#39;bootstrap&#39;. See ?testOutliers for details ## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. ## ## Scaled residual values: 0.4574871 0.1249353 0.3765507 0.8898169 0.9947878 0.1384856 0.8427982 0.7401398 0.5451185 0.9247921 0.04867315 0.9158836 0.4210366 0.2450423 0.6110091 0.05695033 0.5344315 0.9367373 0.8666021 0.7861569 ... Histograms of the simulated residuals will be different than before. Here the simulated residuals should be flat. Its ok if the bars bump up and down, but they should be on average flat across the graph.The bars shouldn’t be peaked (eg. “normally” distributed) or U-shaped or increasing/decreasing. hist(simulateResiduals(r3)) ## histogram should be flat The residuals from the Poisson model look ok, but not perfect. The line in the QQ plot deviates from 1:1 and the variances are a little different among the groups. The histogram looks ok. Now let’s look at residuals for negative binomial model: simulateResiduals(r4, plot=T) ## plot simulated residuals ## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. ## ## Scaled residual values: 0.4821899 0.2591288 0.3967079 0.8858496 0.992 0.117201 0.7325008 0.6766148 0.4083655 0.8192085 0.10593 0.8988023 0.621979 0.3468156 0.7885132 0.2689479 0.657901 0.92479 0.7941977 0.8523115 ... simulateResiduals(r4, hist=T) ## histogram should be flat ## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. ## ## Scaled residual values: 0.4821899 0.2591288 0.3967079 0.8858496 0.992 0.117201 0.7325008 0.6766148 0.4083655 0.8192085 0.10593 0.8988023 0.621979 0.3468156 0.7885132 0.2689479 0.657901 0.92479 0.7941977 0.8523115 ... 4.3 Binomial GLM In this section we will run a GLM with a binomial error distribution. We load the following packages: library(tidyverse) library(emmeans) library(car) library(agridat) library(DHARMa) library(glmmTMB) library(viridis) We will use the Titanic survival dataset for the binomial GLM. ## LOAD TITANIC SURVIVAL DATASET data(&quot;TitanicSurvival&quot;) t1 &lt;- TitanicSurvival %&gt;% filter(age&gt;17) # filter out children head(t1) ## survived sex age passengerClass ## Allen, Miss. Elisabeth Walton yes female 29 1st ## Allison, Mr. Hudson Joshua Crei no male 30 1st ## Allison, Mrs. Hudson J C (Bessi no female 25 1st ## Anderson, Mr. Harry yes male 48 1st ## Andrews, Miss. Kornelia Theodos yes female 63 1st ## Andrews, Mr. Thomas Jr no male 39 1st Let’s quickly plot the data: ggplot(t1, aes(x=passengerClass, y=survived, color=sex)) + geom_jitter(height=.2, width=0.2) Figure 2.34: Plot of titanic survival data. Now we can construct a GLM to estimate survival as a function of sex and passengerClass while include Age as co-variate. tglm1 &lt;- glmmTMB(survived ~ sex * passengerClass + age, data=t1, family = binomial(link = &quot;logit&quot;)) Let’s look at the ANOVA table and summary of the model: ## print off anova table Anova(tglm1) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: survived ## Chisq Df Pr(&gt;Chisq) ## sex 141.6195 1 &lt; 2.2e-16 *** ## passengerClass 54.3629 2 1.568e-12 *** ## age 8.0343 1 0.00459 ** ## sex:passengerClass 42.2193 2 6.795e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## print off summary summary(tglm1) ## Family: binomial ( logit ) ## Formula: survived ~ sex * passengerClass + age ## Data: t1 ## ## AIC BIC logLik deviance df.resid ## 766.4 799.9 -376.2 752.4 885 ## ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.440407 0.637348 6.967 3.24e-12 *** ## sexmale -4.105509 0.540239 -7.599 2.97e-14 *** ## passengerClass2nd -1.709637 0.609584 -2.805 0.00504 ** ## passengerClass3rd -3.957656 0.562162 -7.040 1.92e-12 *** ## age -0.025352 0.008944 -2.834 0.00459 ** ## sexmale:passengerClass2nd -0.203859 0.698880 -0.292 0.77052 ## sexmale:passengerClass3rd 2.655928 0.597514 4.445 8.79e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now we can check residuals: hist(resid(tglm1)) ## residuals should be normally distributed, even for glm Figure 4.12: Residual plots. plot(resid(tglm1)~fitted(tglm1)) + ## residuals should be evenly dispersed around 0 across the range of x&#39;s abline(h=0) # funnel shapes or curvature is bad Figure 4.13: Residual plots. ## integer(0) qqPlot(resid(tglm1)) ## residuals should line up pretty closely to the blue line Figure 3.6: Residual plots. ## Allison, Mrs. Hudson J C (Bessi Evans, Miss. Edith Corse ## 3 90 boxplot(resid(tglm1) ~ t1$passengerClass) ## variances should be homogeneous for each group Figure 3.7: Residual plots. ## simulate residuals plot(simulateResiduals(tglm1)) ## plot simulated residuals Figure 4.14: Simulated residual plots. hist(simulateResiduals(tglm1)) ## histogram should be flat Figure 4.15: Simulated residual plots. To make sense of the model output let’s use the emmeans package: emmeans(tglm1, pairwise ~ sex:passengerClass) ## $emmeans ## sex passengerClass emmean SE df asymp.LCL asymp.UCL ## female 1st 3.592 0.516 Inf 2.580 4.6034 ## male 1st -0.514 0.192 Inf -0.890 -0.1371 ## female 2nd 1.882 0.325 Inf 1.246 2.5183 ## male 2nd -2.427 0.303 Inf -3.022 -1.8324 ## female 3rd -0.366 0.203 Inf -0.764 0.0322 ## male 3rd -1.815 0.170 Inf -2.149 -1.4814 ## ## Results are given on the logit (not the response) scale. ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df z.ratio p.value ## female 1st - male 1st 4.106 0.540 Inf 7.599 &lt;.0001 ## female 1st - female 2nd 1.710 0.610 Inf 2.805 0.0567 ## female 1st - male 2nd 6.019 0.602 Inf 9.997 &lt;.0001 ## female 1st - female 3rd 3.958 0.562 Inf 7.040 &lt;.0001 ## female 1st - male 3rd 5.407 0.551 Inf 9.810 &lt;.0001 ## male 1st - female 2nd -2.396 0.377 Inf -6.354 &lt;.0001 ## male 1st - male 2nd 1.913 0.364 Inf 5.262 &lt;.0001 ## male 1st - female 3rd -0.148 0.291 Inf -0.507 0.9959 ## male 1st - male 3rd 1.302 0.270 Inf 4.826 &lt;.0001 ## female 2nd - male 2nd 4.309 0.444 Inf 9.700 &lt;.0001 ## female 2nd - female 3rd 2.248 0.383 Inf 5.872 &lt;.0001 ## female 2nd - male 3rd 3.698 0.367 Inf 10.088 &lt;.0001 ## male 2nd - female 3rd -2.061 0.362 Inf -5.699 &lt;.0001 ## male 2nd - male 3rd -0.612 0.344 Inf -1.776 0.4812 ## female 3rd - male 3rd 1.450 0.255 Inf 5.678 &lt;.0001 ## ## Results are given on the log odds ratio (not the response) scale. ## P value adjustment: tukey method for comparing a family of 6 estimates The estimates in the above output are transformed via the link funciton (logit in this case). We can obtain back-transformed means. This will provide the estimated probability of survival for each sex:passengerClass combination. emmeans(tglm1, pairwise ~ sex:passengerClass, type=&quot;response&quot;) ## type= does contrasts before back-transforming (more appropriate!) ## $emmeans ## sex passengerClass prob SE df asymp.LCL asymp.UCL ## female 1st 0.9732 0.0135 Inf 0.9296 0.990 ## male 1st 0.3743 0.0450 Inf 0.2911 0.466 ## female 2nd 0.8679 0.0372 Inf 0.7766 0.925 ## male 2nd 0.0811 0.0226 Inf 0.0465 0.138 ## female 3rd 0.4096 0.0491 Inf 0.3178 0.508 ## male 3rd 0.1400 0.0205 Inf 0.1044 0.185 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the logit scale ## ## $contrasts ## contrast odds.ratio SE df null z.ratio p.value ## female 1st / male 1st 60.6736 32.8000 Inf 1 7.599 &lt;.0001 ## female 1st / female 2nd 5.5270 3.3700 Inf 1 2.805 0.0567 ## female 1st / male 2nd 411.1691 248.0000 Inf 1 9.997 &lt;.0001 ## female 1st / female 3rd 52.3345 29.4000 Inf 1 7.040 &lt;.0001 ## female 1st / male 3rd 223.0143 123.0000 Inf 1 9.810 &lt;.0001 ## male 1st / female 2nd 0.0911 0.0344 Inf 1 -6.354 &lt;.0001 ## male 1st / male 2nd 6.7767 2.4600 Inf 1 5.262 &lt;.0001 ## male 1st / female 3rd 0.8626 0.2510 Inf 1 -0.507 0.9959 ## male 1st / male 3rd 3.6756 0.9910 Inf 1 4.826 &lt;.0001 ## female 2nd / male 2nd 74.3935 33.1000 Inf 1 9.700 &lt;.0001 ## female 2nd / female 3rd 9.4690 3.6200 Inf 1 5.872 &lt;.0001 ## female 2nd / male 3rd 40.3503 14.8000 Inf 1 10.088 &lt;.0001 ## male 2nd / female 3rd 0.1273 0.0460 Inf 1 -5.699 &lt;.0001 ## male 2nd / male 3rd 0.5424 0.1870 Inf 1 -1.776 0.4812 ## female 3rd / male 3rd 4.2613 1.0900 Inf 1 5.678 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 6 estimates ## Tests are performed on the log odds ratio scale In order to plot we need to create new variable that is 0 or 1, instead of ‘yes’ or ‘no’. t1$surv &lt;- if_else(t1$survived==&#39;yes&#39;,1,0) Now we can make a plot with regression lines: ggplot(t1, aes(x=age, y=surv, color=sex)) + geom_jitter(height=.1, width=0) + geom_smooth(method=&quot;glm&quot;, method.args=list(family=&quot;binomial&quot;), formula = y ~ x, se=F, lwd=1.5) + facet_wrap(~passengerClass) + theme_bw(base_size = 20) Figure 4.16: Plot of survival probabilities by sex and class. We can improve the aesthetics of the plot: tm &lt;- emmeans(tglm1, ~ sex:passengerClass, type=&quot;response&quot;) %&gt;% as.data.frame() plot1 &lt;- ggplot() + geom_jitter(data=t1 %&gt;% filter(sex==&#39;female&#39;), aes(x=passengerClass, y=surv+.01, color=sex), height=0, width=.25, size=1, alpha=.1) + geom_jitter(data=t1 %&gt;% filter(sex==&#39;male&#39;), aes(x=passengerClass, y=surv-.01, color=sex), height=0, width=.25, size=1, alpha=.1) + geom_errorbar(data=tm, aes(x=passengerClass, y=prob, ymin=(prob-SE), ymax=(prob+SE), color=sex), width=.2, lwd=1.25, position = position_dodge(width=0.5)) + geom_point(data=tm , aes(x=passengerClass, y=prob, color=sex), size=5, position=position_dodge(width=0.5)) + scale_y_continuous(&#39;survival&#39;, labels = scales::percent) + scale_color_viridis(discrete = T) + theme(panel.background = element_blank(), panel.border = element_rect(color=&quot;black&quot;, fill=NA, size=2)) + theme(axis.ticks.length=unit(0.3, &quot;cm&quot;), axis.text.x = element_text(margin=margin(5,5,5,5,&quot;pt&quot;),colour=&quot;black&quot;), axis.text.y = element_text(margin=margin(5,5,5,5,&quot;pt&quot;),colour=&quot;black&quot;)) + ## change axis tick marks to make them a little longer #theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + theme(text = element_text(size=20)) ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. plot1 plot2 &lt;- ggplot() + geom_jitter(data=t1 , aes(x=passengerClass, y=surv, color=passengerClass), height=.01, width=.35, size=1, alpha=.2) + geom_errorbar(data=tm , aes(x=passengerClass, y=prob, ymin=(prob-SE), ymax=(prob+SE), color=passengerClass), width=.2, lwd=1.25) + ## make bars thinner geom_point(data=tm , aes(x=passengerClass, y=prob, color=passengerClass), size=5) + facet_wrap(~sex) + scale_y_continuous(&#39;survival&#39;, labels = scales::percent) + scale_color_viridis(discrete = T, option = &#39;C&#39;, direction=-1) + theme(panel.background = element_blank(), panel.border = element_rect(color=&quot;black&quot;, fill=NA, size=2)) + theme(axis.ticks.length=unit(0.3, &quot;cm&quot;), axis.text.x = element_text(margin=margin(5,5,5,5,&quot;pt&quot;),colour=&quot;black&quot;), axis.text.y = element_text(margin=margin(5,5,5,5,&quot;pt&quot;),colour=&quot;black&quot;)) + theme(text = element_text(size=20)) plot2 4.4 Gamma GLM In this section we will learn about implementing the Gamma distribution in R. This distribution is helpful in for modeling a variety of data but is most frequently applied to data that is right skewed but not necessarily count data. The next several examples will illustrate how to generate Gamma distributed data based on several parameters as well as how to implement Gamma GLMs. 4.4.1 Example 1 Here we generate a distribution using the rgamma() function. Remember to set.seed so we get reproducible results. set.seed(15) var1 &lt;- rgamma(1000, shape = 2, scale = .5) hist(var1, main=&#39;mean=1, scale=0.5&#39;) Note that the mean of this distribution should be 1 because the mean is equal to product of the shape parameter and scale parameter. So, 2 multiplied by 0.5 should be 1. set.seed(15) g1 &lt;- glmmTMB(var1 ~ 1, family=Gamma(link=&quot;inverse&quot;)) ## Warning in glmmTMB(var1 ~ 1, family = Gamma(link = &quot;inverse&quot;)): use of the ## &#39;data&#39; argument is recommended g1 ## Formula: var1 ~ 1 ## AIC BIC logLik df.resid ## 1822.0994 1831.9149 -909.0497 998 ## ## Number of obs: 1000 ## ## Dispersion estimate for Gamma family (sigma^2): 0.483 ## ## Fixed Effects: ## ## Conditional model: ## (Intercept) ## 0.9657 ## mean should be ~1 with inverse link function 1/0.9657 ## [1] 1.035518 In the above example we run a Gamma GLM as an intercept only model. By running an intercept only model we assume no other predictors can predict the response variable. Therefore the output from this model would simply be the mean of the response, which should be ~1 (which it is after applying the right link function to the intercept). g1a &lt;- glmmTMB(var1 ~ 1, family=Gamma(link=&quot;log&quot;)) ## Warning in glmmTMB(var1 ~ 1, family = Gamma(link = &quot;log&quot;)): use of the &#39;data&#39; ## argument is recommended g1a ## Formula: var1 ~ 1 ## AIC BIC logLik df.resid ## 1822.0994 1831.9149 -909.0497 998 ## ## Number of obs: 1000 ## ## Dispersion estimate for Gamma family (sigma^2): 0.483 ## ## Fixed Effects: ## ## Conditional model: ## (Intercept) ## 0.03491 ## mean should be ~1 with log link function exp(0.03491) ## [1] 1.035527 The same is shown above but in this case we use a log link and so to back transform the intercept we exponentiate. Using either the inverse or log-link should give us the same answer. 4.4.2 Example 2 In the example below we generate another Gamma distributed variable but we change the shape and scale parameter. But because the mean is based on the product of the two, the mean remains as 1. set.seed(15) var1 &lt;- rgamma(1000, shape = 4, scale = .25) hist(var1, main=&#39;mean=1, scale=0.25&#39;) # mean = a (shape) * b (rate) # mean = 4 * .25 = 1.0 g1 &lt;- glmmTMB(var1 ~ 1, family=Gamma(link=&quot;inverse&quot;)) ## Warning in glmmTMB(var1 ~ 1, family = Gamma(link = &quot;inverse&quot;)): use of the ## &#39;data&#39; argument is recommended g1 ## Formula: var1 ~ 1 ## AIC BIC logLik df.resid ## 1281.0616 1290.8771 -638.5308 998 ## ## Number of obs: 1000 ## ## Dispersion estimate for Gamma family (sigma^2): 0.233 ## ## Fixed Effects: ## ## Conditional model: ## (Intercept) ## 0.9711 ## mean should be 1 with inverse link function 1/0.9711 ## [1] 1.02976 g1a &lt;- glmmTMB(var1 ~ 1, family=Gamma(link=&quot;log&quot;)) ## Warning in glmmTMB(var1 ~ 1, family = Gamma(link = &quot;log&quot;)): use of the &#39;data&#39; ## argument is recommended g1a ## Formula: var1 ~ 1 ## AIC BIC logLik df.resid ## 1281.0616 1290.8771 -638.5308 998 ## ## Number of obs: 1000 ## ## Dispersion estimate for Gamma family (sigma^2): 0.233 ## ## Fixed Effects: ## ## Conditional model: ## (Intercept) ## 0.02937 ## mean should be 1 with log link function exp(0.02937) ## [1] 1.029806 We once again repeat the same exercise as example 1 and see that when the appropriate back transformation based on link functions are applied to the intercept, it results in the original mean. 4.4.3 Example 3 In this example we change the mean of the distribution to 0.5 by changing the shape and scale parameters to 1 and 0.5 respectively. We then repeat the previous examples and run the inverse and log link Gamma GLMs and observe how the intercepts are backtransformed to approximate the previously set mean value. set.seed(15) var1 &lt;- rgamma(1000, shape = 1, scale = .5) hist(var1, main=&#39;mean=0.5, scale=0.5&#39;) # mean = a (shape) * b (rate) # mean = 1 * .5 = 0.5 g1 &lt;- glmmTMB(var1 ~ 1, family=Gamma(link=&quot;inverse&quot;)) ## Warning in glmmTMB(var1 ~ 1, family = Gamma(link = &quot;inverse&quot;)): use of the ## &#39;data&#39; argument is recommended g1 ## Formula: var1 ~ 1 ## AIC BIC logLik df.resid ## 654.8480 664.6635 -325.4240 998 ## ## Number of obs: 1000 ## ## Dispersion estimate for Gamma family (sigma^2): 0.968 ## ## Fixed Effects: ## ## Conditional model: ## (Intercept) ## 1.963 ## mean should be 1 with inverse link function 1/g1$sdr$par.fixed[1] ## beta ## 0.5095401 g1a &lt;- glmmTMB(var1 ~ 1, family=Gamma(link=&quot;log&quot;)) ## Warning in glmmTMB(var1 ~ 1, family = Gamma(link = &quot;log&quot;)): use of the &#39;data&#39; ## argument is recommended g1a$sdr$par.fixed[1] ## beta ## -0.6742484 ## mean should be 1 with log link function exp(g1a$sdr$par.fixed[1]) ## beta ## 0.5095393 4.4.4 Example 4 This example follows the previous examples but with the mean changed to 0.25. set.seed(15) var1 &lt;- rgamma(1000, shape = .25, scale = 1) hist(var1, main=&#39;mean=0.25, scale=1&#39;) # mean = a (shape) * b (rate) # mean = .5 * 1 = 0.5 g1 &lt;- glmmTMB(var1 ~ 1, family=Gamma(link=&quot;inverse&quot;)) ## Warning in glmmTMB(var1 ~ 1, family = Gamma(link = &quot;inverse&quot;)): use of the ## &#39;data&#39; argument is recommended g1 ## Formula: var1 ~ 1 ## AIC BIC logLik df.resid ## -3174.013 -3164.197 1589.006 998 ## ## Number of obs: 1000 ## ## Dispersion estimate for Gamma family (sigma^2): 3.94 ## ## Fixed Effects: ## ## Conditional model: ## (Intercept) ## 3.964 ## mean should be 1 with inverse link function 1/g1$sdr$par.fixed[1] ## beta ## 0.2522893 g1a &lt;- glmmTMB(var1 ~ 1, family=Gamma(link=&quot;log&quot;)) ## Warning in glmmTMB(var1 ~ 1, family = Gamma(link = &quot;log&quot;)): use of the &#39;data&#39; ## argument is recommended g1a ## Formula: var1 ~ 1 ## AIC BIC logLik df.resid ## -3174.013 -3164.197 1589.006 998 ## ## Number of obs: 1000 ## ## Dispersion estimate for Gamma family (sigma^2): 3.94 ## ## Fixed Effects: ## ## Conditional model: ## (Intercept) ## -1.377 ## mean should be 1 with log link function exp(g1a$sdr$par.fixed[1]) ## beta ## 0.2522895 4.5 Briefly running Gamma GLMs In this section we simulate data for two groups: set.seed(25) v1 &lt;- rgamma(100, shape = 3, scale = .5) %&gt;% as.data.frame() colnames(v1) &lt;- &quot;var&quot; v1$group &lt;- &quot;one&quot; head(v1) ## var group ## 1 1.0881400 one ## 2 2.4520815 one ## 3 3.5583571 one ## 4 0.9405657 one ## 5 0.9225875 one ## 6 1.4339098 one v2 &lt;- rgamma(100, shape = 1, scale = .2) %&gt;% as.data.frame() colnames(v2) &lt;- &quot;var&quot; v2$group &lt;- &quot;two&quot; head(v2) ## var group ## 1 0.15108136 two ## 2 0.01761541 two ## 3 0.47914554 two ## 4 0.05403794 two ## 5 0.13555345 two ## 6 0.19157853 two Then bind the two groups into one dataset: dat1 &lt;- rbind(v1,v2) #mean group 1 = 1.5, group 2 = 0.5 dat1 %&gt;% mutate(obs=rep(1:100,2)) %&gt;% group_by(obs) %&gt;% pivot_wider(names_from = group,values_from = var) %&gt;% ungroup() %&gt;% select(one,two) ## # A tibble: 100 × 2 ## one two ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.09 0.151 ## 2 2.45 0.0176 ## 3 3.56 0.479 ## 4 0.941 0.0540 ## 5 0.923 0.136 ## 6 1.43 0.192 ## 7 0.559 0.185 ## 8 1.20 0.0195 ## 9 1.15 0.0725 ## 10 0.854 0.00261 ## # ℹ 90 more rows We then check the data distributions: hist(dat1$var) Figure 2.41: Histogram. ggplot(dat1, aes(x=var)) + geom_histogram(bins=8, fill=&quot;grey&quot;, color=&quot;black&quot;) + facet_wrap(~group, scales=&quot;free&quot;) + theme_bw(base_size = 16) Figure 4.17: Histogram for each group. Then we construct several different models (a Gaussian, a Gamma, and a log-normal) and use AIC to compare models. Obviously we know the data come from a Gamma distribution because we simulated them. However, we will fit these different models to examine model fit using residuals and AIC. #### construct model w/ normal distribution mod0 &lt;- glmmTMB(var ~ group, data=dat1) plot(simulateResiduals(mod0)) summary(mod0) ## Family: gaussian ( identity ) ## Formula: var ~ group ## Data: dat1 ## ## AIC BIC logLik deviance df.resid ## 408.0 417.9 -201.0 402.0 197 ## ## ## Dispersion estimate for gaussian family (sigma^2): 0.437 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.49247 0.06610 22.58 &lt;2e-16 *** ## grouptwo -1.28086 0.09349 -13.70 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 emmeans(mod0, ~group, type=&quot;response&quot;) ## group emmean SE df lower.CL upper.CL ## one 1.492 0.0661 197 1.3621 1.623 ## two 0.212 0.0661 197 0.0813 0.342 ## ## Confidence level used: 0.95 #### construct model w/ log-normal distribution mod0a &lt;- glmmTMB(log(var) ~ group, data=dat1) plot(simulateResiduals(mod0a)) summary(mod0a) ## Family: gaussian ( identity ) ## Formula: log(var) ~ group ## Data: dat1 ## ## AIC BIC logLik deviance df.resid ## 585.6 595.5 -289.8 579.6 197 ## ## ## Dispersion estimate for gaussian family (sigma^2): 1.06 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.2288 0.1030 2.221 0.0264 * ## grouptwo -2.3798 0.1457 -16.330 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 emmeans(mod0a, ~group, type=&quot;response&quot;) ## group response SE df lower.CL upper.CL ## one 1.257 0.130 197 1.026 1.540 ## two 0.116 0.012 197 0.095 0.143 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale #### construct model w/ Gamma distribution and inverse link mod1 &lt;- glmmTMB(var ~ group, data=dat1, family=Gamma(link = &quot;inverse&quot;)) plot(simulateResiduals(mod1)) summary(mod1) ## Family: Gamma ( inverse ) ## Formula: var ~ group ## Data: dat1 ## ## AIC BIC logLik deviance df.resid ## 160.7 170.6 -77.3 154.7 197 ## ## ## Dispersion estimate for Gamma family (sigma^2): 0.693 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.67003 0.05577 12.01 &lt;2e-16 *** ## grouptwo 4.05552 0.39726 10.21 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 emmeans(mod1, ~group, type=&quot;response&quot;) ## group response SE df asymp.LCL asymp.UCL ## one 1.492 0.1240 Inf 1.283 1.783 ## two 0.212 0.0176 Inf 0.182 0.253 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the inverse scale #### construct model w/ Gamma distribution and log link mod2 &lt;- glmmTMB(var ~ group, data=dat1, family=Gamma(link = &quot;log&quot;)) plot(simulateResiduals(mod2)) summary(mod2) ## Family: Gamma ( log ) ## Formula: var ~ group ## Data: dat1 ## ## AIC BIC logLik deviance df.resid ## 160.7 170.6 -77.3 154.7 197 ## ## ## Dispersion estimate for Gamma family (sigma^2): 0.693 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.40043 0.08323 4.811 1.5e-06 *** ## grouptwo -1.95342 0.11771 -16.595 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 emmeans(mod2, ~group, type=&quot;response&quot;) ## group response SE df asymp.LCL asymp.UCL ## one 1.492 0.1240 Inf 1.27 1.757 ## two 0.212 0.0176 Inf 0.18 0.249 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale AIC: AIC(mod0,mod0a,mod1,mod2) ## df AIC ## mod0 3 408.0006 ## mod0a 3 585.5705 ## mod1 3 160.6724 ## mod2 3 160.6724 Which model is the best ranked model? "],["mixed-effect-models.html", "5 Mixed-effect models 5.1 Interactions in statistical models 5.2 Block Designs 5.3 Variance components 5.4 Split-plot and nested designs 5.5 Repeated Measures 5.6 Additional LMM reading", " 5 Mixed-effect models After completing this module, students will be able to: 5.1 Differentiate between fixed and random effects 5.2 Interpret output for fixed and random effects in R 5.3 Design experiments with blocks, repeated measures, and fixed effect treatments 5.1 Interactions in statistical models Before we learn about random effects, we will briefly cover interactions in statistical models. We will need the following libraries: library(tidyverse) library(car) ### helpful for analyzing linear models library(emmeans) ### helpful for getting means from linear models library(multcompView) library(glmmTMB) library(performance) For this section we will construct data using some code. For this example, we need to “doctor” up a data set. We will use the InsectSprays data set (from previous examples) but we will make a few changes. Don’t worry about the changes, but the code below does this. The main one is that we have added a new column to the dataset called ‘weeds’, which represents the amount of weed cover in the plot. # generate a fake dataset to use for the example set.seed(17) data(&quot;InsectSprays&quot;) d &lt;- InsectSprays %&gt;% filter(spray==&#39;A&#39;|spray==&#39;B&#39;|spray==&#39;C&#39;|spray==&#39;F&#39;) %&gt;% droplevels() d$count[13:24] &lt;- d$count[13:24]+5 d$weeds &lt;- abs(round(rnorm(48,2*d$count,10),1)) d$weeds[25:36] &lt;- c(55.3,46.8,30.2,62.3,24.2,33.2,18.2,12.6,39.7,41.0,46.9,42.8) Let’s plot the raw data to show the count values as a function of spray types using boxplots: ggplot(d, aes(x=spray,y=count)) + geom_boxplot(outlier.shape = NA) + geom_jitter(height=0,width=.1) Figure 5.1: Plot of insect count data. We want to look at pairwise means between these spray types. To do this we can use the emmeans package and set the pairwise arguments from the emmeans function as pairwise ~ spray. We need to apply this function to a linear model (basically an ANOVA) that assesses the counts as a function of the spray type. The pairwise comparisons are seen in the ‘contrasts’ section of the output below: anova_means &lt;- emmeans(glmmTMB(count~spray, data=d), pairwise~spray) anova_means ## $emmeans ## spray emmean SE df lower.CL upper.CL ## A 14.50 1.26 43 11.961 17.04 ## B 20.33 1.26 43 17.794 22.87 ## C 2.08 1.26 43 -0.456 4.62 ## F 16.67 1.26 43 14.127 19.21 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## A - B -5.83 1.78 43 -3.276 0.0108 ## A - C 12.42 1.78 43 6.973 &lt;.0001 ## A - F -2.17 1.78 43 -1.217 0.6198 ## B - C 18.25 1.78 43 10.249 &lt;.0001 ## B - F 3.67 1.78 43 2.059 0.1830 ## C - F -14.58 1.78 43 -8.190 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 4 estimates It would also be helpful to view counts as a function of weed cover. We can do this using facet_wrap() and geom_smooth: ggplot(d, aes(x=weeds,y=count)) + geom_point() + facet_wrap(~spray) + geom_smooth(method=&#39;lm&#39;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 2.2: Insect counts plotted against weed cover for the four spray treatments. 5.1.1 ANCOVA: multiple intercept model In the previous example we ran an ANOVA with only a categorical predictor. ANOVAs have long been taught to be used for categorical predictors only if your response variable is continuous. This is not entirely accurate considering that an ANOVA is the same thing as a linear model which can use continuous predictors. For this part, let’s run a model that assesses counts as a function of both a categorical and continuous variable, spray type and weed cover. This would be a biologically reasonable model to run as we see from the plots above that our counts are not only different based on spray types but also vary along a weed cover gradient where, in most cases, there are higher insect counts associated with higher weed cover. Our goal should therefore be to assess the effects of spray types on counts while accounting for this relationship between counts and weed cover. We can incorporate both types of predictors in the same linear model format as before. With the inclusion of a continuous variable, an ANOVA is often called an ANCOVA (an analysis of covariance). Here is the basic form of the linear model with a categorical and continous variable: \\[y = \\beta_{0} + \\beta_{i} + \\beta_{1}*x + \\varepsilon\\] \\(y\\) is the response variable \\(\\beta_{0}\\) is the intercept \\(\\beta_{i}\\) is the adjustment to the intercept for each group \\(_{i}\\) \\(\\beta_{1}\\) is the slope \\(x\\) is the predictor variable and \\(\\varepsilon\\) are the residuals. Let’s run our ANCOVA model: lm1i &lt;- glmmTMB(count ~ spray + weeds, data=d) Let’s get an ANOVA table with Type II sums of squares (see Box 5.1 for overview of Sums of Squares and differences with SAS Type III): Anova(lm1i, type=2) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: count ## Chisq Df Pr(&gt;Chisq) ## spray 228.168 3 &lt; 2.2e-16 *** ## weeds 51.318 1 7.854e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note: Don’t use the base anova()! If we want to look at the model coefficients we can use summary(): summary(lm1i) ## model coefficients ## Family: gaussian ( identity ) ## Formula: count ~ spray + weeds ## Data: d ## ## AIC BIC logLik deviance df.resid ## 254.7 265.9 -121.4 242.7 42 ## ## ## Dispersion estimate for gaussian family (sigma^2): 9.2 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 7.69502 1.29175 5.957 2.57e-09 *** ## sprayB 2.88374 1.30463 2.210 0.0271 * ## sprayC -13.64507 1.24977 -10.918 &lt; 2e-16 *** ## sprayF 1.48599 1.24159 1.197 0.2314 ## weeds 0.21271 0.02969 7.164 7.85e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is quite a bit in that output and its difficult to piece together. What does it all mean?!?! Really all we want to do is calculate the estimated means for each group once we’ve accounted for the effect of weed cover. With emmeans we can just that – that is, calculate estimated marginal mean for each group (ie. groups means after accounting for the effect of weeds). We can extract the emmeans means (ie. group means after accounting for the effect of weeds): ancova_means &lt;- emmeans(lm1i, pairwise~spray) ancova_means ## $emmeans ## spray emmean SE df lower.CL upper.CL ## A 15.71 0.892 42 13.915 17.51 ## B 18.60 0.908 42 16.765 20.43 ## C 2.07 0.875 42 0.303 3.84 ## F 17.20 0.879 42 15.428 18.97 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## A - B -2.88 1.30 42 -2.210 0.1370 ## A - C 13.65 1.25 42 10.918 &lt;.0001 ## A - F -1.49 1.24 42 -1.197 0.6322 ## B - C 16.53 1.26 42 13.107 &lt;.0001 ## B - F 1.40 1.28 42 1.094 0.6952 ## C - F -15.13 1.24 42 -12.199 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 4 estimates If we want to plot these readouts from the emmeans output then we first need to convert this table into a data frame using as.data.frame(): lm1i_coef &lt;- as.data.frame(emmeans(lm1i, ~spray)) Now that we have saved the emmeans outputs into a data.frame we can extract intercepts and add slopes into new dataframe. We can do this by using the `at=list(weeds=0)’ in emmeans to specify that we want to means of each spray treatment when the covariate weeds is equal to zero (in other words, the intercepts for each group). We also extract the slope manually from the saved model. lm1i_coef2 &lt;- as.data.frame(emmeans(lm1i, ~spray, at=list(weeds=0))) lm1i_coef2$slope &lt;- coef(lm1i)[5] We can now plot the data with the fitted model and also color in specific values of importance. We can color code the different intercepts as well as the different means: ggplot(data=d, aes(x=weeds,y=count)) + geom_point() + facet_wrap(~spray) + geom_abline(data=lm1i_coef2, aes(intercept=emmean, slope=slope)) + geom_point(data=lm1i_coef2, aes(x=0,y=emmean),color=&quot;red&quot;) + geom_point(data=lm1i_coef, aes(x=mean(d$weeds),y=emmean), color=&quot;blue&quot;, size=2) ## Warning: Removed 4 rows containing missing values or values outside the scale range ## (`geom_abline()`). Figure 2.8: Plot of insect counts against weed cover for each spray type. Red dots represent intercepts and blue dots represent emmeans. Box 5.1. Sums of Squares In car::Anova() the Type II sums of squares is the default and is preferred over Type I or III. Type II SS are calculated based on the principal of marginally, meaning the test statistic for each term in the model is calculated after all other terms excluding the term’s high-order relatives. How the car package calculates SS is different from SAS. In SAS, Type III SS are almost always preferred. In car::Anova() Type II are similar to Type III in SAS, although there are some differences. See the help for ??car::Anova for additional information. Long story short, Type II SS are preferred in car::Anova(). 5.1.2 ANCOVA: multiple intercept AND slope model In the previous example above we ran a ANCOVA to account for variation around the intercept. The idea being: weed cover percentages are associated with insect counts and therefore they need to be accounted for because different spray types were done across a gradient of weed cover. For example, in the previous plot, Spray A tended to have plots with low weed cover, whereas Spray B tended to have plots with high weed cover (mostly &gt;40%). Therefore, we assumed that insect counts, and therefore the intercepts per spray type, would be different. The previous model does a good job at capturing these different intercepts for each spray type. There is one problem with the previous model, though. Look at the fitted regression line for Spray C – it doesn’t fit that well. The intercept looks way too low, the slope looks too steep, but the emmean estimate looks ok. If we wanted to also account for different slope values across the grass cover gradient then we can do so by including an interaction term between spray type and weeds. This would be done by coding in the interaction as spray:weeds into the model predictors as shown below. This is how we do an ANCOVA with multiple intercept and slopes: lm1is &lt;- glmmTMB(count ~ spray + weeds + spray:weeds, data=d) The above code essentially translates to: “Use a linear model to assess count as a function of the additive effects of spray and weeds as well as the interactive effect of the two predictors. Another shorthand way to code the above would be: glmmTMB(count ~ spray * weeds, data=d) ## Formula: count ~ spray * weeds ## Data: d ## AIC BIC logLik df.resid ## 235.0966 251.9375 -108.5483 39 ## ## Number of obs: 48 ## ## Dispersion estimate for gaussian family (sigma^2): 5.39 ## ## Fixed Effects: ## ## Conditional model: ## (Intercept) sprayB sprayC sprayF weeds ## 5.619629 2.440333 -2.441188 0.613378 0.277584 ## sprayB:weeds sprayC:weeds sprayF:weeds ## -0.009947 -0.306581 0.018897 Let’s get an ANOVA table and a summary of the model as we did in the previous examples: Anova(lm1is, type=2) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: count ## Chisq Df Pr(&gt;Chisq) ## spray 389.083 3 &lt; 2.2e-16 *** ## weeds 87.510 1 &lt; 2.2e-16 *** ## spray:weeds 33.852 3 2.129e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(lm1is) ## Family: gaussian ( identity ) ## Formula: count ~ spray + weeds + spray:weeds ## Data: d ## ## AIC BIC logLik deviance df.resid ## 235.1 251.9 -108.5 217.1 39 ## ## ## Dispersion estimate for gaussian family (sigma^2): 5.39 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.619629 1.677717 3.350 0.000809 *** ## sprayB 2.440333 3.111865 0.784 0.432921 ## sprayC -2.441188 2.545417 -0.959 0.337532 ## sprayF 0.613378 2.227141 0.275 0.783001 ## weeds 0.277584 0.048074 5.774 7.74e-09 *** ## sprayB:weeds -0.009947 0.073238 -0.136 0.891963 ## sprayC:weeds -0.306581 0.067566 -4.537 5.69e-06 *** ## sprayF:weeds 0.018897 0.060668 0.311 0.755434 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From the ANOVA table we can see all three terms (the two main effects as well as the interaction) are all highly significant. The summary() output is difficult to make sense of. Once again we can use emmeans to calculate the estimated marginal mean for each group (ie. groups means after accounting for the effect of weeds): ancova_is_means &lt;- emmeans(lm1is, pairwise~spray) ## NOTE: Results may be misleading due to involvement in interactions ancova_is_means ## $emmeans ## spray emmean SE df lower.CL upper.CL ## A 16.09 0.724 39 14.620 17.55 ## B 18.15 0.808 39 16.517 19.78 ## C 2.09 0.670 39 0.729 3.44 ## F 17.41 0.677 39 16.042 18.78 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## A - B -2.065 1.080 39 -1.904 0.2432 ## A - C 14.000 0.987 39 14.185 &lt;.0001 ## A - F -1.326 0.991 39 -1.337 0.5453 ## B - C 16.065 1.050 39 15.305 &lt;.0001 ## B - F 0.739 1.050 39 0.702 0.8958 ## C - F -15.326 0.953 39 -16.089 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 4 estimates In addition to retrieving estimated means we should also calculate and extract the different slope values. We can calculate the slope for each group with emtrends(): ancova_is_slopes &lt;- emtrends(lm1is, pairwise~spray, var=&quot;weeds&quot;) ancova_is_slopes ## $emtrends ## spray weeds.trend SE df lower.CL upper.CL ## A 0.278 0.0481 39 0.180 0.375 ## B 0.268 0.0553 39 0.156 0.379 ## C -0.029 0.0475 39 -0.125 0.067 ## F 0.296 0.0370 39 0.222 0.371 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## A - B 0.00995 0.0732 39 0.136 0.9991 ## A - C 0.30658 0.0676 39 4.537 0.0003 ## A - F -0.01890 0.0607 39 -0.311 0.9894 ## B - C 0.29663 0.0728 39 4.072 0.0012 ## B - F -0.02884 0.0665 39 -0.434 0.9723 ## C - F -0.32548 0.0602 39 -5.407 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 4 estimates We can extract the emmeans means (ie. group means after accounting for the effect of weeds): lm1is_coef &lt;- as.data.frame(emmeans(lm1is, ~spray)) ## NOTE: Results may be misleading due to involvement in interactions We can also extract the intercepts and add slopes into a new data frame: lm1is_coef2a &lt;- as.data.frame(emmeans(lm1is, ~spray, at=list(weeds=0))) ## NOTE: Results may be misleading due to involvement in interactions lm1is_coef2b &lt;- as.data.frame(emtrends(lm1is, var=&quot;weeds&quot;)) lm1is_coef2 &lt;- full_join(lm1is_coef2a,lm1is_coef2b,by=&quot;spray&quot;) Finally we can plot the data of the fitted model: ggplot(data=d, aes(x=weeds,y=count)) + geom_point() + facet_wrap(~spray) + geom_abline(data=lm1is_coef2, aes(intercept=emmean, slope=weeds.trend), lty=2) + geom_point(data=lm1is_coef2, aes(x=0,y=emmean), color=&quot;orange&quot;) + geom_point(data=lm1is_coef, aes(x=mean(d$weeds),y=emmean), color=&quot;purple&quot;, size=2) Figure 2.15: Plot of insect counts against weed cover for each spray type. Orange dots represent intercepts and purple dots represent emmeans. Note with the plot above that we can observe the slopes differing in values compared to the previous model that only accounted for varying intercepts. Now, even for Spray C the intercept and slope look like good estimates. Here’s an alternative nice plot of the data with weed cover: ggplot(d, aes(x=weeds,y=count)) + geom_point() + facet_wrap(~spray) + geom_smooth(method=&#39;lm&#39;, color=&#39;black&#39;) + theme_bw(base_size = 16) + labs(x = &quot;Weed Coverage %&quot;, y = &quot;Count&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 2.16: Plot of insect counts against weed cover for each spray type. 5.2 Block Designs 5.2.1 Data prep and Plotting Let’s load the packages and data. Note we are using the InsectSprays data set one again. library(tidyverse) library(car) library(glmmTMB) data(&quot;InsectSprays&quot;) Now we can add our blocks to the data by constructing a vector of factor variables using the as.factor(rep(c(1:12), 6)). This code essentially builds 12 different blocks in the data set with each block being comprised of 6 replicates for a total of 72 replicates or samples. InsectSprays$block &lt;- as.factor(rep(c(1:12), 6)) d &lt;- InsectSprays %&gt;% filter(spray==&#39;A&#39;|spray==&#39;B&#39;|spray==&#39;C&#39;|spray==&#39;F&#39;) glimpse(d) ## Rows: 48 ## Columns: 3 ## $ count &lt;dbl&gt; 10, 7, 20, 14, 14, 12, 10, 23, 17, 20, 14, 13, 11, 17, 21, 11, 1… ## $ spray &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, B, B, B, B, B, B, B… ## $ block &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9… We can graph the data by treatment group based on spray type by using a combination of raw data points and boxplots: #plot data by treatment group ggplot(d, aes(x=spray,y=count)) + geom_boxplot(outlier.shape = NA) + geom_jitter(height=0,width=.1) Figure 2.19: Plot of insect counts. A plot by treatment and block (note one observation per block, which is why the boxplots are just a point and line): ggplot(d, aes(x=spray,y=count)) + geom_boxplot(outlier.shape = NA) + geom_jitter(height=0,width=.1) + facet_wrap(~block) #12 blocks Figure 3.1: Plot of insect counts by block. 5.2.2 Models with block designs If we were to ignore the blocked design in our data then we would simply run a linear model of counts as a function of spray type: lm1 &lt;- glmmTMB(count~spray, data=d) Anova(lm1) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: count ## Chisq Df Pr(&gt;Chisq) ## spray 86.656 3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can easily account for the blocking factors by adding the block variable to our model as a fixed effect. We can see there is still a significant Spray effect and also a significant block effect. lm2 &lt;- glmmTMB(count~spray+block, data=d) Anova(lm2) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: count ## Chisq Df Pr(&gt;Chisq) ## spray 141.948 3 &lt; 2.2e-16 *** ## block 30.627 11 0.001262 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2.3 Blocks as random effects In the previous example, we aren’t necessarily interested in testing the significance, or estimating parameters, for the block effect – we just want to account for it. Therefore, block may be more appropriate fitted as a random effect. Another nice thing about the glmmTMB() function is that we can also incorporate random effects. In this section we will utilize random effects to account for blocks as opposed to fixed effects in the models prior to this section. Now let’s set up a linear mixed effect model with a block as a random effect (random intercept). The syntax to set this up is relatively similar to how we specify a regular linear model. However to add a random effect we utilize parentheses. More specifically we include the term (1|block) to include a random intercept that varies across blocks: # block as random effect lm3 &lt;- glmmTMB(count~spray+(1|block), data=d) Anova(lm3) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: count ## Chisq Df Pr(&gt;Chisq) ## spray 106.46 3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the ANOVA table above, we can see the results for the spray effect. Box 5.2. Alternative packages for linear mixed models Another great package for running linear mixed models the lme4 package. This was actually a precursor to glmmTMB – for the part, glmmTMB has superceded lme4, although there are a few things that are usefully implemented in lme4 that are not available with glmmTMB objects. One is the extension package lmerTest which calculates F-values, Type III SS, and p-values using algorithms borrowed from SAS. The anova() function is from the lmerTest package and uses Type III SS. See Box 5.1 for a refresher on sums of squares. The results are usually similar between glmmTMB and lme4, but sometimes its useful to be able to obtain F-values for mixed models. Now let’s compare the model for blocks as fixed vs. random effects: # compare summary() for fixed vs. random blocking effect summary(lm2) ## Family: gaussian ( identity ) ## Formula: count ~ spray + block ## Data: d ## ## AIC BIC logLik deviance df.resid ## 285.9 315.9 -127.0 253.9 32 ## ## ## Dispersion estimate for gaussian family (sigma^2): 11.6 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 10.3541 1.9052 5.435 5.49e-08 *** ## sprayB 0.8333 1.3913 0.599 0.549213 ## sprayC -12.4167 1.3913 -8.924 &lt; 2e-16 *** ## sprayF 2.1667 1.3913 1.557 0.119412 ## block2 0.5000 2.4099 0.207 0.835624 ## block3 7.7500 2.4099 3.216 0.001300 ** ## block4 4.2500 2.4099 1.764 0.077802 . ## block5 4.0000 2.4099 1.660 0.096946 . ## block6 2.7500 2.4099 1.141 0.253808 ## block7 2.5000 2.4099 1.037 0.299545 ## block8 4.7500 2.4099 1.971 0.048717 * ## block9 8.2500 2.4099 3.423 0.000618 *** ## block10 8.7500 2.4099 3.631 0.000282 *** ## block11 3.5000 2.4099 1.452 0.146400 ## block12 2.7500 2.4099 1.141 0.253808 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(lm3) ## Family: gaussian ( identity ) ## Formula: count ~ spray + (1 | block) ## Data: d ## ## AIC BIC logLik deviance df.resid ## 287.5 298.8 -137.8 275.5 42 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## block (Intercept) 3.539 1.881 ## Residual 15.487 3.935 ## Number of obs: 48, groups: block, 12 ## ## Dispersion estimate for gaussian family (sigma^2): 15.5 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 14.5000 1.2592 11.516 &lt; 2e-16 *** ## sprayB 0.8333 1.6066 0.519 0.604 ## sprayC -12.4167 1.6066 -7.729 1.09e-14 *** ## sprayF 2.1667 1.6066 1.349 0.177 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 coef(lm3) ## prints model coefficients ## $block ## (Intercept) sprayB sprayC sprayF ## 1 12.52003 0.8333361 -12.41666 2.166669 ## 2 12.75882 0.8333361 -12.41666 2.166669 ## 3 16.22128 0.8333361 -12.41666 2.166669 ## 4 14.54975 0.8333361 -12.41666 2.166669 ## 5 14.43035 0.8333361 -12.41666 2.166669 ## 6 13.83338 0.8333361 -12.41666 2.166669 ## 7 13.71398 0.8333361 -12.41666 2.166669 ## 8 14.78854 0.8333361 -12.41666 2.166669 ## 9 16.46007 0.8333361 -12.41666 2.166669 ## 10 16.69886 0.8333361 -12.41666 2.166669 ## 11 14.19156 0.8333361 -12.41666 2.166669 ## 12 13.83338 0.8333361 -12.41666 2.166669 Notice the different outputs above and how they differ between the model types. When blocks are added as a fixed effect in lm2 we see every block effect in the summary. Whereas for the mixed effect model lm3, the effect of block is not printed in the summary; rather, we have to print it out using the coef(). The coefficients for the spray treatment types are essentially the same as well as the standard errors between the two models. Let’s check residuals: plot(resid(lm2)~fitted(lm2)) # check residuals of fixed-effect block models plot(resid(lm3)~fitted(lm3)) # check residuals of random-effect block model hist(resid(lm2)) hist(resid(lm3)) Finally, we can compare estimated marginal means: emmeans(lm2, pairwise~spray) ## fixed effect block ## $emmeans ## spray emmean SE df lower.CL upper.CL ## A 14.50 0.984 32 12.4960 16.50 ## B 15.33 0.984 32 13.3293 17.34 ## C 2.08 0.984 32 0.0793 4.09 ## F 16.67 0.984 32 14.6627 18.67 ## ## Results are averaged over the levels of: block ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## A - B -0.833 1.39 32 -0.599 0.9316 ## A - C 12.417 1.39 32 8.924 &lt;.0001 ## A - F -2.167 1.39 32 -1.557 0.4166 ## B - C 13.250 1.39 32 9.523 &lt;.0001 ## B - F -1.333 1.39 32 -0.958 0.7737 ## C - F -14.583 1.39 32 -10.481 &lt;.0001 ## ## Results are averaged over the levels of: block ## P value adjustment: tukey method for comparing a family of 4 estimates emmeans(lm3, pairwise~spray) ## random effect block ## $emmeans ## spray emmean SE df lower.CL upper.CL ## A 14.50 1.26 42 11.959 17.04 ## B 15.33 1.26 42 12.792 17.87 ## C 2.08 1.26 42 -0.458 4.62 ## F 16.67 1.26 42 14.126 19.21 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## A - B -0.833 1.61 42 -0.519 0.9541 ## A - C 12.417 1.61 42 7.729 &lt;.0001 ## A - F -2.167 1.61 42 -1.349 0.5379 ## B - C 13.250 1.61 42 8.247 &lt;.0001 ## B - F -1.333 1.61 42 -0.830 0.8400 ## C - F -14.583 1.61 42 -9.077 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 4 estimates The results are similar for the fixed and random effect model, so what’s the difference? We will discuss the advantages of random effects in lecture and cover other aspects in the sections below. 5.3 Variance components One of the major differences for mixed-effect models is that we can calculate the variance component of the random effects. We will go through how to do this in lecture, but basically the variance component is how much variation there is among the intercepts of the levels of the random effect. In the InsectSprays example, if the block had very little effect on the insect counts (all blocks are about the same), the variance component would be low (near zero). However, if there was a large amount of variation among the blocks (some blocks as very few insects and some had a lot), the variance component would be high. The concept of variance components is closely related to the coefficient of determination or \\(R^2\\). 5.3.1 R2 Let’s review the \\(R^2\\) using some fake data we will make up. The code below will make a dataset with 8 sites. At each site, temperature (temp) was measured, so just one temp per site. Body size was measured on 5 insects per site, with each measured indivdual getting a unique ID. Make and view the dataset below: set.seed(10) fakedata &lt;- data.frame(Site=factor(40), ID=factor(40), temp=double(40), size=double(40), stringsAsFactors = F) fakedata$Site &lt;- rep(1:8, each=5) fakedata$ID &lt;- rep(1:5, times=8) fakedata$temp &lt;- rep(c(10,18,12,15,8,11,10,16), each=5) fakedata$size &lt;- round(rnorm(40, (2*fakedata$temp), 8), 1) head(fakedata) ## Site ID temp size ## 1 1 1 10 20.1 ## 2 1 2 10 18.5 ## 3 1 3 10 9.0 ## 4 1 4 10 15.2 ## 5 1 5 10 22.4 ## 6 2 1 18 39.1 hist(fakedata$size) Make a plot of the data: ggplot(fakedata, aes(x=temp, y=size)) + geom_point() + geom_smooth(method=&quot;lm&quot;) + theme_bw(base_size=16) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 2.27: Size plotted against temp. Calculate R2 for linear model and linear mixed model. Obe nice thing about the simple lm() function is that it is easy to obtain the R2. lm1 &lt;- lm(size ~ temp , data=fakedata) summary(lm1) # look at R2 from MuMIn package ## ## Call: ## lm(formula = size ~ temp, data = fakedata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.101 -5.146 0.216 5.060 12.193 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.4118 4.5097 -1.422 0.163 ## temp 2.2515 0.3492 6.447 1.39e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.157 on 38 degrees of freedom ## Multiple R-squared: 0.5224, Adjusted R-squared: 0.5098 ## F-statistic: 41.57 on 1 and 38 DF, p-value: 1.39e-07 For the mixed model, where is the R2? lmm1 &lt;- glmmTMB(size ~ temp + (1|Site), data=fakedata) summary(lmm1) ## Family: gaussian ( identity ) ## Formula: size ~ temp + (1 | Site) ## Data: fakedata ## ## AIC BIC logLik deviance df.resid ## 273.4 280.2 -132.7 265.4 36 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## Site (Intercept) 11.84 3.441 ## Residual 36.83 6.068 ## Number of obs: 40, groups: Site, 8 ## ## Dispersion estimate for gaussian family (sigma^2): 36.8 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.4119 6.1743 -1.038 0.299 ## temp 2.2515 0.4781 4.709 2.49e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Load performance package to calculate R2 for mixed models. Note that we get two values, \\(R^2m\\) and \\(R^2c\\). \\(R^2m\\) is the marginal \\(R^2\\) for the fixed-effects and \\(R^2c\\) is the conditional \\(R^2\\) for the fixed plus random effects. The MuMIn package also has a function r.squaredGLMM() that calculates \\(R^2m\\) and \\(R^2c\\). Usually the results are identical, but sometimes not (as \\(R^2\\) for generalized mixed models are still being developed). So, use a bit of caution. For example, be suspitous if you see a very high \\(R^2m\\) (&gt;0.9). library(performance) r2(lmm1) ## # R2 for Mixed Models ## ## Conditional R2: 0.643 ## Marginal R2: 0.529 5.3.2 Chick weight example Load in the ChickWeight dataset. It contains weight (g) of small chickens grown on four different diets. Chickens were weighed every few days for 21 days. data(&quot;ChickWeight&quot;) ChickWeight$Diet &lt;- as.factor(ChickWeight$Diet) ?ChickWeight ## starting httpd help server ... done head(ChickWeight) ## Grouped Data: weight ~ Time | Chick ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 Plot out data ggplot(ChickWeight, aes(x=Time,y=weight))+ geom_point()+ facet_wrap(~Diet)+ geom_smooth(method=&quot;lm&quot;)+ theme_bw(base_size = 16) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 2.32: Chick weight by week on the four different diets. Construct a regular linear model ignoring chick cw0 &lt;- glmmTMB(weight ~ Time * Diet , data=ChickWeight) Anova(cw0) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: weight ## Chisq Df Pr(&gt;Chisq) ## Time 1761.751 1 &lt; 2.2e-16 *** ## Diet 113.477 3 &lt; 2.2e-16 *** ## Time:Diet 70.601 3 3.173e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Examine means at time 20 emmeans(cw0, pairwise~Diet, at=list(Time=20)) # All contrasts are significant ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## Diet emmean SE df lower.CL upper.CL ## 1 168 3.95 569 160 176 ## 2 201 5.17 569 191 211 ## 3 247 5.17 569 237 257 ## 4 225 5.30 569 215 235 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Diet1 - Diet2 -33.0 6.50 569 -5.084 &lt;.0001 ## Diet1 - Diet3 -78.9 6.50 569 -12.144 &lt;.0001 ## Diet1 - Diet4 -57.3 6.61 569 -8.674 &lt;.0001 ## Diet2 - Diet3 -45.9 7.30 569 -6.283 &lt;.0001 ## Diet2 - Diet4 -24.3 7.40 569 -3.279 0.0061 ## Diet3 - Diet4 21.6 7.40 569 2.923 0.0189 ## ## P value adjustment: tukey method for comparing a family of 4 estimates Construct a new model with chick as fixed effect cw1a &lt;- glmmTMB(weight ~ Time * Diet + Chick , data=ChickWeight) ## dropping columns from rank-deficient conditional model: Chick^47, Chick^48, Chick^49 summary(cw1a) ## look for R2 ## Family: gaussian ( identity ) ## Formula: weight ~ Time * Diet + Chick ## Data: ChickWeight ## ## AIC BIC logLik deviance df.resid ## 5431.7 5671.5 -2660.8 5321.7 523 ## ## ## Dispersion estimate for gaussian family (sigma^2): 584 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 196.2615 49.3936 3.973 7.08e-05 *** ## Time 6.6906 0.2474 27.047 &lt; 2e-16 *** ## Diet2 -133.6647 29.8694 -4.475 7.64e-06 *** ## Diet3 93.6606 132.3998 0.707 0.47931 ## Diet4 -800.0810 318.8249 -2.509 0.01209 * ## Chick.L 1502.0057 538.9094 2.787 0.00532 ** ## Chick.Q 1070.1507 554.1817 1.931 0.05348 . ## Chick.C 673.7465 362.1979 1.860 0.06286 . ## Chick^4 43.8753 67.7943 0.647 0.51751 ## Chick^5 -618.1626 301.4119 -2.051 0.04028 * ## Chick^6 -595.7249 280.7504 -2.122 0.03385 * ## Chick^7 -39.1387 18.4354 -2.123 0.03375 * ## Chick^8 451.8890 195.4582 2.312 0.02078 * ## Chick^9 364.8086 177.0176 2.061 0.03932 * ## Chick^10 16.9198 44.0390 0.384 0.70083 ## Chick^11 -178.3482 81.8023 -2.180 0.02924 * ## Chick^12 -177.5333 99.0864 -1.792 0.07318 . ## Chick^13 -116.1829 65.5923 -1.771 0.07651 . ## Chick^14 -20.6745 14.3656 -1.439 0.15010 ## Chick^15 89.9792 50.4586 1.783 0.07455 . ## Chick^16 163.1248 88.2217 1.849 0.06445 . ## Chick^17 146.0524 72.1376 2.025 0.04290 * ## Chick^18 6.7008 18.7118 0.358 0.72026 ## Chick^19 -200.1907 93.3114 -2.145 0.03192 * ## Chick^20 -200.9105 95.0470 -2.114 0.03453 * ## Chick^21 -20.5819 14.9446 -1.377 0.16845 ## Chick^22 145.1502 67.9424 2.136 0.03265 * ## Chick^23 156.5127 75.6180 2.070 0.03847 * ## Chick^24 61.5087 38.2141 1.610 0.10749 ## Chick^25 -36.2350 12.3280 -2.939 0.00329 ** ## Chick^26 -36.5757 33.8045 -1.082 0.27926 ## Chick^27 -82.5708 51.0420 -1.618 0.10573 ## Chick^28 -98.9502 50.2965 -1.967 0.04914 * ## Chick^29 -24.8506 16.0039 -1.553 0.12048 ## Chick^30 78.3099 42.4720 1.844 0.06521 . ## Chick^31 156.3767 77.9511 2.006 0.04485 * ## Chick^32 12.0154 8.0104 1.500 0.13362 ## Chick^33 60.8397 26.7850 2.271 0.02312 * ## Chick^34 87.6695 40.1077 2.186 0.02883 * ## Chick^35 17.7933 10.1954 1.745 0.08094 . ## Chick^36 42.9841 25.7234 1.671 0.09472 . ## Chick^37 -27.0019 16.9516 -1.593 0.11119 ## Chick^38 96.3421 50.8375 1.895 0.05808 . ## Chick^39 -189.9712 92.6935 -2.049 0.04042 * ## Chick^40 -95.4900 59.2927 -1.610 0.10729 ## Chick^41 92.7147 49.5890 1.870 0.06153 . ## Chick^42 -52.6442 19.0304 -2.766 0.00567 ** ## Chick^43 -96.4086 33.2667 -2.898 0.00375 ** ## Chick^44 -60.4537 24.9001 -2.428 0.01519 * ## Chick^45 -102.5664 45.1616 -2.271 0.02314 * ## Chick^46 -13.8247 13.8978 -0.995 0.31986 ## Chick^47 NA NA NA NA ## Chick^48 NA NA NA NA ## Chick^49 NA NA NA NA ## Time:Diet2 1.9185 0.4088 4.693 2.69e-06 *** ## Time:Diet3 4.7322 0.4088 11.576 &lt; 2e-16 *** ## Time:Diet4 2.9653 0.4142 7.159 8.11e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There are some problems with this model. Basically, there are so many levels of chick that the model errors out (runs out of degrees of freedom). This causes the Hessian matrix warning. Examine means at time 20 for the new model with the chick fixed-effect. emmeans(cw1a, pairwise~Diet, at=list(Time=20)) # Contrasts similar to above ## NOTE: A nesting structure was detected in the fitted model: ## Chick %in% Diet ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## Diet emmean SE df lower.CL upper.CL ## 1 165 3.07 523 159 171 ## 2 201 3.69 523 194 208 ## 3 247 3.69 523 239 254 ## 4 224 3.80 523 217 232 ## ## Results are averaged over the levels of: Chick ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Diet1 - Diet2 -35.4 4.80 523 -7.383 &lt;.0001 ## Diet1 - Diet3 -81.3 4.80 523 -16.951 &lt;.0001 ## Diet1 - Diet4 -58.9 4.88 523 -12.052 &lt;.0001 ## Diet2 - Diet3 -45.9 5.22 523 -8.798 &lt;.0001 ## Diet2 - Diet4 -23.5 5.30 523 -4.428 0.0001 ## Diet3 - Diet4 22.4 5.30 523 4.236 0.0002 ## ## Results are averaged over the levels of: Chick ## P value adjustment: tukey method for comparing a family of 4 estimates Construct model with chick as a random (block) effect. No more warnings when chick is a random effect! cw1 &lt;- glmmTMB(weight ~ Time * Diet + (1|Chick), data=ChickWeight) summary(cw1) ## look for variance component. Where is R2 ??? ## Family: gaussian ( identity ) ## Formula: weight ~ Time * Diet + (1 | Chick) ## Data: ChickWeight ## ## AIC BIC logLik deviance df.resid ## 5508.0 5551.6 -2744.0 5488.0 568 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## Chick (Intercept) 498.0 22.32 ## Residual 638.4 25.27 ## Number of obs: 578, groups: Chick, 50 ## ## Dispersion estimate for gaussian family (sigma^2): 638 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 31.5079 5.9114 5.330 9.82e-08 *** ## Time 6.7130 0.2573 26.086 &lt; 2e-16 *** ## Diet2 -2.8807 10.1920 -0.283 0.777 ## Diet3 -13.2565 10.1920 -1.301 0.193 ## Diet4 -0.3930 10.2008 -0.039 0.969 ## Time:Diet2 1.8962 0.4267 4.444 8.85e-06 *** ## Time:Diet3 4.7098 0.4267 11.037 &lt; 2e-16 *** ## Time:Diet4 2.9494 0.4323 6.823 8.91e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Examine means at time 20 for the new model with the chick random-effect emmeans(cw1, pairwise~Diet, at=list(Time=20)) # Contrasts similar to above ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## Diet emmean SE df lower.CL upper.CL ## 1 166 5.89 568 154 177 ## 2 201 8.04 568 185 217 ## 3 247 8.04 568 231 263 ## 4 224 8.10 568 208 240 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Diet1 - Diet2 -35.0 9.97 568 -3.516 0.0027 ## Diet1 - Diet3 -80.9 9.97 568 -8.120 &lt;.0001 ## Diet1 - Diet4 -58.6 10.00 568 -5.852 &lt;.0001 ## Diet2 - Diet3 -45.9 11.40 568 -4.035 0.0004 ## Diet2 - Diet4 -23.6 11.40 568 -2.063 0.1665 ## Diet3 - Diet4 22.3 11.40 568 1.958 0.2055 ## ## P value adjustment: tukey method for comparing a family of 4 estimates Print off only variance component cvar &lt;- VarCorr(cw1) print(cvar, comp=c(&quot;Variance&quot;,&quot;Std.Dev.&quot;)) ## ## Conditional model: ## Groups Name Variance Std.Dev. ## Chick (Intercept) 498.02 22.316 ## Residual 638.41 25.267 Print off anova table. Anova(cw1) ## Anova() from car package. ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: weight ## Chisq Df Pr(&gt;Chisq) ## Time 3088.536 1 &lt; 2.2e-16 *** ## Diet 20.221 3 0.0001527 *** ## Time:Diet 131.333 3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Examine model residuals. Residuals are not great, but we’ll fix this later. For now we will proceed with caution. plot(resid(cw1)~fitted(cw1)) Figure 5.2: Residual plots. Examine emmeans and contrasts emmeans(cw1, pairwise~Diet, at=list(Time=20)) # 4 of 6 differ at time 20 ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## Diet emmean SE df lower.CL upper.CL ## 1 166 5.89 568 154 177 ## 2 201 8.04 568 185 217 ## 3 247 8.04 568 231 263 ## 4 224 8.10 568 208 240 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Diet1 - Diet2 -35.0 9.97 568 -3.516 0.0027 ## Diet1 - Diet3 -80.9 9.97 568 -8.120 &lt;.0001 ## Diet1 - Diet4 -58.6 10.00 568 -5.852 &lt;.0001 ## Diet2 - Diet3 -45.9 11.40 568 -4.035 0.0004 ## Diet2 - Diet4 -23.6 11.40 568 -2.063 0.1665 ## Diet3 - Diet4 22.3 11.40 568 1.958 0.2055 ## ## P value adjustment: tukey method for comparing a family of 4 estimates Calculate R2 for the mixed-model r2(cw1) ## # R2 for Mixed Models ## ## Conditional R2: 0.873 ## Marginal R2: 0.773 Question: How much of the variance in weight is explained by Diet and Time? How much by Chick? Harder question: calculate the R2c by hand based (based on lecture notes) on the R2m and variance components (just to check) 5.4 Split-plot and nested designs library(tidyverse) library(glmmTMB) library(emmeans) library(MuMIn) library(agridat) ## install and load package for datasets library(multcomp) ## install and load package for multiple comparisons Some experimental treatments are easier to apply than others. Or, some treatments work better in bigger plots while others work better in smaller plots. For these logistical reasons (and others), ecologists often design their experiments with some nestedness. Lets take a look at the dataset below as an example. The ‘gomez.multilocsplitplot’ dataset contains data on rice yield from an experiment that manipulated nitrogen (6 levels) and genotype (2 levels). Logistically, it is much easier to apply nitrogen to big plots and then plant the two genotypes within those plots (see lecture notes for sketch of design). The experiment was setup at three locations (blocks). Based on the experimental design, do we have the same number of independent plots (ie. true replicates) for the nitrogen treatment and the genotype treatment? Remember, nitrogen was added to big (whole) plots and then the genotypes were planted within that (split-plots). So, what we have is a split-plot design. Let’s take a look at how to analyze these data. Load the rice data and process for analyses. data(&quot;gomez.multilocsplitplot&quot;) gomez.multilocsplitplot$nitro &lt;- as.factor(gomez.multilocsplitplot$nitro) gomez &lt;- gomez.multilocsplitplot head(gomez) ## loc nitro rep gen yield ## 1 L1 0 R1 G1 1979 ## 2 L1 30 R1 G1 4572 ## 3 L1 60 R1 G1 5630 ## 4 L1 90 R1 G1 7153 ## 5 L1 120 R1 G1 7223 ## 6 L1 150 R1 G1 7239 ?gomez.multilocsplitplot ggplot(gomez, aes(x=gen, y=yield, fill=nitro))+ geom_boxplot(outlier.shape = NA)+ geom_point(position = position_jitterdodge(jitter.height=0,jitter.width=.1))+ facet_wrap(~loc) Figure 5.3: Boxplot of gomez rice data. One addition aspect of the experiment is that there were additional replicates planted within each plot. This technically creates another level of nestedness, which we need to deal with. For now, we will average the data by loc, nitro, gen to account for pseudo-replication (n = 36 plots total). gomez_summarized &lt;- gomez %&gt;% group_by(loc,nitro,gen) %&gt;% summarize(yield=mean(yield, na.rm=T)) ## `summarise()` has grouped output by &#39;loc&#39;, &#39;nitro&#39;. You can override using the ## `.groups` argument. Now we are ready to fit a linear model to the data. To see the difference between no block, blocking, and split-plot designs, we will make a model for each of these. Take a look at the anova table for each, keeping an eye especially of the DF values. Start with a regular two-way anova using summarized dataset – no blocking. mm0 &lt;- glmmTMB(yield ~ gen*nitro, data=gomez_summarized) Anova(mm0) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: yield ## Chisq Df Pr(&gt;Chisq) ## gen 11.6485 1 0.0006426 *** ## nitro 170.7516 5 &lt; 2.2e-16 *** ## gen:nitro 1.4288 5 0.9211385 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Two-way anova with block as a random effect mm1 &lt;- glmmTMB(yield ~ gen*nitro+(1|loc), data=gomez_summarized) Anova(mm1) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: yield ## Chisq Df Pr(&gt;Chisq) ## gen 12.5997 1 0.0003858 *** ## nitro 184.6953 5 &lt; 2.2e-16 *** ## gen:nitro 1.5455 5 0.9077652 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Two-way anova with block and nitro nested within block as random effects. mm2 &lt;- glmmTMB(yield ~ gen*nitro+(1|loc/nitro), data=gomez_summarized) ## Warning in finalizeTMB(TMBStruc, obj, fit, h, data.tmb.old): Model convergence ## problem; non-positive-definite Hessian matrix. See vignette(&#39;troubleshooting&#39;) Anova(mm2) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: yield ## Chisq Df Pr(&gt;Chisq) ## gen 28.6780 1 8.547e-08 *** ## nitro 107.1280 5 &lt; 2.2e-16 *** ## gen:nitro 3.5214 5 0.6201 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Summary for split-plot model summary(mm2) ## Family: gaussian ( identity ) ## Formula: yield ~ gen * nitro + (1 | loc/nitro) ## Data: gomez_summarized ## ## AIC BIC logLik deviance df.resid ## NA NA NA NA 21 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## nitro:loc (Intercept) 1.586e+05 398.2967 ## loc (Intercept) 5.281e-01 0.7267 ## Residual 1.085e+05 329.4078 ## Number of obs: 36, groups: nitro:loc, 18; loc, 3 ## ## Dispersion estimate for gaussian family (sigma^2): 1.09e+05 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3392.67 298.41 11.369 &lt; 2e-16 *** ## genG2 305.88 268.96 1.137 0.255 ## nitro30 1652.82 422.02 3.916 8.99e-05 *** ## nitro60 2380.84 422.02 5.642 1.69e-08 *** ## nitro90 2993.67 422.02 7.094 1.31e-12 *** ## nitro120 3119.38 422.02 7.392 1.45e-13 *** ## nitro150 2794.73 422.02 6.622 3.54e-11 *** ## genG2:nitro30 464.86 380.37 1.222 0.222 ## genG2:nitro60 275.19 380.37 0.723 0.469 ## genG2:nitro90 532.03 380.37 1.399 0.162 ## genG2:nitro120 24.67 380.37 0.065 0.948 ## genG2:nitro150 396.02 380.37 1.041 0.298 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 emmeans for just for nitro and genotype) emmeans(mm2, pairwise~nitro) ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## nitro emmean SE df lower.CL upper.CL ## 0 3546 266 21 2992 4100 ## 30 5431 266 21 4877 5985 ## 60 6064 266 21 5510 6618 ## 90 6805 266 21 6251 7359 ## 120 6677 266 21 6123 7231 ## 150 6538 266 21 5984 7092 ## ## Results are averaged over the levels of: gen ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## nitro0 - nitro30 -1885 377 21 -5.004 0.0007 ## nitro0 - nitro60 -2518 377 21 -6.685 &lt;.0001 ## nitro0 - nitro90 -3260 377 21 -8.652 &lt;.0001 ## nitro0 - nitro120 -3132 377 21 -8.313 &lt;.0001 ## nitro0 - nitro150 -2993 377 21 -7.944 &lt;.0001 ## nitro30 - nitro60 -633 377 21 -1.681 0.5582 ## nitro30 - nitro90 -1374 377 21 -3.648 0.0163 ## nitro30 - nitro120 -1246 377 21 -3.309 0.0342 ## nitro30 - nitro150 -1107 377 21 -2.940 0.0735 ## nitro60 - nitro90 -741 377 21 -1.968 0.3922 ## nitro60 - nitro120 -613 377 21 -1.628 0.5904 ## nitro60 - nitro150 -474 377 21 -1.259 0.8030 ## nitro90 - nitro120 128 377 21 0.340 0.9993 ## nitro90 - nitro150 267 377 21 0.709 0.9788 ## nitro120 - nitro150 139 377 21 0.369 0.9990 ## ## Results are averaged over the levels of: gen ## P value adjustment: tukey method for comparing a family of 6 estimates emmeans(mm2, pairwise~gen) ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## gen emmean SE df lower.CL upper.CL ## G1 5550 122 21 5296 5803 ## G2 6138 122 21 5884 6391 ## ## Results are averaged over the levels of: nitro ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## G1 - G2 -588 110 21 -5.355 &lt;.0001 ## ## Results are averaged over the levels of: nitro 5.4.1 Additional nesting Above we dealt with the extra replicates by averaging them to avoid pseudo-replication. However, we can ully embrace the nestedness and include plot as a random effect instead of averaging (n=108 data points, use them all!). Let’s do this now and compare the results to the model mm2 above. Two-way anova with block, nitro, and gen nested within block as random effects, using the full dataset. mm3 &lt;- glmmTMB(yield ~ gen*nitro+(1|loc/nitro/gen), data=gomez) ## Warning in finalizeTMB(TMBStruc, obj, fit, h, data.tmb.old): Model convergence ## problem; non-positive-definite Hessian matrix. See vignette(&#39;troubleshooting&#39;) Anova(mm3) ## identical to averaged model in mm2 ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: yield ## Chisq Df Pr(&gt;Chisq) ## gen 16.6251 1 4.554e-05 *** ## nitro 243.6201 5 &lt; 2.2e-16 *** ## gen:nitro 2.0375 5 0.8439 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mm3) ## additional variance component, so no information sacrificed ## Family: gaussian ( identity ) ## Formula: yield ~ gen * nitro + (1 | loc/nitro/gen) ## Data: gomez ## ## AIC BIC logLik deviance df.resid ## NA NA NA NA 92 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## gen:nitro:loc (Intercept) 3.756e+00 1.938e+00 ## nitro:loc (Intercept) 2.267e-08 1.506e-04 ## loc (Intercept) 7.300e-01 8.544e-01 ## Residual 5.618e+05 7.495e+02 ## Number of obs: 108, groups: gen:nitro:loc, 36; nitro:loc, 18; loc, 3 ## ## Dispersion estimate for gaussian family (sigma^2): 5.62e+05 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3392.52 249.85 13.578 &lt; 2e-16 *** ## genG2 306.14 353.34 0.866 0.386 ## nitro30 1652.49 353.34 4.677 2.91e-06 *** ## nitro60 2380.66 353.34 6.738 1.61e-11 *** ## nitro90 2994.36 353.34 8.475 &lt; 2e-16 *** ## nitro120 3119.27 353.34 8.828 &lt; 2e-16 *** ## nitro150 2795.04 353.34 7.910 2.56e-15 *** ## genG2:nitro30 464.71 499.69 0.930 0.352 ## genG2:nitro60 275.15 499.69 0.551 0.582 ## genG2:nitro90 531.64 499.69 1.064 0.287 ## genG2:nitro120 24.81 499.69 0.050 0.960 ## genG2:nitro150 395.80 499.69 0.792 0.428 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now look at emmeans for nitro and gen emmeans(mm3, pairwise~nitro) ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## nitro emmean SE df lower.CL upper.CL ## 0 3546 177 92 3195 3896 ## 30 5430 177 92 5080 5781 ## 60 6064 177 92 5713 6415 ## 90 6806 177 92 6455 7157 ## 120 6677 177 92 6326 7028 ## 150 6539 177 92 6188 6889 ## ## Results are averaged over the levels of: gen ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## nitro0 - nitro30 -1885 250 92 -7.544 &lt;.0001 ## nitro0 - nitro60 -2518 250 92 -10.079 &lt;.0001 ## nitro0 - nitro90 -3260 250 92 -13.049 &lt;.0001 ## nitro0 - nitro120 -3132 250 92 -12.534 &lt;.0001 ## nitro0 - nitro150 -2993 250 92 -11.979 &lt;.0001 ## nitro30 - nitro60 -633 250 92 -2.535 0.1247 ## nitro30 - nitro90 -1375 250 92 -5.505 &lt;.0001 ## nitro30 - nitro120 -1247 250 92 -4.990 &lt;.0001 ## nitro30 - nitro150 -1108 250 92 -4.435 0.0004 ## nitro60 - nitro90 -742 250 92 -2.970 0.0428 ## nitro60 - nitro120 -613 250 92 -2.455 0.1486 ## nitro60 - nitro150 -475 250 92 -1.900 0.4088 ## nitro90 - nitro120 129 250 92 0.514 0.9955 ## nitro90 - nitro150 267 250 92 1.070 0.8922 ## nitro120 - nitro150 139 250 92 0.555 0.9936 ## ## Results are averaged over the levels of: gen ## P value adjustment: tukey method for comparing a family of 6 estimates emmeans(mm3, pairwise~gen) ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## gen emmean SE df lower.CL upper.CL ## G1 5549 102 92 5347 5752 ## G2 6138 102 92 5935 6340 ## ## Results are averaged over the levels of: nitro ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## G1 - G2 -588 144 92 -4.077 0.0001 ## ## Results are averaged over the levels of: nitro Examine pairwise comparisons as compact letter displays (ie. CLD, sometimes called tukey groupings). Treatments that share the same number in the “.group” column do not differ significantly, while treatments that with different numbers are significantly different. Oftentimes people use letters rather than numbers. In certain cases the CLD can be convenient, although most times it is better to directly report the contrasts. Nevertheless, lets go through an example here. First calculate the emmeans and then the CLD. mm3em &lt;- emmeans(mm3, pairwise~nitro) ## NOTE: Results may be misleading due to involvement in interactions cld(mm3em) ## nitro emmean SE df lower.CL upper.CL .group ## 0 3546 177 92 3195 3896 1 ## 30 5430 177 92 5080 5781 2 ## 60 6064 177 92 5713 6415 23 ## 150 6539 177 92 6188 6889 34 ## 120 6677 177 92 6326 7028 34 ## 90 6806 177 92 6455 7157 4 ## ## Results are averaged over the levels of: gen ## Confidence level used: 0.95 ## P value adjustment: tukey method for comparing a family of 6 estimates ## significance level used: alpha = 0.05 ## NOTE: If two or more means share the same grouping symbol, ## then we cannot show them to be different. ## But we also did not show them to be the same. Extract means and make plot of nitrogen emmeans with CLD. I have manually added the letters based on the cld() table above. n1 &lt;- emmeans(mm3, ~nitro) %&gt;% as.data.frame() ## NOTE: Results may be misleading due to involvement in interactions ggplot(n1, aes(x=nitro, y=emmean)) + geom_point(size=5) + geom_errorbar(aes(ymin=lower.CL, ymax=upper.CL), width=0, lwd=2) + ylab(&quot;yield (g) +/- 95% CI&quot;) + theme_bw(base_size = 20)+ annotate(&quot;text&quot;, x=c(1,2,3,4,5,6), y=7750, label=c(&quot;A&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;), size=10) Figure 2.41: Rice yield in different nitrogen treatments. Letters indicate significant differences based on tukey-adjusted p-values. 5.4.2 R CHALLENGE In this challenge we are interested in answering the question: Do grazing or Nitrogen affect insect abundance? The experiment measured insect abundances in experimental plots with nitrogen addition (control, low, medium, and high amounts of N added) and different grazing regimes (not grazed and grazed). See the image below for the physical layout of the plots. Figure 4.17: Example of two sites of the nitrogen x grazing experimental design. Based on the experimental design, complete the questions below: Construct an appropriate linear model Model assumptions met? Do grazing or Nitrogen affect insect abundance? How do the results change depending on whether you include a block or split-plot? 4a. How big is the blocking and split-plot effect? Load dataset and reorder levels. d1 &lt;-read_csv(&quot;InsectData.csv&quot;) ## Rows: 48 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): N_Add, site, grazed ## dbl (1): abund ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(d1) ## # A tibble: 6 × 4 ## abund N_Add site grazed ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 25 High.N A YES ## 2 24 Low.N A YES ## 3 20 Med.N A YES ## 4 17 No.N A YES ## 5 20 High.N A NO ## 6 22 Low.N A NO d1$N_Add &lt;- factor(d1$N_Add, levels=c(&quot;No.N&quot;,&quot;Low.N&quot;,&quot;Med.N&quot;,&quot;High.N&quot;)) ## reorder factor levels Couple plots to begin ggplot(d1, aes(x=abund)) + geom_histogram(aes(y=..density..), color=&quot;white&quot;, fill=&quot;grey&quot;,bins=8)+ geom_density(alpha=.5, color=&quot;red&quot;, lwd=1.5) + labs(title=&quot;histogram of raw data&quot;) + theme_bw() + theme(text = element_text(size=18)) ## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ggplot(d1, aes(y=abund, x=grazed, fill=N_Add))+ geom_boxplot()+ geom_point(position = position_jitterdodge(jitter.height=0,jitter.width=.1), size=3, stroke=1.5, pch=21, color=&quot;grey&quot;)+ scale_fill_manual(values=c(&quot;white&quot;,&quot;grey90&quot;,&quot;grey50&quot;,&quot;grey25&quot;))+ theme_bw(base_size = 18) 5.5 Repeated Measures Many reseach projects involve collecting data on the same units through time. For example, plant ecologists may be interested in monitoring changes is plant richness in plots through time or stream biogeochemists often monitor seasonal changes in stream chemistry. These repeated measures are a unique case of nestedness and therefore many on the principles we learned about regarding split-plots also apply here. Basically, the unit being repeatedly sampled (e.g. plot or stream) is the whole plot while time is the split-plot. There are some more complexities of repeated measures, sometimes called temporal autocorrelation, that we will get to in this section. First, lets load packages and look at the chick weight data from before. LOAD AND PROCESSES DATA library(tidyverse) library(car) library(emmeans) library(viridis) library(MuMIn) library(glmmTMB) data(&quot;ChickWeight&quot;) ChickWeight$Diet &lt;- as.factor(ChickWeight$Diet) ?ChickWeight Plot out the chick weight data ggplot(data=ChickWeight)+ geom_point(data=ChickWeight, aes(x=Time,y=weight, color=as.numeric(Chick)))+ scale_color_viridis() + facet_wrap(~Diet)+ geom_smooth(data=ChickWeight, method=&quot;lm&quot;,aes(x=Time,y=weight))+ theme_bw(base_size = 16) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 5.4: Chick weight data. We’ve used the chick weight data before, but to refresh there are 50 chicks each given one of four diets. Chicks are weighed through time and we are interested in comparing the growth rate or final weight among the four diets. We will start by making the same model we used in Section 5.3.2 for looking at variance components. The model has weight as the response variable, time as the (fixed-effect) predictor and Chick as a random (block) effect. cw1 &lt;- glmmTMB(weight ~ Time * Diet + (1|Chick), data=ChickWeight) summary(cw1) ## Family: gaussian ( identity ) ## Formula: weight ~ Time * Diet + (1 | Chick) ## Data: ChickWeight ## ## AIC BIC logLik deviance df.resid ## 5508.0 5551.6 -2744.0 5488.0 568 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## Chick (Intercept) 498.0 22.32 ## Residual 638.4 25.27 ## Number of obs: 578, groups: Chick, 50 ## ## Dispersion estimate for gaussian family (sigma^2): 638 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 31.5079 5.9114 5.330 9.82e-08 *** ## Time 6.7130 0.2573 26.086 &lt; 2e-16 *** ## Diet2 -2.8807 10.1920 -0.283 0.777 ## Diet3 -13.2565 10.1920 -1.301 0.193 ## Diet4 -0.3930 10.2008 -0.039 0.969 ## Time:Diet2 1.8962 0.4267 4.444 8.85e-06 *** ## Time:Diet3 4.7098 0.4267 11.037 &lt; 2e-16 *** ## Time:Diet4 2.9494 0.4323 6.823 8.91e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Anova(cw1) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: weight ## Chisq Df Pr(&gt;Chisq) ## Time 3088.536 1 &lt; 2.2e-16 *** ## Diet 20.221 3 0.0001527 *** ## Time:Diet 131.333 3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Remember that the residuals from this model look weird. The histogram is great, but the residuals ~ fitted plot has funneling and curvature. hist(resid(cw1)) ## resids from first model Figure 5.5: Histograms of residuals. plot(residuals(cw1)~fitted(cw1)) ## resids from first model Figure 5.6: Histograms of residuals. One of the ‘unique’ aspects of temporal data is that measurements closer in time are likely to be much more highly correlated than time points further apart. For example, if we correlate all the chick weights from week 2 with week 4, they will be very highly correlated, whereas week 2 vs week 10 will be less correlated: Figure 5.7: Correlations of chick weights between different weeks. We can capture that temporal autocorrelation by fitting a special covariation or correlation structure via a random effect (more about this in lecture). Below is a model incorporating an autoregressive covariance (ar1) structure to account for the temporal autocorrelation: cw1ar &lt;- glmmTMB(weight ~ Time * Diet + ar1(0 + as.factor(Time)|Chick), data=ChickWeight) In the summary() output notice that for the ‘Chick’ random effect, we have the usual variance component plus now we also get a correlation coeffiect “Corr”, which is 0.97, which is very high indicating strong temporal autocorrelation. The parameter estimates are a little different, but everything else is similar to the previous model. summary(cw1ar) ## Family: gaussian ( identity ) ## Formula: weight ~ Time * Diet + ar1(0 + as.factor(Time) | Chick) ## Data: ChickWeight ## ## AIC BIC logLik deviance df.resid ## 4484.3 4532.2 -2231.1 4462.3 567 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. Corr ## Chick as.factor(Time)0 1.651e+03 4.063e+01 0.97 (ar1) ## Residual 2.620e-07 5.118e-04 ## Number of obs: 578, groups: Chick, 50 ## ## Dispersion estimate for gaussian family (sigma^2): 2.62e-07 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 40.3595 9.0781 4.446 8.76e-06 *** ## Time 6.0679 0.3465 17.510 &lt; 2e-16 *** ## Diet2 -0.9268 15.7205 -0.059 0.952989 ## Diet3 -2.3392 15.7239 -0.149 0.881736 ## Diet4 -1.0019 15.7209 -0.064 0.949183 ## Time:Diet2 2.2042 0.5837 3.776 0.000159 *** ## Time:Diet3 4.8553 0.5837 8.319 &lt; 2e-16 *** ## Time:Diet4 3.1306 0.5864 5.339 9.36e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Anova(cw1ar) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: weight ## Chisq Df Pr(&gt;Chisq) ## Time 1459.447 1 &lt; 2.2e-16 *** ## Diet 11.624 3 0.00879 ** ## Time:Diet 75.942 3 2.277e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s take a look at the residuals: hist(residuals(cw1ar)) ## hist looks ok Figure 5.8: Histograms of residuals. plot(residuals(cw1ar)~fitted(cw1ar)) ## resids look great Figure 5.9: Histograms of residuals. They look great! It seems that the model that accounts for temporal autocorrelation fits much better than just incorporating chick as a regular random effect. We can also compare the AIC of the two models to make sure. Indeed, it is much lower for the ar1 model. AIC(cw0,cw1ar) ## df AIC ## cw0 9 5728.997 ## cw1ar 11 4484.270 Now we can look at the emmeans for the two models. Notice the estimates are similar, but the SE is much higher for the AR1 model. This changes the p-values for the contrasts a little bit too. It is important we interpret the results from the AR1 model because it is much better, even though the SEs are higher. emmeans(cw1, pairwise~Diet, at=list(Time=20)) # Yes, 4 of 6 differ at time 20 ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## Diet emmean SE df lower.CL upper.CL ## 1 166 5.89 568 154 177 ## 2 201 8.04 568 185 217 ## 3 247 8.04 568 231 263 ## 4 224 8.10 568 208 240 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Diet1 - Diet2 -35.0 9.97 568 -3.516 0.0027 ## Diet1 - Diet3 -80.9 9.97 568 -8.120 &lt;.0001 ## Diet1 - Diet4 -58.6 10.00 568 -5.852 &lt;.0001 ## Diet2 - Diet3 -45.9 11.40 568 -4.035 0.0004 ## Diet2 - Diet4 -23.6 11.40 568 -2.063 0.1665 ## Diet3 - Diet4 22.3 11.40 568 1.958 0.2055 ## ## P value adjustment: tukey method for comparing a family of 4 estimates emmeans(cw1ar, pairwise~Diet, at=list(Time=20)) # Yes, 4 of 6 differ at time 20 ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## Diet emmean SE df lower.CL upper.CL ## 1 162 9.15 567 144 180 ## 2 205 12.60 567 180 230 ## 3 256 12.60 567 232 281 ## 4 223 12.70 567 198 248 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Diet1 - Diet2 -43.2 15.6 567 -2.766 0.0298 ## Diet1 - Diet3 -94.8 15.6 567 -6.072 &lt;.0001 ## Diet1 - Diet4 -61.6 15.6 567 -3.938 0.0005 ## Diet2 - Diet3 -51.6 17.9 567 -2.888 0.0210 ## Diet2 - Diet4 -18.5 17.9 567 -1.030 0.7317 ## Diet3 - Diet4 33.2 17.9 567 1.851 0.2506 ## ## P value adjustment: tukey method for comparing a family of 4 estimates 5.5.1 R CHALLANGE - REPEATED MEASURES Take a look at the ‘harris.wateruse’ dataset. The dataset contains information on water use by two species of horticultural trees (S1 and S2). The dataset also contains two age groups, although we will focus on just one group (A1). 10 trees of each species were assessed for water usage for approximately every 5 days over the course of a year. There are some missing values. library(agridat) wat &lt;- harris.wateruse %&gt;% filter(age==&#39;A1&#39;) head(wat) ## species age tree day water ## 1 S2 A1 T01 161 0.92 ## 2 S2 A1 T01 166 0.93 ## 3 S2 A1 T01 170 0.86 ## 4 S2 A1 T01 174 0.95 ## 5 S2 A1 T01 178 0.90 ## 6 S2 A1 T01 184 1.11 str(wat) ## &#39;data.frame&#39;: 520 obs. of 5 variables: ## $ species: Factor w/ 2 levels &quot;S1&quot;,&quot;S2&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ age : Factor w/ 2 levels &quot;A1&quot;,&quot;A2&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ tree : Factor w/ 40 levels &quot;T01&quot;,&quot;T02&quot;,&quot;T03&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ day : int 161 166 170 174 178 184 189 194 199 206 ... ## $ water : num 0.92 0.93 0.86 0.95 0.9 1.11 NA 1.25 1.14 1.07 ... Plot data ggplot(data=wat, aes(x=day, y=water, color=species))+ geom_point()+ scale_color_viridis(discrete = T, end=.8) + theme_bw(base_size = 16) Figure 5.10: Plot of water usage by two horticultural tree species through time. To Do: Fit several models, one that accounts for tree ID as a regular random effect and one that accounts for temporal autocorrelation. Additionally, it seems like there is some non-linearity so try a quadratic term. Choose the right fixed effects to answer the question: do the species differ in water use and does this change through time? Examine the residuals and AIC for your models. Which is best? If your model has temporal autocorrelation, how strong is it? Does water use differ between the species at these two time points: 175 days and 275 days? 5.6 Additional LMM reading For those of you interested in other aspects of GLMMs, we will cover other ways we tangle with statistics and data with them in the next module. However, even with another module dedicated to GLMMs we will be barely scratching the surface. If you want more information on what else we can do with these powerful models check out the box below! Box 1. Mixed Models. Mixed-effect models are powerful tools that can help gain stronger inference or predictive power in your analyses. This is especially seen in experimental or hierarchically structured designs of studies. These models have been used to account for pseudoreplication effects but please be wary of this! Because models that don’t have the proper random effect structures may actually not be accounting for the pseudoreplication! For more insight into this please read this paper. So what else can be done with GLMMs? One thing it allows is the modeling of specific spatial and temporal structures to account for spatial or temporal autocorrelation! This is a problem with ecological data. In the context of spatial autocorrelation, patterns in your data may be a result of things being generally more similar to one another due to the spatial distance between data points than due to actual biological processes. GLMMs can account for this! For a crash course on this, check this link out! "],["generalized-linear-mixed-models.html", "6 Generalized linear mixed models 6.1 Poisson and NB GLMMs 6.2 Binomial GLMMs 6.3 Beta Distributions 6.4 Zero-Inflated Models 6.5 Model App exercise", " 6 Generalized linear mixed models After completing this module, students will be able to: 6.1 Construct models with both fixed and random effects and non-normal distributions 6.1 Poisson and NB GLMMs In this module we will go over a variety of advanced models. We will start with Poisson generalized linear mixed effect models. The poisson error distribution is often used to model count data and because of this it is a popular tool for most biologist or ecologists as biological or ecological data is often in the form of counts (e.g., species richness). Let’s first load the libraries we will need: library(tidyverse) library(emmeans) library(car) library(agridat) library(glmmTMB) library(DHARMa) library(performance) library(MuMIn) library(bbmle) library(aods3) library(boot) Let’s load in the primary data set we will be working with and read about the beall.webworms data set. The variables of interest are the y-count of webworms, spray- spray treatment, and lead-lead treatment. Don’t worry about the blocks or other variables for now. data(&quot;beall.webworms&quot;) d1 &lt;- beall.webworms ?beall.webworms ## info about the beall.webworms dataset ## starting httpd help server ... done We can also glimpse the first several rows of the data: head(d1) ## view data set ## row col y block trt spray lead ## 1 1 1 1 B1 T1 N N ## 2 2 1 0 B1 T1 N N ## 3 3 1 1 B1 T1 N N ## 4 4 1 3 B1 T1 N N ## 5 5 1 6 B1 T1 N N ## 6 6 1 0 B2 T1 N N Let’s examine a plot of data where we look at the raw data points as well as violin plots of the y response variable on the y axis and the x-axis as the spray treatment and lead treatment type: ggplot(d1, aes(x=spray, y=y, fill=lead)) + geom_violin(scale=&quot;width&quot;, adjust=2) + geom_point(position = position_jitterdodge(jitter.width=.5, jitter.height=.1, dodge.width = 1), alpha=.1)+ facet_wrap(~block) Here we create two models, r1 and r2. They were previously used in module 4C. So let’s first create r1. r1 will be a Poisson GLM with the repose variable of y being ,modeled as a function of both the additive and interactive effects of spray and lead treatments. r1 &lt;- glm(y ~ spray * lead, data=d1, family=&quot;poisson&quot;) summary(r1) ## ## Call: ## glm(formula = y ~ spray * lead, family = &quot;poisson&quot;, data = d1) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.33647 0.04688 7.177 7.12e-13 *** ## sprayY -1.02043 0.09108 -11.204 &lt; 2e-16 *** ## leadY -0.49628 0.07621 -6.512 7.41e-11 *** ## sprayY:leadY 0.29425 0.13917 2.114 0.0345 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1955.9 on 1299 degrees of freedom ## Residual deviance: 1720.4 on 1296 degrees of freedom ## AIC: 3125.5 ## ## Number of Fisher Scoring iterations: 6 Anova(r1) ## Analysis of Deviance Table (Type II tests) ## ## Response: y ## LR Chisq Df Pr(&gt;Chisq) ## spray 188.707 1 &lt; 2.2e-16 *** ## lead 42.294 1 7.853e-11 *** ## spray:lead 4.452 1 0.03485 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 emmeans(r1, ~spray:lead, type=&#39;response&#39;) ## spray lead rate SE df asymp.LCL asymp.UCL ## N N 1.400 0.0656 Inf 1.277 1.535 ## Y N 0.505 0.0394 Inf 0.433 0.588 ## N Y 0.852 0.0512 Inf 0.758 0.959 ## Y Y 0.412 0.0356 Inf 0.348 0.488 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale We check overdispersion in the model with the check_overdisperson wrapper: check_overdispersion(r1) # overdispersion ratio calculator from performance ## # Overdispersion test ## ## dispersion ratio = 1.355 ## Pearson&#39;s Chi-Squared = 1755.717 ## p-value = &lt; 0.001 ## Overdispersion detected. Now let’s implement the second model, r2, a negative binomial error distribution. While most of the information regarding this error distribution and its comparison to the Poisson should be covered in the main lecture material a brief explanation on when and why you may implement the negative binomial is as follows: The Poisson distribution assumes that the mean and the variance of the distribution are the same, so as the mean increases the variance increases the same amount. The negative binomial assumes that these two components are different, usually with the variance being greater than the mean (often the case with real world data). Poisson models can result in over-dispersion because the variance in real data is often greater than the means. This issue can lead to several problems such as but not limited to: poor model convergence, weak effect sizes of model coefficients, or p-values that are way too low. The negative binomial distribution can be used in the cases above to improve model fit or reach model convergence. Oftentimes it performs better than the poisson even if both models converge successfully. Visually examining simulated residuals is usually the best way to assess model fit. Another way is to compare AIC (Akaike information criterion) of Poisson vs negative binomial models. r2 &lt;- glmmTMB(y ~ spray * lead, data=d1, family=&quot;nbinom2&quot;) Anova(r2) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: y ## Chisq Df Pr(&gt;Chisq) ## spray 125.5047 1 &lt; 2.2e-16 *** ## lead 26.8005 1 2.256e-07 *** ## spray:lead 3.3942 1 0.06542 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 emmeans(r2, ~spray:lead, type=&#39;response&#39;) ## spray lead response SE df asymp.LCL asymp.UCL ## N N 1.400 0.0855 Inf 1.242 1.578 ## Y N 0.505 0.0441 Inf 0.425 0.599 ## N Y 0.852 0.0611 Inf 0.741 0.981 ## Y Y 0.412 0.0391 Inf 0.342 0.497 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale Now let’s simulate residuals for poisson: plot(simulateResiduals(r1)) ## DHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = &#39;bootstrap&#39;. See ?testOutliers for details hist(simulateResiduals(r1)) ## histogram should be flat Here are the residuals for the negative binomial model: plot(simulateResiduals(r2)) hist(simulateResiduals(r2)) ## histogram should be flat We can also compare the AIC: AIC(r1,r2) ## df AIC ## r1 4 3125.478 ## r2 5 3052.969 6.1.1 Improving the models: What’s next? How can we improve the models? What’s next? Are there any aspects of the experimental design missing from the model that we should maybe account for similar to the grass cover predictor from the previous module? We did notice some dispersion issues in the previous model and one way we can wrestle and address this is to implement an individual-level random effect to use for overdispersed Poission GLMs. For more information of this approach please see this paper. To do this we can construct models that includes any missing factors. In the case of overdispersed Poission GLMs we can create an observation level random effect variable by simple creating another column titled obs and filling it with unique number for each observation such as a sequence of numbers from 1 to the number of observations we have in the dataset: d1$obs &lt;- 1:length(d1$y) ## makes a unique number for each row in dataset We can also create other models that include different random effects that account for blocks. In total let’s run 6 different models. For the sake of clarity we have built a table to showcase the 6 different models and we have also coded them in: Model name Response Fixed Effect(s) Random Effect(s) Error Distribution r0 y spray * lead none Gaussian r1 y spray * lead none Poisson r2 y spray * lead none Neg. Binomial r3 y spray * lead Obs. level random effect for overdispersion Poisson r4 y spray * lead Block random effect Neg. Binomial r5 y spray * lead Both Obs. level and Block random effects Poisson r0 &lt;- glmmTMB(y ~ spray * lead, data=d1, family=&quot;gaussian&quot;) ### gaussian glm just to see how bad it really is r1 &lt;- glmmTMB(y ~ spray * lead, data=d1, family=&quot;poisson&quot;) ### poisson glm r2 &lt;- glmmTMB(y ~ spray * lead, data=d1, family=&quot;nbinom2&quot;) ### nb glm r3 &lt;- glmmTMB(y ~ spray * lead + (1|obs), data=d1, family=&quot;poisson&quot;) ### overdispersed poisson glm r4 &lt;- glmmTMB(y ~ spray * lead + (1|block), data=d1, family=&quot;nbinom2&quot;) ### nb w/ block r5 &lt;- glmmTMB(y ~ spray * lead + (1|obs) + (1|block), data=d1, family=&quot;poisson&quot;) ### OD poisson w/ block Six models above differ only distribution and random effects. Fixed effects are the same. We can now use the AIC framework for selecting the most appropriate or most plausible model, distribution, and random effect structure. One key thing to remember is that the more complex models will be penalized in the calculation of AIC values: #### for model selection, use AIC or likihood ratio test model.sel(r0,r1,r2,r3,r4,r5) ## from MuMIn ## Model selection table ## cnd((Int)) dsp((Int)) cnd(led) cnd(spr) cnd(led:spr) family random df ## r4 0.2835 + + + + n2(lg) c(b) 6 ## r5 0.1399 + + + + ps(lg) c(o)+c(b) 6 ## r3 0.1042 + + + + ps(lg) c(o) 5 ## r2 0.3365 + + + + n2(lg) 5 ## r1 0.3365 + + + + ps(lg) 4 ## r0 1.4000 + + + + gs(id) 5 ## logLik AICc delta weight ## r4 -1497.706 3007.5 0.00 0.903 ## r5 -1499.931 3011.9 4.45 0.097 ## r3 -1518.031 3046.1 38.63 0.000 ## r2 -1521.484 3053.0 45.54 0.000 ## r1 -1558.739 3125.5 118.03 0.000 ## r0 -1929.308 3868.7 861.18 0.000 ## Abbreviations: ## family: gs(id) = &#39;gaussian(identity)&#39;, n2(lg) = &#39;nbinom2(log)&#39;, ## ps(lg) = &#39;poisson(log)&#39; ## Models ranked by AICc(x) ## Random terms: ## c(b): cond(1 | block) ## c(o): cond(1 | obs) AICtab(r0,r1,r2,r3,r4,r5,base=T,logLik=T,weights=T) ## from bbmle ## logLik AIC dLogLik dAIC df weight ## r4 -1497.7 3007.4 431.6 0.0 6 0.903 ## r5 -1499.9 3011.9 429.4 4.5 6 0.097 ## r3 -1518.0 3046.1 411.3 38.7 5 &lt;0.001 ## r2 -1521.5 3053.0 407.8 45.6 5 &lt;0.001 ## r1 -1558.7 3125.5 370.6 118.1 4 &lt;0.001 ## r0 -1929.3 3868.6 0.0 861.2 5 &lt;0.001 Looks like the best model according to AIC was r4, the negative binomial GLMM with a random effect for blocks. Of course we would also want to check the model residuals and glance at the parameter estimates and SE to make sure they look reasonable. 6.2 Binomial GLMMs In this section we look at the binomial distribution. We often implement this distribution when we have a binary outcome in our data. For example survival is often coded as a binary where the only results can be that there is survival or there is no survival. Let’s load in the data we will use: data(&quot;wheatley.carrot&quot;) ?wheatley.carrot Let’s clean up the data by filtering out any insecticide value that was coded as ‘nil’. Let’s also create a new variable called propdmg which stand for damage proportion. This new variable is calculated by dividing the number of damaged plants (damaged) by the number of total plants (total). dat1 &lt;- wheatley.carrot %&gt;% filter(insecticide!=&#39;nil&#39;) dat1$propdmg &lt;- dat1$damaged/dat1$total head(dat1) ## treatment insecticide depth rep damaged total propdmg ## 1 T01 diazinon 1.0 R1 120 187 0.64171123 ## 2 T02 diazinon 2.5 R1 60 184 0.32608696 ## 3 T03 diazinon 5.0 R1 35 179 0.19553073 ## 4 T04 diazinon 10.0 R1 5 178 0.02808989 ## 5 T05 diazinon 25.0 R1 66 182 0.36263736 ## 6 T06 disulfoton 1.0 R1 97 187 0.51871658 Let’s visualize the distribution of the new variable as well: hist(dat1$propdmg) Now let’s plot out some of the data. Let’s visualize the damage proportion across different depths as well as the insecticide variable (diazinon vs. disulfoton). Because the variable depth is so wide ranging it’ll be easier for us to visualize the axis if depth is log-transformed. ggplot(dat1, aes(x=log(depth), y=(damaged/total), color=insecticide)) + geom_point(size=3) + theme_bw(base_size = 16) Figure 2.13: Plot of carrot data. It would be useful to also plot a line of best fit across these data points. Note that for plotting we can use the “beta_family” although for analysis we use “binomial”. ggplot(dat1, aes(x=log(depth), y=(damaged/total), color=insecticide)) + geom_point(size=3) + geom_smooth(method=&#39;glm&#39;, method.args=list(family=&quot;beta_family&quot;), formula = y~x) + theme_bw(base_size = 16) ## Warning: deviance residuals not defined for family &#39;beta&#39;: returning NA ## Warning: deviance residuals not defined for family &#39;beta&#39;: returning NA Figure 6.1: Plot of carrot data with best fit line. From the plot above it looks like the best fit lines do not necessarily fit or follow the raw data well. Look at the red beta line above and notice how the line and the confidence intervals do not overlap with almost 3 different types of depth groups. This is one of the more important parts of visualizing your data before using models because we now see potential evidence that our data follow a possible non-linear trend. To confirm this we can use ggplot2 to plot a more non-linear trend line while using the Beta family. To do this we have to switch the formula used in the formula argument of geom_smooth from y~x to y~x+I(x^2). This change essentially means that the model used to fit the trend line is actually a quadratic model (notice the extra squared term). More on quadratic models Note: A quadratic model is part of larger family of models called polynomial models. Polynomial models are used to model non-linear trends BUT they are still essentially linear regressions. One way to think about these sort of models is that we are essentially modeling the trends to have distinct slopes. Keep in mind, the interpretations of the models can be complicated and are often misinterpreted! Some decent resources for helping you understand these models can be found (here)[https://www.datatechnotes.com/2018/02/polynomial-regression-curve-fitting-in-r.html] ggplot(dat1, aes(x=log(depth), y=(damaged/total), color=insecticide)) + geom_point() + geom_smooth(method=&#39;glm&#39;, method.args=list(family=&quot;beta_family&quot;), formula = y~x+I(x^2)) ## Warning: deviance residuals not defined for family &#39;beta&#39;: returning NA ## Warning: deviance residuals not defined for family &#39;beta&#39;: returning NA Figure 2.14: Plot of carrot data with quadratic fit. Once we plot the new trend line and its potential non-linearity we see a generally better fit with less data points not overlapping with the confidence intervals. Now let’s build two different binomial error distributed GLMMs with different set ups that relate to either a linear assumption of the data or a non-linear assumption. To do this there are several key differences between what we have done before till now: Our response variable needs to be binary but need to account for damaged plants and non-damaged plants. This can be calculated by subtracting the number of damaged plants from the total number of plants: total-damaged. To combine the values of the damaged and the non damaged we use the function cbind() which combines the two into one vector that the model can use for a response variable: cbind(damaged,total-damaged). We need to model a quadratic model for mod2. In order to do this we will essentially follow the same formula as we did with the ggplot above. So for mod2 our quadratic predictor of depth would be: depth + I(depth^2). Notice we do not add a quadratic component to the predictor insecticide as we did not see a non-linear trend with this variable. mod1 &lt;- glmmTMB(cbind(damaged,total-damaged) ~ insecticide * depth + (1|rep), data=dat1, family=&#39;binomial&#39;) mod2 &lt;- glmmTMB(cbind(damaged,total-damaged) ~ insecticide * depth + I(depth^2) + (1|rep), data=dat1, family=&#39;binomial&#39;) For easier syntax interpretation note above that mod2 is applying a non-linear modification where we include depth and depth squared. The “^” is another way of saying raise depth to the 2nd power. The I() isolate this and tells the R interpreter that the model is including a second order fixed effect. For more info check ?formula. We can plot residuals: plot(simulateResiduals(mod1)) plot(simulateResiduals(mod2)) Which of the two is better? Remember, the best model might not be perfect. Finally, we can also run ANOVAs on the models: Anova(mod1) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: cbind(damaged, total - damaged) ## Chisq Df Pr(&gt;Chisq) ## insecticide 91.449 1 &lt; 2.2e-16 *** ## depth 2.543 1 0.1107873 ## insecticide:depth 14.525 1 0.0001383 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Anova(mod2) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: cbind(damaged, total - damaged) ## Chisq Df Pr(&gt;Chisq) ## insecticide 111.62 1 &lt; 2.2e-16 *** ## depth 563.71 1 &lt; 2.2e-16 *** ## I(depth^2) 594.17 1 &lt; 2.2e-16 *** ## insecticide:depth 15.85 1 6.858e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.3 Beta Distributions In this section we show an example of applying the beta family using true proportions (or percentages). The beta family has major utility when modeling proportion data as in the past the most suitable distributions would have been the gamma distribution or a arcsin transformation of the y response variable. First let’s load data from a case study. The study by Lynn et al. examined herbivory on several plant species at 42 different sites spanning a large latitudinal gradient. They are interested to know if there is more herbivore damage near the equator. If you are interested in the data the paper for this is found (here)[https://onlinelibrary.wiley.com/doi/10.1111/ecog.06114].Let’s take a look but filter species to just one for simplicity: b1 &lt;-read_csv(&quot;LynnETAL_2022_Ecography_sodCaseStudy.csv&quot;) %&gt;% filter(species==&#39;SALA&#39;) ## Rows: 301 Columns: 18 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): study, family, species_name, species, meas_year, site, notes ## dbl (11): latitude, longitude, o_lat, o_long, mean_herb, na_wdcpc, mat, iso,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(b1) ## # A tibble: 6 × 18 ## study family species_name species meas_year site latitude longitude o_lat ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 RivkinET… Alism… Sagittaria … SALA 2015 ACT-… 44.5 -77.3 NA ## 2 RivkinET… Alism… Sagittaria … SALA 2015 BPV-… 45.7 -77.6 NA ## 3 RivkinET… Alism… Sagittaria … SALA 2015 BSM-… 44.8 -76.1 NA ## 4 RivkinET… Alism… Sagittaria … SALA 2015 BTC-… 44.2 -76.8 NA ## 5 RivkinET… Alism… Sagittaria … SALA 2015 CBT-… 47.4 -79.7 NA ## 6 RivkinET… Alism… Sagittaria … SALA 2015 CFR-… 44.7 -77.1 NA ## # ℹ 9 more variables: o_long &lt;dbl&gt;, mean_herb &lt;dbl&gt;, na_wdcpc &lt;dbl&gt;, mat &lt;dbl&gt;, ## # iso &lt;dbl&gt;, dtr &lt;dbl&gt;, precip &lt;dbl&gt;, aet &lt;dbl&gt;, notes &lt;chr&gt; Let’s examine a histogram of the response variable mean_herb: hist(b1$mean_herb) ## examine histogram. Data are entered as percentage (0-100%) Let’s plot out the data. Note: data was collected at 42 unique sites (one data point per site). ggplot(b1 , aes(x=latitude, y=mean_herb)) + geom_point() Figure 3.1: Plot of herbivory data. We see a potential decreasing of mean_herb across a latitudinal gradient (from latitude 42 to ~48 degrees). Now we need to convert mean_herb to a proportion and then do a small transformation to remove 0s and 1s: b1$mean_herb1 &lt;- (b1$mean_herb/100) b1$mean_herb1 &lt;- (b1$mean_herb1*(length(b1$mean_herb1)-1)+.5)/length(b1$mean_herb1) Let’s plot this along with a trend line applying the beta family as done previously above: ggplot(b1, aes(x=latitude, y=mean_herb1)) + geom_point() + geom_smooth(method=&#39;glm&#39;, method.args=list(family=&quot;beta_family&quot;), formula = y~x) ## Warning: deviance residuals not defined for family &#39;beta&#39;: returning NA Figure 2.21: Plot of herbivory by latitude with best fit line. Now let’s build a GLMM with a beta distribution. Implementing this model structure is relatively simple with glmmTMB. In the code below we simply specify the family argument with beta_family. bm1 &lt;- glmmTMB(mean_herb1 ~ latitude, data=b1, family=&#39;beta_family&#39;) Anova(bm1) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: mean_herb1 ## Chisq Df Pr(&gt;Chisq) ## latitude 9.1383 1 0.002503 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s check the summary: summary(bm1) ## Family: beta ( logit ) ## Formula: mean_herb1 ~ latitude ## Data: b1 ## ## AIC BIC logLik deviance df.resid ## -126.9 -121.7 66.5 -132.9 39 ## ## ## Dispersion parameter for beta family (): 30.5 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.69631 2.58221 2.206 0.0274 * ## latitude -0.17525 0.05797 -3.023 0.0025 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model indicates a strong statistical effect of latitude on mean_herb with lower values of mean_herb tracking an increase in latitude. Plot and check residuals: plot(simulateResiduals(bm1)) The QQ plot is good. The residuals versus prediction plot and line is not ideal but is probably ok. 6.3.1 R CHALLENGE For this part of the problem set you will use real data (Rivkin et al. 2018 Am J Bot). If you want to know more about the study, follow this (link)[https://bsapubs.onlinelibrary.wiley.com/doi/full/10.1002/ajb2.1098]. Herbivory data was collected from 43 populations of an aquatic plant across a latitudinal gradient in Canada. At each population, many plants (~5-15) were examined for herbivory damage. Some additional covariates were recorded, such as competition around the plant (1-3 from less to more) and plant height (cm). Read in data, look at the data structure, and plot the variable of interest, lead damage: d1 &lt;-read.csv(&quot;ajb21098-sup-0002-appendixs2.csv&quot;) head(d1) ## Population Latitude Longitude Competition Individual PlantHeight LeafDamage ## 1 ACT-ON 44.54829 -77.32384 2 4 41.0 0.02000000 ## 2 ACT-ON 44.54829 -77.32384 2 6 43.0 0.03000000 ## 3 ACT-ON 44.54829 -77.32384 2 18 17.5 0.04000000 ## 4 ACT-ON 44.54829 -77.32384 2 2 37.0 0.06666667 ## 5 ACT-ON 44.54829 -77.32384 2 3 48.0 0.07000000 ## 6 ACT-ON 44.54829 -77.32384 2 12 39.5 0.08000000 ## LarvalAbundance ## 1 0 ## 2 0 ## 3 0 ## 4 1 ## 5 0 ## 6 0 hist(d1$LeafDamage) Let’s remove 0’s and 1’s (see Smithson &amp; Verkuilen 2006 (here)[https://psycnet.apa.org/doiLanding?doi=10.1037%2F1082-989X.11.1.54] or Douma &amp; Weedon 2018 (here)[https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13234]): d1$LeafDamage &lt;- (d1$LeafDamage*(length(d1$LeafDamage)-1)+.5)/length(d1$LeafDamage) Now plot data where we have a proportion value of leaf damage on the y-axis along the x-axis of latitude. Let’s also fit a best fit line using the beta family: ggplot(d1 , aes(x=Latitude, y=LeafDamage)) + geom_point() + geom_smooth(method=&#39;glm&#39;, method.args=list(family=&quot;beta_family&quot;), formula = y~x) ## Warning: deviance residuals not defined for family &#39;beta&#39;: returning NA Question: Does herbivory increase towards the equator? How do residuals look? Any way to improve them? For this part of the problem set we will use the Rats data set with only females. Half treated with a drug and the other half were untreated. Then they were checked for tumors. Does the drug reduce probability of developing tumor? After 50 days? After 100 days? Load data: library(survival) ## Warning: package &#39;survival&#39; was built under R version 4.3.3 ## ## Attaching package: &#39;survival&#39; ## The following object is masked from &#39;package:boot&#39;: ## ## aml rats &lt;- (survival::rats) %&gt;% filter(sex==&#39;f&#39;) ?rats head(rats) ## litter rx time status sex ## 1 1 1 101 0 f ## 2 1 0 49 1 f ## 3 1 0 104 0 f ## 4 3 1 104 0 f ## 5 3 0 102 0 f ## 6 3 0 104 0 f Plot data: ggplot(rats , aes(x=time, y=status)) + geom_point() + geom_smooth(method=&#39;glm&#39;, method.args=list(family=&quot;binomial&quot;), formula = y~x) + facet_wrap(~rx) 6.4 Zero-Inflated Models In this section we will go over applying zero-inflated models. While the main lecture should have covered these types of models in depth, let’s quickly review some main points when it comes to these models: In biological data, especially in ecology, zeros are often widespread in the dataset. However these zeroes and what they represent, i.e. absence of species, are not necessarily ‘true’. For example, in a survey of species I may not have found species A at specific sites but that may not be because species A truly does not exist in those sites but instead those zeros may be due to poor sampling methods or sampling mistakes. Therefore the underlying thought is that many of these zeros in these datasets are technically ‘false zeros’ that should be modeled separately from the rest of the data. Therefore, because there are likely to be two distinct processes that generate zeros in a dataset a zero-inflated model will apply two types of models. The first model is a logit model that will model the excess zeros while the second model can be a count model such as a poisson or negative binomial GLM. Now let’s load the data (wheatley carrot infection by carrot fly larvae): data(&quot;ridout.appleshoots&quot;) Let’s clean up the data and check out the distribution of the data in question: dat1 &lt;- ridout.appleshoots %&gt;% mutate(photo=as.factor(photo)) hist(dat1$roots) Now let’s run several models: a Poisson, a negative binomial, and a negative binomial with a general intercept for zero inflation: ## poisson model mod1 &lt;- glmmTMB(roots ~ photo * bap , data=dat1, family=&#39;poisson&#39;) plot(simulateResiduals(mod1)) ## negative binomial model mod2 &lt;- glmmTMB(roots ~ photo * bap , data=dat1, family=&#39;nbinom2&#39;) plot(simulateResiduals(mod2)) ## negative binomial w/ a general intercept for zero inflation (ie. ZI equal for all observations) mod3 &lt;- glmmTMB(roots ~ photo * bap , data=dat1, family=&#39;nbinom2&#39;, zi=~1) plot(simulateResiduals(mod3)) summary(mod3) ## back-transform ZI intercept (exp(p)/(1+exp(p))) ## Family: nbinom2 ( log ) ## Formula: roots ~ photo * bap ## Zero inflation: ~1 ## Data: dat1 ## ## AIC BIC logLik deviance df.resid ## 1342.4 1364.0 -665.2 1330.4 264 ## ## ## Dispersion parameter for nbinom2 family (): 12.6 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.895659 0.072535 26.134 &lt;2e-16 *** ## photo16 -0.029863 0.135890 -0.220 0.8261 ## bap 0.008760 0.006724 1.303 0.1927 ## photo16:bap -0.032083 0.013065 -2.456 0.0141 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.226 0.151 -8.125 4.49e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 inv.logit(fixef(mod3)$zi) ## use inv.logit from boot package to backtransform ## (Intercept) ## 0.2267941 The zero-inflated model is much better than without, so there is definately some zero-inflation. To understand the zero-inflation parameter (-1.226) we need to back-transform it. The zero-inflated model uses a logit-link fuction, which is seperate from the distribution and link function of the main model (although in this example with the beta family, it also has a logit-link). We can do this manually or use the inv.logit function from the boot package. We get a value of 0.22, meaning there is a 22% change any observation will be a zero beyond what is expected by the main model assuming a beta distribution. We don’t really know what causes the extra zeros, but this model does an ok job of caturing them. Maybe we can do a little better though. What do we expect to cause the zero inflation? We can check out further details in the data’s metadata: ??ridout.appleshoots The metadata state that “Almost all of the shoots in the 8 hour photoperiod rooted. Under the 16 hour photoperiod only about half rooted.” Perhaps the photoperiod treatment is a better predictor of extra zeros? ggplot(dat1, aes(x=roots)) + geom_histogram() + facet_grid(bap~photo) + theme_bw(base_size=16) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 6.2: Plot of apple shoot data. Now let’s run a negative binomial with zero inflation modeled as a function of treatment: mod4 &lt;- glmmTMB(roots ~ photo * bap , data=dat1, family=&#39;nbinom2&#39;, zi=~photo) summary(mod4) ## Family: nbinom2 ( log ) ## Formula: roots ~ photo * bap ## Zero inflation: ~photo ## Data: dat1 ## ## AIC BIC logLik deviance df.resid ## 1252.3 1277.5 -619.2 1238.3 263 ## ## ## Dispersion parameter for nbinom2 family (): 13.8 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.898347 0.071568 26.525 &lt;2e-16 *** ## photo16 -0.023525 0.130488 -0.180 0.8569 ## bap 0.008222 0.006689 1.229 0.2190 ## photo16:bap -0.029492 0.012427 -2.373 0.0176 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.4000 0.8446 -5.210 1.89e-07 *** ## photo16 4.2831 0.8627 4.965 6.87e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Plot residuals: plot(simulateResiduals(mod4)) Now let’s compare all models with AIC. Which one was the best? Why do you think? ### check with AIC AIC(mod1,mod2,mod3,mod4) ## df AIC ## mod1 4 1575.916 ## mod2 5 1412.415 ## mod3 6 1342.435 ## mod4 7 1252.333 6.5 Model App exercise This shinyapp will allow you to practice and implement a variety of model types to fit randomly generated data. You can test yourself by randomly generating data and then choosing which models you would apply based on plots of the data as well as the data structure (e.g. count data vs. binary). knitr::include_app(&quot;https://leoohyama.shinyapps.io/adv_residuals/&quot;, height = 1000) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
